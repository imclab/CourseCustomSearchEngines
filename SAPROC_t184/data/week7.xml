<?xml version="1.0"?>
<Item TextType="CompleteItem" SchemaVersion="1.2" id="WEB012110" ACSPageNumber="1" ACSTemplate="generic_unnumbered" ACSCourseItem="T184" LastRendering="VLE Preview" DiscussionAlias="Discussion" SessionAlias="" SecondColour="None" ThirdColour="None" FourthColour="None" Logo="colour"><CourseCode>T184</CourseCode><CourseTitle>Robotics and the meaning of life: a practical guide to things that think</CourseTitle><ItemID>T184</ItemID><ItemTitle>Lesson 7</ItemTitle><Unit><UnitID>T184 Lesson 7 Introduction</UnitID><UnitTitle>T184 Lesson 7 Introduction</UnitTitle><ByLine>Prepared for the module team by Jeffrey Johnson and Tony Hirst</ByLine><Session><Title>Lesson 7: Optimisation</Title><Paragraph><i>Prepared for the module team by Jeffrey Johnson and Tony Hirst, update by Jon Rosewell</i></Paragraph><Paragraph>Optimisation means achieving the &#x2018;best&#x2019; outcome in a given situation, or the &#x2018;best&#x2019; solution to a given problem. For example, if you were to mislay your keys, the best outcome would be to find them again in the minimum time possible. Alternatively, if you were to take a module, the best outcome would be to pass the exam with the highest mark possible.</Paragraph><Paragraph>So, optimisation requires a goal &#x2013; i.e. find the car keys &#x2013; and a means of measuring how well this goal has been achieved &#x2013; i.e. in the minimum time possible. Of course, not all solutions can be described as &#x2018;best&#x2019;. They can also be &#x2018;better&#x2019; or just plain &#x2018;good&#x2019; &#x2013; usually any solution is good compared with not finding a solution at all. So even if it takes you an hour to find your car keys, this is probably better than not finding them at all. Not always though. If losing your keys means missing an appointment, it might be better to abandon that goal and to call for a taxi. In this case the goal of finding your keys has lower priority than the goal of getting to the appointment on time.</Paragraph><Paragraph>The same assumptions apply for optimisation in robotics as they do in real life. In RobotLab6 the goal for the line-following robot was to &#x2018;stay on the line&#x2019;. This was later refined to &#x2018;stay on the line and get to the finish in the least possible time&#x2019;. In artificial intelligence and robotics, one of the major objectives is to get intelligent systems to make relatively good decisions given the information they have available to them. Many situations are so complex that there is rarely a &#x2018;best decision&#x2019;, but rather a choice between alternative &#x2018;good decisions&#x2019;. The technique of optimisation makes it possible to quantify different alternatives, and provides a basis on which to make operational decisions.</Paragraph><Paragraph>Sometimes the best course of action may depend on what decisions have already been taken by others. In these situations an approach known as &#x2018;game theory&#x2019; may be used to help in the decision-making process.</Paragraph><Paragraph>Another approach makes use of the idea of a &#x2018;potential field&#x2019;. This can be used to describe an imaginary force or set of forces that act on the robot to modify its behaviour. Potential field approaches allow robots to navigate through an environment by imagining various force fields acting on them. The &#x2018;potential&#x2019; of the physical points around the robot relate to the ease or difficulty with which the robot can reach those points. More traditional approaches require the robot to plan its course, often using optimisation techniques to choose between different possible plans that lead to the same outcome, but at different cost.</Paragraph><Paragraph>Lesson 7 comprises a number of study sessions, listed to the left of this page.</Paragraph><Section><Title>Learning outcomes </Title><Paragraph>At the end of Lesson 7 you should be able to:</Paragraph><BulletedList><ListItem>explain what is meant by optimisation;</ListItem><ListItem>explain the meaning of local and global optima;</ListItem><ListItem>explain what is meant by the terms: landscape, maximum and minimum;</ListItem><ListItem>appreciate how game theory can help in the decision-making process;</ListItem><ListItem>give the example of Braitenberg&#x2019;s vehicles responding to potentials;</ListItem><ListItem>explain the interaction of Asimov&#x2019;s laws in terms of their potential.</ListItem></BulletedList></Section></Session><Session><Title>7.1 Search</Title><Paragraph>Many of the problems faced by people &#x2013; and robots &#x2013; in a complex environment do not have obvious solutions, so we have to search for them. For example, what should a robot football player do at any given moment? What should a robot astronaut do if it damaged itself in a fall? What should a robot nursemaid do if its child has a temper tantrum? How can a robot get its work done in the minimum time? How can a robot use its power supply most efficiently?</Paragraph><InternalSection><Heading>Solution space and search space</Heading><Paragraph>It has proven very productive to think of such questions in terms of searching for solutions. In a general way, one can hypothesise the set of all possible solutions to a problem. Consider the problem of finding a pair of dominos with spots adding up to 21. The four solutions to this problem are shown below. Rather abstractly, this set of four solutions is called a &#x2018;solution space&#x2019;. It contains everything that could be considered to be a solution to this problem.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f01.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f01.jpg" x_imagewidth="512" x_imageheight="107"/><Caption>The solution space for the 21-spot domino problem</Caption></Figure><Paragraph>The issue of finding a solution to a problem can then be rephrased as searching for a member of the solution space. Where to search? Are the following candidate solutions for the 21-spot domino problem?</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f02.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f02.jpg" x_imagewidth="451" x_imageheight="117"/><Caption/></Figure><Paragraph>The pair of dominos is a candidate. It could be a solution, but in this case it is not. The pair of aces is not a candidate. Playing cards are not dominos as specified. The knife, fork and spoon combination has the wrong number of the wrong things, and is not a candidate.</Paragraph><Paragraph>The set of all candidate solutions to a problem is referred to as the <b>search space</b>. The search space for the 21-spot domino problem is the set of all pairs of dominos. The <b>solution space</b> is that part of the search space that contains actual solutions to the problem.</Paragraph></InternalSection><SAQ><Heading>SAQ 1</Heading><Question><NumberedList class="lower-alpha"><ListItem>What is the search space for the problem of finding a spoon in the kitchen? What is the solution space?</ListItem><ListItem>What is the search space for the problem of finding a holiday? What is the solution space?</ListItem></NumberedList></Question><Answer><NumberedList class="lower-alpha"><ListItem>The search space for finding a spoon in the kitchen is the set of all places that spoons could be: in the drawer, in the cupboard, on the floor, behind the fridge, in the sink, and so on.  The solution space is the set of all those places where there are spoons.</ListItem><ListItem>The search space for finding a holiday is the set of all possible holidays.  The solution space is the set of all holidays that satisfy your personal criteria.</ListItem></NumberedList></Answer></SAQ><Section><Title>Brute force search</Title><Paragraph>The notions of search space and solution space help in problem solving because they help us to focus on where to search. Once we have decided what the problem is and what characterises a possible solution, we know how to search that space in order to find a solution.</Paragraph><Paragraph>One approach is an &#x2018;exhaustive&#x2019; or &#x2018;brute force&#x2019; search. This means looking at every member of the search space until a solution is found. When search spaces are relatively small this can be a good option. When they are large this is a poor option.</Paragraph><Paragraph>To get a feel for how big some search spaces are, consider the proverbial monkey sitting at a computer keyboard. It is said that if you have enough monkeys randomly hitting the keys, then eventually they will type out the complete works of William Shakespeare.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f03.gif" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f03.gif" x_imagewidth="228" x_imageheight="160"/><SourceReference>GST Software Publishing</SourceReference></Figure><Paragraph>Suppose a monkey types four characters. What are the chances that those four characters will be &#x2018;T184&#x2019;? My computer has 47 character keys and the spacebar. So there are 48 x 48 x 48 x 48 four character permutations. That&#x2019;s over five million! If the monkey were to hit four keys at random, you&#x2019;d expect T184 to occur roughly once in every five million attempts.</Paragraph><Paragraph>As an experiment, open a text processing program such as Microsoft Word or Notepad. Using the blunt end of a pencil, type in four characters with your eyes shut. Did you manage to type in T184? Are you surprised?</Paragraph><Paragraph>When I tried this, I typed in &#x2018;FQUE&#x2019;. Actually I thought this was quite good, because all my letters were close to the target as shown below. It does not surprise me that I didn&#x2019;t get T184, but this experiment suggests that I was not hitting the keys at random, and that I had some knowledge of the positions of the target keys. So I would expect to get T184 in less than five million attempts, since my keystrokes are biased by my knowledge of the keyboard.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f04.gif" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f04.gif" x_imagewidth="450" x_imageheight="164"/><Caption/></Figure><Paragraph>When searching for the optimum solution, you cannot know if you&#x2019;ve found it until every point in the search space has been tested. And some search spaces are very big indeed. Appropriately, these are called &#x2018;hard&#x2019; problems!</Paragraph><Paragraph>However, in other problems it is possible to exclude large parts of the search space from consideration. For example, suppose you are asked which two numbers in the range 0 to 9 can be multiplied together to give 35. Immediately you can &#x2018;prune&#x2019; the search space to remove any pair with a zero or an even number in it, since 35 is an odd number. Multiplying any number by zero gives zero, and multiplying any number by an even number gives an even number. Are there any other numbers that can be discounted?</Paragraph><Paragraph>Most problems are much more complex than examining pairs of dominoes. For example, how many combinations of playing cards have spots that add up to 40, assuming jacks, queens, kings and aces all count as having 10 spots? The search space for this problem is the set of all subsets of 52 cards. There are 52 x 51 x 50 x &#x2026; 3 x 2 x 1 of these. That&#x2019;s a &#x2018;1&#x2019; followed by sixty-three &#x2018;0s&#x2019;, or billions of billions of billions. If it takes a robot one millionth of a second to examine each combination of playing cards to see if the spots add up to 40, how long would it take in total? There are over 30 million seconds in a year, so it could examine 30,000,000,000,000 a year. Not bad, but not good enough. It would take billions of years to examine them all.</Paragraph><Paragraph>This is one of the reasons why Moore&#x2019;s law, which you came across in Lessons 1 and 4, will not necessarily make machines more intelligent. An increase of processing power by ten, a hundred, or even a million does not help much with problems like this. There are some problems that are just too complex to succumb to brute force search.</Paragraph></Section><Section><Title>The structured search</Title><Paragraph>Humans solve problems using their intelligence. We manage to understand massive search spaces in ways that enable us to find solutions to bewilderingly complex problems without resorting to &#x2018;brute force&#x2019;.</Paragraph><Paragraph>Consider the (nice) problem of choosing a holiday. There are thousands, if not millions, of possibilities: beach holidays, skiing holidays, sightseeing holidays, jungle treks, and so on. Suppose you tried a brute force search and looked at them all in the brochures taking, say, half a minute each. If there were one million holidays, and you worked at it twenty hours a day, it would take you over a year, so you&#x2019;d be exhausted and miss your holiday anyway. Not a very good outcome!</Paragraph><Paragraph>Of course people don&#x2019;t search for holidays like this. They use a &#x2018;structured&#x2019; search. For example, if you have young children you might decide you want a beach holiday not too far away, and restrict your attention to that part of the search space. Having made that decision, money might be a bit tight this year, so you would only look at inexpensive holidays.</Paragraph><Paragraph>These decisions can be represented as a tree. Every time one decision is made, large parts of the tree are &#x2018;pruned&#x2019; from the search. In this case, by the time you are looking for an inexpensive beach holiday in Blackpool, the reduced search space might be small enough to search exhaustively.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f05.gif" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f05.gif" x_imagewidth="499" x_imageheight="257"/><Caption/></Figure><Paragraph>Robots can also form structured searches by using their knowledge base. When they have a problem they can rule out many potential solutions. For example, a robot might be searching for the best way to get its batteries charged. By structuring the search space, it can focus its attention on those possibilities that are relevant.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f06.gif" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f06.gif" x_imagewidth="421" x_imageheight="197"/><Caption/></Figure></Section></Session><Session><Title>7.2 Trade-offs and equilibrium</Title><Paragraph>In this session you will read what I think is one of Asimov&#x2019;s most interesting  stories. It concerns the interplay between the three laws.</Paragraph><Activity><Heading>Read</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\book_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="book_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure>Read Chapter 2, &#x2018;Runaround&#x2019;, of Asimov&#x2019;s book, and answer the SAQ.</Paragraph></Question></Activity><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f07.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f07.jpg" x_imagewidth="140" x_imageheight="225"/><SourceReference><font val="Verdana"><language val="">&#xA9; Harper Collins Publishers</language></font></SourceReference></Figure><SAQ><Heading>SAQ 2</Heading><Question><Paragraph>Summarise this story and explain how the problem is resolved.</Paragraph></Question><Answer><Paragraph>The robot Speedy has been sent out to get some selenium.  However, the robot walks in circles around a selenium pool because of an interplay between the second and third laws. </Paragraph><Paragraph>Due to the release of volcanic gases, this selenium pool is dangerous, and the closer the robot gets to it, the more dangerous it gets. This means that the &#x2018;potential&#x2019; of the third law increases as the robot gets closer to the selenium source. Self-protection has relatively high potential for this robot because it is very valuable.   The instruction to the robot to get the selenium was rather casual, and so it has a relatively low potential for the second law.  This combination of circumstances means that the potentials for the second law (obey command) and third law (protect yourself) are equal.  When the robot moves a bit further way from the selenium source, the second law gets stronger, and the robot moves back towards the source. This makes it go round in a circle.  Also, half its positronic circuits don&#x2019;t work, so the robot behaves as if it is intoxicated.  </Paragraph><Paragraph>To shake the robot out of the impasse, Powell uses another robot to put himself in mortal danger.  When Speedy realises that a human is in mortal danger, the potential for the first law increases until it exceeds the potential for the second and third laws, and the robot abandons what it was doing to save Powell. </Paragraph><Paragraph>Donovan then sends Speedy to another selenium pool, with orders to get the essential selenium at all costs.  This time Speedy got it in forty-two minutes and three seconds.</Paragraph></Answer></SAQ><Section><Title>Trade-offs</Title><Paragraph>Suppose you are walking towards a bonfire, taking a temperature measurement at  metre intervals.  The closer you get to the bonfire, the hotter it becomes. This is illustrated below.  At 8 metres from the bonfire the temperature is 11 &#xB0;C and at 7 metres it is 20 &#xB0;C, and so on.  You wisely stop 5 metres from the fire because 67 &#xB0;C is getting dangerously hot.  You don a protective suit so you can  get closer and you continue to within 1 metre of the fire.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f08.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f08.jpg" x_imagewidth="511" x_imageheight="231"/><Caption/></Figure><Paragraph>Suppose you are working with a material that gets easier to work as its temperature increases, as illustrated below.  What would be the optimum distance from the fire to work the material?  If you were trying to minimize cost, then according to this graph, the lowest cost would be 45 pence at 3 metres from the fire.  If you were to go any closer it would get more expensive, and if you were to go further away it would get  more expensive.  In some sense, 3 metres would be the best, or  &#x2018;optimum&#x2019;, distance from the fire.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f09.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f09.jpg" x_imagewidth="496" x_imageheight="216"/><Caption/></Figure><Paragraph>However, suppose you are paid extra the more uncomfortable you get. For simplicity let&#x2019;s assume that you are paid an extra penny for each additional degree centigrade.  Then, from your employer&#x2019;s viewpoint, the optimum place for you to work would be 8 or more metres from the fire.</Paragraph><Paragraph>However, at 8 metres from the fire the material is as hard to work as it can be, and the cost of that is very high.  What is the optimum, taking into account both temperature-related wage costs and the cost of working the material?</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f10.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f10.jpg" x_imagewidth="474" x_imageheight="231"/><Caption/></Figure><Paragraph>There is a &#x2018;trade-off&#x2019; between the reduced cost of working the material at a higher temperature on the one hand, and the increased cost of being too hot on the other.  For simplicity I have assumed that the cost of getting too hot is directly related to temperature, so that the total cost of working at 1 metre from the fire is 560 pence.  Moving closer to the fire decreases the cost of working the materials, but increases the amount you are paid.</Paragraph><Paragraph>In this simple example, the optimum position is found from plotting a graph of the aggregate costs.  This is shown below.   There is clearly a minimum cost  at 4 metres from the fire, and this is the optimum in this case.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f11.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f11.jpg" x_imagewidth="473" x_imageheight="340"/><Caption/></Figure><Paragraph>In many situations it is necessary to trade off one thing against another.  For example, if a robot moves very fast in a cluttered environment it gets to its destination faster, but if it goes too fast, it is more likely to have an accident.  In this case the robot might have to trade off speed against safety.</Paragraph><Paragraph>When the conflicting requirements balance each other, a system is said to be in equilibrium.</Paragraph><SAQ><Heading>SAQ 3</Heading><Question><Paragraph>What was being traded off by Speedy in Asimov&#x2019;s &#x2018;Runaround&#x2019; story?</Paragraph></Question><Answer><Paragraph>The danger of collecting selenium (third law) was being traded off against the need to obey an instruction (second law).  When the first law finally came into play, it was much stronger than either the second or third laws, and  Speedy could break out of the equilibrium.</Paragraph></Answer></SAQ></Section><Section><Title>Landscapes with global and local optima</Title><Paragraph>In the above example there was just one optimum, but in many situations there are many alternative optima.  For example, Pam, a mountain climber, may get more satisfaction from climbing mountain A than mountain B (see below).  If there were no other considerations, she might choose A.  However, suppose she starts from D.  The cost of getting to A might be prohibitive.  In this case B is a good second best, or a good &#x2018;sub-optimum&#x2019; solution to the problem of choosing which mountain to climb.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f12.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f12.jpg" x_imagewidth="401" x_imageheight="159"/><Caption/></Figure><Paragraph>Consider the plight of Dave, who has a respiratory problem, trying to survive in this landscape.  Suppose for him, high altitude causes discomfort, and the lower he is the more comfortable he feels.  Where is the best place for him  to be in this landscape?</Paragraph><Paragraph>D is lower than C, so D is the best, or &#x2018;global&#x2019;, optimum.  However, suppose Dave were staying in a hotel at C.  Would he be better off moving to D?  If there were a mountain ridge between C and D, peaking at B, he would have to climb over the ridge.  No matter what he did he would feel more uncomfortable.  So, to reach the global optimum, he would have to leave the relatively comfortable &#x2018;local&#x2019; optimum at C, and become more uncomfortable, before he could reach the more comfortable position at D.</Paragraph><Paragraph>In a situation like this, it is often better to stay in a relatively good local optimum, than expend a lot of energy trying to find the global optimum.</Paragraph><Paragraph>The idea of a landscape is actually one that is widely used in optimisation. The height of the landscape is associated with some measure of &#x2018;goodness&#x2019; or &#x2018;badness&#x2019; about the quality of a solution to a problem. Your position in the landscape corresponds to one possible solution to the problem.</Paragraph><Paragraph>If there is a peak in the landscape that is surrounded by points that have a lower &#x2018;goodness&#x2019; value, we call it a &#x2018;local maximum&#x2019;. If there is a valley in the landscape that is surrounded by points that only have higher &#x2018;goodness&#x2019; values, we call it a &#x2018;local minimum&#x2019;.  The single largest maximum in the landscape, when it exists, is called the &#x2018;global maximum&#x2019;, and the smallest minimum in the landscape, when it exists, is called the &#x2018;global minimum&#x2019;.</Paragraph><Paragraph>In some problems, the height of the landscape may refer to how bad a solution is. For example, when a neural network is trained, the &#x2018;error&#x2019; between the desired output and the actual output may be represented using the idea of a landscape. In this case, the optimisation process is looking for minima &#x2013; points where the error is lowest.</Paragraph><Paragraph>Both maxima and minima can thus be seen as optima, depending on the problem, and optimisation may be seen as a way of trying to find the highest point in a landscape (maximisation) or the lowest point (minimisation).</Paragraph><SAQ><Heading>SAQ 4</Heading><Question><Paragraph>In the UK, the price of petrol and diesel fuel depends on many things, including where you live. Suppose the fuel used in your car is 85 pence per litre from your nearest garage.  If you look elsewhere the cost of fuel varies as shown in the following graph.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184w_lesson7_f13.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184w_lesson7_f13.jpg" x_imagewidth="383" x_imageheight="184"/><Caption/></Figure><Paragraph>Where are the local maxima and minima in the price landscape shown?   Are there any global optima in the landscape?  If so, where?</Paragraph></Question><Answer><Paragraph>The local maxima and minima, and the global maximum and minimum, are shown below.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f14.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f14.jpg" x_imagewidth="384" x_imageheight="189"/><Caption/></Figure></Answer></SAQ><InternalSection><Heading>Hill climbing</Heading><Paragraph>In optimisation, the metaphor of a landscape is taken a further step by the term &#x2018;hill climbing&#x2019;. Suppose you were blindfolded on the slope between A and B in the mountain range we looked at earlier. Suppose the height of the mountain represents a cost you have to incur.  Ideally you want a lower cost. You can&#x2019;t see the whole mountain range because of the blindfold, but you can feel around with your feet. In this way you can find that going one way is up and the other is down. So you take a step down, getting a better solution to reducing the cost. If you do this repeatedly you will climb down the mountain until you are in the valley at C. When you get to C you have &#x2018;hill-climbed&#x2019; into a local minimum.</Paragraph></InternalSection></Section><Section><Title>The travelling salesman problem</Title><Paragraph>The so-called &#x2018;travelling salesman&#x2019; is a classic problem in artificial intelligence. Consider a salesman wanting to visit London (1), Plymouth (2), Canterbury (3), Stratford-upon-Avon (4), Brighton (5), Cambridge (6), Cardiff (7), Oxford (8) and Bath (9). These towns are illustrated below. Starting at London, what is the shortest way to visit each of them once, returning back to London? The problem is relatively easy for a small number of cities like the nine shown here, but quickly gets much more difficult if you start adding cities.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f15.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f15.jpg" x_imagewidth="331" x_imageheight="177"/><Caption/></Figure><Paragraph>The route shown in (a) below takes the cities in the order I&#x2019;ve numbered them.  This is longer than the second route (b).  The solutions to the problem can be represented by a string of numbers.  For (a) it is 1-2-3-4-5-6-7-8-9-1 and for (b) 1-3-5-2-9-7-4-8-6-1.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f16.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f16.jpg" x_imagewidth="512" x_imageheight="154"/><Caption/></Figure><Paragraph>So the problem becomes one of finding a way of ordering the numbers so that the route they represent is the shortest. It is assumed that it does not matter which way round the circuit the salesman travels, e.g. the route 1-2-3-4-5-6-7-8-9-1 is considered to be the same as 1-9-8-7-6-5-4-3-2-1. Now that a simple &#x2018;representation&#x2019; of the problem has been formulated it is easy to see that the search space is the set of all sequences of the numbers 1, 2, 3, 4, 5, 6, 7, 8 and 9.  The solution space contains the sequence (or sequences) that involve the shortest distance between them.</Paragraph><Paragraph>The travelling salesman problem is interesting because the size of the search space gets very big very quickly.  Suppose you start in London.  You have eight choices of city to go to.  After you arrive at your first destination you have seven choices, at your second destination you have six choices, and so on. This gives 8 &#xD7; 7 &#xD7; 6 &#xD7; 5 &#xD7; 4 &#xD7; 3 &#xD7; 2 &#xD7; 1 choices in total.  These include travelling each route in both directions, so the number of routes is divided by 2.  Hence, for 9 cities the number of routes is  (8 &#xD7; 7 &#xD7; 6 &#xD7; 5 &#xD7; 4 &#xD7; 3 &#xD7; 2 &#xD7; 1) &#xF7; 2.</Paragraph><Paragraph>For N cities, there are  [(N&#x2013;1) &#xD7; (N&#x2013;2) &#xD7; ... &#xD7; 3 &#xD7; 2 &#xD7;1] &#xF7; 2 routes for the salesman. This gets astronomically large when N exceeds 12.</Paragraph><Paragraph>The travelling salesman problem clearly has practical applications.  Imagine how many millions of round trips are made each year by lorries, delivering the food and goods that we all demand.  If they could be sure of  taking the minimum route they would save a lot of time and money.  Also, the travelling salesman problem has applications in microchip design, when large numbers of transistors have to be connected using the shortest connections. It has many other applications.</Paragraph></Section><Section><Title>Genetic algorithms</Title><Paragraph>A genetic algorithm is a computer program modelled on natural evolution. Solutions to problems are &#x2018;evolved&#x2019; by creating new &#x2018;child&#x2019; solutions from &#x2018;parent&#x2019; solutions, with poor solutions being removed from the population. First you have to be able to represent the problem as a string of numbers, like 1-3-5-2-9-7-4-6-1.  These are called &#x2018;genomes&#x2019; &#x2013; each genome describes a candidate solution to the problem. After evaluating each genome (for example, by calculating the length of the tour) the better candidate solutions are preferentially selected and used to generate new candidate solutions that make up the next generation of genomes.</Paragraph><Paragraph>The &#x2018;offspring&#x2019; candidate solutions are generated by combining elements from two parent genomes. Just as human children bear traits from both parents, the next generation of candidate solutions are made up from different aspects of their &#x2018;parents&#x2019;.</Paragraph><Paragraph>Consider the parent solutions (c) and (d):</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f17.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f17.jpg" x_imagewidth="512" x_imageheight="154"/><Caption/></Figure><Paragraph>Each solution has some good and some bad parts.   Two possible child solutions derived from these are shown in (e) and (f) below.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f18.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f18.jpg" x_imagewidth="512" x_imageheight="155"/><Caption/></Figure><Paragraph>Solution  (e) is rather poor, but solution (f) has inherited the best parts of both parents, and is a good new solution. By combining the parts of the routes in this way, new connections are made, and these are shown in green.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f19.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f19.jpg" x_imagewidth="512" x_imageheight="62"/><Caption/></Figure><Paragraph>This kind of &#x2018;breeding&#x2019; between two genomes can be performed easily on computers.   Another mechanism used in genetic algorithms is mutation.</Paragraph><Paragraph>Whereas breeding generates offspring that only rearrange the parents&#x2019; genomes, mutation randomly changes small parts of the offspring&#x2019;s genome. For example, the path shown in (g) swaps the positions of cities 7 and 8 (mutates) to give a new route shown as (h) below.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184lesson7_f20.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184lesson7_f20.jpg" x_imagewidth="512" x_imageheight="180"/><Caption/></Figure></Section></Session><Session><Title>7.3 Game theory and the prisoner&#x2019;s dilemma</Title><Paragraph>There are many situations in which people&#x2019;s actions depend on the actions of others. For example, if someone is nasty to you, you are less likely to do something nice in return. So that other person might reason along the lines that &#x2018;if they are nice to you, then you are likely to be nice to them&#x2019;.</Paragraph><Paragraph>I have a friend whose daughter is sometimes exceptionally nice to him. After a few minutes of this, she invariably asks him to take her somewhere. He always does. Perhaps he subconsciously reasons that she is being nice to him in the hope that he&#x2019;ll give her what she wants (i.e. take her somewhere); and that if he doesn&#x2019;t do this, she&#x2019;ll stop being nice to him. Of course, his daughter is often nice without having ulterior motives. But the point of this is that a robot may be in a situation in which it has to reason about possible strategies for interacting with its unpredictable environment, unpredictable humans or other unpredictable robots.</Paragraph><Paragraph>One approach that is used widely in decision-making systems is &#x2018;game theory&#x2019;. This is used, for example, when a decision needs to be made about the best course of action in the context of a range of possible decisions made by someone else.</Paragraph><Paragraph>I will introduce game theory using a simple game, known as the &#x2018;prisoner&#x2019;s dilemma&#x2019;. Suppose that the police are holding you and an associate in prison on suspicion of a serious charge, of which you are both guilty. You are kept isolated from each other, so that neither of you knows how the other is answering the interrogator&#x2019;s questions. The dilemma that faces you is this:</Paragraph><BulletedList><ListItem>You and your associate know that if you both keep quiet (or in the parlance of game theory, &#x2018;cooperate&#x2019; with each other but not with the police) the serious charge against you will be dropped, but you will both eventually be found guilty on a lesser charge;</ListItem><ListItem>The police have told you both that if you both confess (that is, if you both &#x2018;defect&#x2019;) they will make a recommendation that you both go to prison for a reduced sentence on the most serious charge;</ListItem><ListItem>If one of you confesses (defects) and the other keeps quiet (cooperates) then the person who confessed will be released under a witness protection program and the person who kept quiet will be sent to prison for the maximum term.</ListItem></BulletedList><Paragraph>What should you do? It is possible to tabulate the different possible outcomes and associate each one with a pay-off, i.e. a score that represents how good the outcome is for you.</Paragraph><Table><TableHead/><tbody><tr><td/><td><b>Partner cooperates</b></td><td><b>Partner defects</b></td></tr><tr><td>You cooperate</td><td>Reasonable outcome for you both &#x2013; the lesser charge. Both score 2.</td><td>Lousy outcome for you &#x2013; your partner walks free and you go to jail. You score 0, your partner scores 4.</td></tr><tr><td>You defect</td><td>Excellent outcome for you &#x2013; you go free while your partner goes to jail. You score 4, your partner scores 0.</td><td>Not that good an outcome for you &#x2013; guilty as charged of the serious crime, though with a minimum sentence recommendation. You both score 1.</td></tr></tbody></Table><Paragraph>Looking at the table, the best outcome for you is if you defect against your partner (confess), and your partner cooperates with you (that is, keeps silent). However, if your partner defects as well (the sneak!), the outcome is the next to worst one.</Paragraph><Paragraph>One way of coping with this dilemma is to try to identify a strategy that is least worst for you whatever your associate does. In this example, the safest thing for you to do is defect (and score either 1 or 4). By changing your strategy, you run the risk of doing worse (that is, scoring 0).</Paragraph><Paragraph>Many games, as well as real world decisions, can be analysed using game theory. Indeed, a large amount of research was done by the US RAND Corporation during the Cold War in an attempt to identify optimal (or least bad) nuclear war strategies.</Paragraph><Paragraph>An interesting feature is that if the same game is played over and over again, different sensible strategies emerge if the pay-offs are changed relative to one another. Playing the prisoner&#x2019;s dilemma over and over again is known as the iterated prisoner&#x2019;s dilemma (IPD). In this case, you choose whether to cooperate or defect at each round of the game based on what has happened previously, and a cumulative score is kept. There are many well-known strategies for playing the IPD, and more are continually being discovered.</Paragraph><Paragraph>If you have time, experiment with the IPD at <a href="http://www.iterated-prisoners-dilemma.net/">http://www.iterated-prisoners-dilemma.net/</a>.</Paragraph><SAQ><Heading>SAQ 5</Heading><Question><Paragraph>Consider the following pay-off matrix:</Paragraph><Table><TableHead/><tbody><tr><td/><td><b>Partner cooperates</b></td><td><b>Partner defects</b></td></tr><tr><td>You cooperate</td><td>Both score 8</td><td>You score 1, partner scores 10</td></tr><tr><td>You defect</td><td>You score 10, partner scores 1</td><td>Both score 4</td></tr></tbody></Table><NumberedList class="lower-alpha"><ListItem>In a single play of this prisoner&#x2019;s dilemma, what strategy should you take if you assume your partner will cooperate? What pay-off will you get?</ListItem><ListItem>In a single play of this prisoner&#x2019;s dilemma, what strategy should you take if you assume your partner will defect? What pay-off will you get?</ListItem><ListItem>If you do not know what your partner will do, which strategy should you take and why? Do you think this is an optimal solution in terms of maximizing total pay-off?</ListItem></NumberedList></Question><Answer><NumberedList class="lower-alpha"><ListItem>You should defect and collect 10 points, letting your partner get just a single point</ListItem><ListItem>You should defect &#x2013; that way you get 4 points, rather than just 1.</ListItem><ListItem>You should defect &#x2013; that way you always get at least 4 points. However, if you and your partner both cooperate, you will each get 8 points &#x2013; a maximum in terms of global return. But are you really willing to take that chance...?</ListItem></NumberedList></Answer></SAQ></Session><Session><Title>7.4 Potential fields</Title><Paragraph>One interesting approach to navigation in a robot, which has a lot to do with optimisation, is the use of so-called &#x2018;potential fields&#x2019;. You are probably already familiar with the idea of a potential field, although you may not realise it. A potential field can be thought of as an attractive or repulsive force associated with a particular object. An example of an attractive force is gravity, while an example of a repulsive force is the force that exists between two similar magnet poles when they are in close proximity.</Paragraph><Paragraph>A potential field can be thought of as a force field. The field exists in the region around an object if a force is exerted on another object placed anywhere in that region.</Paragraph><Paragraph>Real potential fields, such as electrostatic or gravitational fields, exist everywhere around an object. A robot does not need to know, or calculate, the &#x2018;forces&#x2019; acting on it at any point in time.  It just needs to sense them. This &#x2018;local&#x2019; information is used by the robot to decide where to move next. So in a potential field approach, the robot uses information from its sensors to identify attractive and repulsive forces in its environment, and then adds these forces together to work out where to go.</Paragraph><Paragraph>The diagram below shows a potential field for an area with a goal and a solid object. The arrows show the direction of the hypothetical force. If a robot enters into this area at any point, for example at A, B or C, it is attracted towards the goal, as indicated by the arrows.  In the case of A and C the robot will automatically avoid the object.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f21.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f21.jpg" x_imagewidth="271" x_imageheight="267"/><Caption/><SourceReference><ItemRights><OwnerRef/><ItemRef/><ItemAcknowledgement>Potential fields tutorial, Brigham Young University [Online at: <a href="http://students.cs.byu.edu/~cs470ta/readings/Pfields.pdf">http://students.cs.byu.edu/~cs470ta/readings/Pfields.pdf</a>]</ItemAcknowledgement></ItemRights>Michael A. Goodrich</SourceReference></Figure><Paragraph>However, the use of potential fields will not necessarily enable a robot to navigate to its intended destination.  It is possible that the forces from several objects will lead the robot into a local minimum from where it cannot proceed further towards its target.</Paragraph><Paragraph>Typically, the strength of a potential field changes, depending on how far you are away from the source of the potential. By analogy, if your friends live a long way from you, you probably feel less compelled to visit them than if they lived closer. For our purposes, the change may be gradual over distance, or it may change sharply at a particular distance from the source.</Paragraph><Paragraph>The potential field approach to navigation uses a robot&#x2019;s sensors to build up a &#x2018;picture&#x2019; in terms of where it can and cannot go. For example, an obstacle placed in front of a mobile robot, which exerts a hypothetical repulsive force on it, would ensure that the robot does not collide with the obstacle.</Paragraph><Paragraph>The image below shows a hypothetical potential field in which the increasing potential is depicted by increasingly darker shades of grey. You will use a variant of this in RobotLab7 to investigate the interplay between the potentials of  the second and third laws in Asimov&#x2019;s &#x2018;Runaround&#x2019; story.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f22.gif" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f22.gif" x_imagewidth="255" x_imageheight="230"/><Caption/></Figure><SAQ><Heading>SAQ 6</Heading><Question><NumberedList class="lower-alpha"><ListItem>A lamp can be thought of as the centre of a potential field for a moth.  Does it exert an attractive or repulsive force on the moth?</ListItem><ListItem>Shopping centres create potential fields attracting consumers to buy things. Which of the following do you think create high or low potentials: Manchester, Birmingham, Edinburgh, Belfast, Netherwallop, Woburn, London, Cardiff and John o&#x2019;Groats?</ListItem></NumberedList></Question><Answer><NumberedList class="lower-alpha"><ListItem>The lamp attracts the moth, and the closer the moth gets to it the more it is attracted.  The lamp exerts an attractive force on the moth.</ListItem><ListItem>I would say that large shopping centres create large potentials attracting people to buy things.  Consequently, all these places have high potentials except Netherwallop, Woburn and John o&#x2019;Groats.</ListItem></NumberedList></Answer></SAQ><Paragraph>It is possible to program a robot to use information from its distance sensors (such as ultrasound sensors) to control its motors just as if the object were exerting a repulsive force &#x2013; analogous to the Braitenberg avoider in RobotLab4.   A target object might on the other hand be thought of as exerting an attractive force, as in the Braitenberg attractor. For example, a robot in need of a recharge might be able to recognise a recharging point in a room and establish this point as exerting an attractive force.</Paragraph><Paragraph>In the Braitenberg vehicle exercise in RobotLab 4, you saw how a greyscale &#x2018;field&#x2019; was used by the robot to judge how far away it was from the centre of a circle.  In a similar way, a robot can use its sensors to calculate a path through an obstacle course if it can somehow create an impression of the (hypothetical) fields associated with various objects.</Paragraph><SAQ><Heading>SAQ 7</Heading><Question><Paragraph>If a robot were at positions A, B and C in the following diagram, draw the paths it would follow in response to the potential field (dark is low repulsive potential, light is high attractive potential).</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f23.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f23.jpg" x_imagewidth="256" x_imageheight="189"/><Caption/></Figure></Question><Answer><Paragraph>Assuming red means low potential, I think robots at A and B would move as shown.   I don&#x2019;t think the robot at C would move, because it&#x2019;s already in the attractive valley.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f24.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f24.jpg" x_imagewidth="256" x_imageheight="188"/><Caption/></Figure></Answer></SAQ><Paragraph>Below is a practical application of the concept of potential field.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f25.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f25.jpg" x_imagewidth="436" x_imageheight="318"/><Caption/><SourceReference><ItemRights><OwnerRef/><ItemRef/><ItemAcknowledgement>Dynamic Design Laboratory, Stanford University website.</ItemAcknowledgement></ItemRights></SourceReference></Figure><Paragraph>In this case the car (US example driving on the right) is forced to change lane by a potential field created by the vehicle in front. The closer it gets, the stronger the force to make it change lane becomes.</Paragraph><Activity><Heading>Read</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\book_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="book_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure>Read Chapter 6, &#x2018;Little Lost Robot&#x2019;, of Asimov&#x2019;s book, and answer the following SAQ.</Paragraph></Question></Activity><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_7\t184_lesson7_f26.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson7_f26.jpg" x_imagewidth="140" x_imageheight="225"/><Caption/></Figure><SAQ><Heading>SAQ 8</Heading><Question><NumberedList class="lower-alpha"><ListItem>Summarise the story of &#x2018;Little Lost Robot&#x2019;.</ListItem><ListItem>Explain how the potentials of the laws interact in this story.</ListItem></NumberedList></Question><Answer><NumberedList class="lower-alpha"><ListItem>Dr Susan Calvin and Dr Peter Bogert are brought to Hyper Base because a specially modified robot had hidden itself amongst a cargo of 62 ordinary robots, and was &#x2018;lost&#x2019;.  Hyper Base uses several robots whose brains are not &#x2018;impressioned&#x2019; with the entire first law &#x2013; a Government secret.  This was necessary to stop the robots interfering when men worked in the dangerous environment of Hyper Base, so that they would let the men come to a limited amount of harm, even though the &#x2018;No robot may harm a human being&#x2019; part of the first law remained.  If the robot could not be found, all 63 must be destroyed, at great cost.  One of the workers had been irritated by one of the &#x2018;Nestor&#x2019; robots, and told it to get lost, invoking the second law.  It had obeyed by hiding amongst the other NS-2 robots.  Susan Calvin does a lot of tests on the robots, but cannot find the rogue Nestor.  One test involves simulating mortal danger with a weight dropped onto a man, to be deflected at the last moment by a force field. This doesn&#x2019;t work because the robots all rush forward to save the person, including the rogue Nestor.  Dr Calvin then adds a complication, the possibility that gamma rays could be used to kill the robots.  The rogue robot understands the physics behind this because it has learnt it from the men.  The other Nestors don&#x2019;t understand and are told that, since the gamma rays would destroy their brains, they should not make the pointless sacrifice.  The rogue Nestor is caught out momentarily, and is the only one to move when the weight falls &#x2013; because it knows that it&#x2019;s not really in danger. On being challenged it attacks Dr Calvin, and is destroyed by a flood of gamma rays.</ListItem><ListItem>The potential for the first law has been modified, allowing the robots to be more tolerant of the danger that the men were in.  The robots had been strictly ordered to stay out of the gamma fields between them and Dr Calvin, since they should not make a pointless sacrifice to try to save her from the falling object. The rogue Nestor had the first law modified so that it could let a human come to harm, but disguised this by appearing to obey that part of the second law.  This combined with its knowledge of physics caused the robot to move forwards when Dr Calvin was in danger, while all the other robots remained stationary.</ListItem></NumberedList></Answer></SAQ></Session><Session><Title>7.5  RobotLab7</Title><Paragraph>In this session you will be doing lab activities using RobotLab7.</Paragraph><Activity><Heading>RobotLab</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\robotlab_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="robotlab_42.png" x_imagewidth="42" x_imageheight="35"/></InlineFigure>Download the lab guide for <olink targetdoc="RobotLab7">RobotLab7</olink>. Work through the guide carefully and do the activities.</Paragraph></Question></Activity></Session><Session><Title>7.6 Summary </Title><Paragraph>In this lesson you have considered optimisation in problem solving, and how this can apply to the development of robots. In particular, you have considered:</Paragraph><BulletedList><ListItem>optimisation in terms of searching for good solutions to problems, and the ways in which the spaces of all possibilities can be searched. This includes structuring the search space, as in the holiday selection problem, or incremental hill-climbing towards better solutions in a landscape representing the goodness of solutions;</ListItem><ListItem>the travelling salesman problem, which is a classic problem in artificial intelligence because, even for small numbers of cities, it has an astronomical number of solutions;</ListItem><ListItem>genetic algorithms, which can be used to search spaces in which the solutions can be represented as sequences of numbers &#x2013; in the case of the travelling salesman problem, the numbering of the cities along a route;</ListItem><ListItem>game theory, and how choices can be made in situations in which the players learn from previous experiences;</ListItem><ListItem>how potential fields can be used to create imaginary forces to navigate robots and explain the interactions between Asimov&#x2019;s laws.</ListItem></BulletedList><Paragraph>Optimisation is a large field of great importance in robotics, and also in human planning and management. Although we have only been able to scratch the surface here, we hope this has given you an insight into a research area that will have a big impact on the development of robots in the future.</Paragraph><StudyNote><Heading>Where next:</Heading><Paragraph>This is the end of Lesson 7. When you are ready go on to <olink targetdoc="Lesson 8">Lesson 8</olink>.</Paragraph></StudyNote></Session></Unit><settings><numbering><Session autonumber="false"/><Section autonumber="false"/><SubSection autonumber="false"/><SubSubSection autonumber="false"/><Activity autonumber="false"/><Exercise autonumber="false"/><Box autonumber="false"/><CaseStudy autonumber="false"/><Quote autonumber="false"/><Extract autonumber="false"/><Dialogue autonumber="false"/><KeyPoints autonumber="false"/><Reading autonumber="false"/><StudyNote autonumber="false"/><Example autonumber="false"/><Verse autonumber="false"/><SAQ autonumber="false"/><ComputerDisplay autonumber="false"/><Summary autonumber="false"/><ProgramListing autonumber="false"/><ITQ autonumber="false"/><Tables autonumber="false"/><Figures autonumber="false"/><MediaContent autonumber="false"/><Chemistry autonumber="false"/></numbering><discussion_alias>Discussion</discussion_alias><session_prefix/></settings></Item>
