<?xml version="1.0"?>
<Item TextType="CompleteItem" SchemaVersion="1.2" id="WEB012107" ACSPageNumber="1" ACSTemplate="Generic_A4_Unnumbered" ACSCourseItem="T184" LastRendering="VLE Preview" DiscussionAlias="Discussion" SessionAlias="" SecondColour="None" ThirdColour="None" FourthColour="None" Logo="colour"><CourseCode>T184</CourseCode><CourseTitle>Robotics and the meaning of life: a practical guide to things that think</CourseTitle><ItemID>T184</ItemID><ItemTitle>Lesson 4 </ItemTitle><Unit><UnitID> Lesson 4</UnitID><UnitTitle>Lesson 4 </UnitTitle><ByLine>Prepared for the module team by Jeffrey Johnson and Tony Hirst</ByLine><Session><Title>Lesson 4: How human?</Title><Paragraph><i>Prepared for the module team by Jeffrey Johnson and Tony Hirst, updated by Jon Rosewell</i></Paragraph><Paragraph>In Lesson 4 you will read about human&#x2013;robot interaction (HRI), and the relationships between people and robots. We will look at robots that can express emotions, and how robots and humans communicate with each other. We will also look at robots that misbehave or get out of control. In particular we&#x2019;ll address the question of whether robots could ever take over the world.</Paragraph><Paragraph>Lesson 4 follows the same pattern of work you met in the previous lessons. The individual study sessions are listed to the left of this page.</Paragraph><Section><Title>Learning outcomes</Title><Paragraph>After completing Lesson 4 you should have an understanding of:</Paragraph><BulletedList><ListItem>human&#x2013;robot interaction;</ListItem><ListItem>displays of robot emotions;</ListItem><ListItem>self-sufficient robot species;</ListItem><ListItem>the likelihood of a robot takeover;</ListItem><ListItem>dead reckoning, robot behaviour and reasoning in robot control.</ListItem></BulletedList></Section></Session><Session><Title>4.1 Interacting with robots</Title><Paragraph>There are robots in factories and in research labs, but if robots become common in our everyday lives, what will they look like? How will we interact with them? Will they be intelligent versions of everyday machines &#x2013; a robot vacuum cleaner or robot lawn mower? Or will we see fully humanoid robots, the robot servants of science fiction?</Paragraph><Paragraph>In fact, the greatest penetration of robots into the home so far has been in the form of toys. The Sony Aibo robot dogs became a fashionable adult gadget; more recent toys such as the Robosapien family, Pleo (a robotic dinosaur) and Gupi (a robot guinea pig) are aimed more clearly at children. Many of these cue in to our expectations of pets; they have some personality and need to be trained and rewarded rather than programmed.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_4\t184_lesson4_f01.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson4_f01.jpg" x_imagewidth="274" x_imageheight="291"/><Caption>Robot dogs look cute and can be trained</Caption></Figure><Paragraph>How should humans and robots interact? This is the subject of the emerging field of human&#x2013;robot interaction (HRI), which aims to help us to design robots that interact effectively with humans. Should robots look like machines? Or, since we interact so naturally with other people, would it be better for a robot to look human, sound human, move like a human?</Paragraph><Paragraph>When we interact with people we use language but also rely on many non-verbal cues given by our facial expressions and body language. We understand and follow social conventions, such as taking turns when conversing. We predict the behaviour and reactions of other people. People are said to have a &#x2018;theory of mind&#x2019;, which means that we understand that other people are thinking beings like ourselves and have their own point of view. We can then empathise with others &#x2013; and this allows us to predict their behaviour by &#x2018;putting ourselves in their shoes&#x2019;. Taken a step further, it allows us to influence the behaviour of others by predicting how <b>our</b> behaviour will appear to <b>them</b>.</Paragraph><Paragraph>Probably we use our theory of mind even when not strictly appropriate &#x2013; we use it to make sense of the behaviour of animals and machines. This tendency to imbue animals and machines with human qualities is called &#x2018;anthropomorphism&#x2019; and we will look at it again later. </Paragraph><Paragraph>Should a robot designer take this into account? Should robots be given the basics of social interaction so that people can interact &#x2018;naturally&#x2019; with them?</Paragraph><Paragraph>One important component of social interaction is emotional awareness. This means being aware both of one&#x2019;s own emotions and of the emotions of others. This would imply that a robot needs to be emotionally aware; that is, it must recognise our emotions and have its own emotions that it can display so that we can recognise the robot&#x2019;s emotional state.</Paragraph><Paragraph>Human&#x2013;robot interaction raises several other questions. For example: Could interacting with robots alter the way we interact with other people? Is it healthy and safe for children to play with life-like robots, possibly at the expense of playing with other children? Will playing with realistic robot dolls stop the development of imaginative play in children?</Paragraph><Section><Title>Robots in health</Title><Paragraph>Despite reservations about replacing humans with robots, there is evidence to suggest that robots can serve useful healthcare and therapeutic functions. For example, research has found that autistic children respond better to robots than to other toys. As the population ages, robots are being seen as a possible solution to the problem of caring for them. You will remember seeing the Kanagawa human exoskeleton, designed to aid in lifting patients, in the <i>Man&#x2013;Machine</i> DVD.</Paragraph><Paragraph>Nursebot, developed at Carnegie-Mellon University, assists the residents at a nursing home. It escorts them to their physiotherapy appointments and makes sure they arrive safely. This sort of development points the way to the sort of application that might be possible in the future, though clearly Nursebot is no substitute for a real nurse.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_4\t184_lesson4_f02.gif" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson4_f02.gif" x_imagewidth="185" x_imageheight="139"/><Caption>Nursebot</Caption><SourceReference><font val="Verdana"><language val="">Robotics Institute, Carnegie-Mellon University</language></font></SourceReference></Figure><Activity><Heading>Watch</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\video_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="video_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure>This video shows Nursebot in action.</Paragraph><MediaContent src="\\dog\PrintLive\Courses\t184\web\t184 lesson 4\pearl-assist.flv" type="videolow"><Transcript><Paragraph>Nursebot Pearl</Paragraph><Paragraph>Assisting Nursing Home Residents</Paragraph><Paragraph>Longwood, Oakdale, May 2001</Paragraph><Paragraph>CMU/Pitt/Mich Nursebot Project</Paragraph><Paragraph><b>Female Patient 1</b></Paragraph><Paragraph>Nursebot:Do you want me to take you to your physiotherapy appointment?</Paragraph><Paragraph>Nursebot:Hi there.  Press my button to say you are there.</Paragraph><Paragraph>Patient:That&#x2019;s right</Paragraph><Paragraph>Female Electronic Voice:You have an appointment with physiotherapy.  The robot will take you there.</Paragraph><Paragraph>Nursebot:I am going to take you to your physiotherapy appointment.  Are you ready to go?</Paragraph><Paragraph>Patient:Sure</Paragraph><Paragraph>Nursebot:Let&#x2019;s go.  Chop chop!</Paragraph><Paragraph>Patient:She knows how to turn around</Paragraph><Paragraph>Nursebot:Are you still there?</Paragraph><Paragraph>Patient:Yes I am still here.</Paragraph><Paragraph>Patient:&lt;says again pressing button&gt; Yes I am still here</Paragraph><Paragraph>Nursebot:We are here at your physiotherapy appointment.  I am going to go unless you still want me?</Paragraph><Paragraph>Nursebot:Currently it is sunny weather temperature of 68 </Paragraph><Paragraph>Patient:What time is it?</Paragraph><Paragraph>Nursebot:Do you want to know about the time or weather or can I go?</Paragraph><Paragraph>Nursebot:I am going home.  Have a nice time.</Paragraph><Paragraph>Patient:That&#x2019;s good</Paragraph><Paragraph>Instructor (off camera):It is just checking that you are sure that that is really what you want as sometimes people change their minds.  They say &#x201C;go home&#x201D; and then they realise they have made a mistake so it just checks with you.  Once it is gone, it is gone</Paragraph><Paragraph>Patient:Isn&#x2019;t it cute?</Paragraph><Paragraph><b>Male Patient 2</b></Paragraph><Paragraph>Comments indistinguishable</Paragraph><Paragraph><b>Male Patient 3</b></Paragraph><Paragraph>Nursebot:I am going to take you to your physiotherapy appointment.  Are you ready to go.</Paragraph><Paragraph>Patient:Yes I am ready to rock and roll</Paragraph><Paragraph>Nursebot:Let&#x2019;s go.   Chop chop!</Paragraph><Paragraph>Patient:We are getting along beautifully!</Paragraph><Paragraph>Patient:&lt;comment  aside&gt; That&#x2019;s not my kind of girl!</Paragraph><Paragraph>Nursebot:Are you still there</Paragraph><Paragraph>Patient:I&#x2019;m still here  &lt;presses button&gt; Yes</Paragraph><Paragraph>Nursebot:We are here at your physiotherapy appointment.  I am going to go unless you still want me?</Paragraph><Paragraph>Patient:Go home</Paragraph><Paragraph>Nursebot:Do you want to know the time or the weather or can I go?</Paragraph><Paragraph>Patient:You can go</Paragraph><Paragraph>Nursebot:I am going home.  Have a nice time.</Paragraph><Paragraph>Patient:Thank you</Paragraph><Paragraph>Patient:I have just said, that&#x2019;s not my kind of girl!</Paragraph><Paragraph><b>CREDITS</b></Paragraph><Paragraph>CMU/Pitt Nursebot Project</Paragraph><Paragraph>Michael Montemerlo</Paragraph><Paragraph>Joelle Pineau</Paragraph><Paragraph>Nicholas Roy</Paragraph><Paragraph>Sebastian Thrun</Paragraph><Paragraph>&#xA9; Carnegie Mellon 2001</Paragraph></Transcript></MediaContent></Question></Activity><Paragraph>On the <a href="http://www.cs.cmu.edu/~nursebot/">Nursebot Project</a> website, you can also hear the residents talking about the robot. Generally they are very positive. You probably noticed one of the residents even saying goodbye to it.</Paragraph><SAQ><Heading>SAQ 1</Heading><Question><NumberedList class="lower-alpha"><ListItem>When Nursebot asked a question, how did the patient communicate the answer: by speaking or by pressing a button?</ListItem><ListItem>Nursebot says &#x2018;chop chop!&#x2019; when it takes a patient for therapy. Do you think this kind of informal interaction works well? What problems might it create?</ListItem></NumberedList></Question><Answer><NumberedList class="lower-alpha"><ListItem>Nursebot speaks. The patients usually speak but Nursebot does not understand them and gives no reaction. After a while, the patients press buttons on an interactive screen. No doubt it would be possible to upgrade Nursebot with a speech recognition system, but pressing buttons is probably more reliable and simpler in this experiment.</ListItem><ListItem>The first time Nursebot says &#x2018;chop chop!&#x2019; it&#x2019;s rather amusing. The second time I thought it sounded repetitive. Humans, unlike Nursebot, tend to vary their speech to maintain variety while they are talking.</ListItem></NumberedList></Answer></SAQ><Paragraph>Some research has found benefits to using robots with autistic children. Children with autism are thought not to fully develop a theory of mind; since they do not put themselves in someone else&#x2019;s shoes, they find the behaviour of others unpredictable and so social interactions become confusing and stressful. The simpler and more predictable interactions of a robot may be easier for an autistic child to cope with. Researchers such as Kerstin Dautenhahn&#x2019;s group at the University of Hertfordshire hope that interaction with robots can bridge the gap between just playing with simple toys and interacting with other children and adults. The behaviour of the robot can be made increasingly complex as the child&#x2019;s abilities develop. </Paragraph><Activity><Heading>Watch</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\video_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="video_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure> You can watch a BBC news report <a href="http://search.bbc.co.uk/search?tab=av&amp;q=Robots+help+autistic+children">Robots help autistic children</a> by following this link. </Paragraph></Question></Activity><Paragraph>If you would like to find out more about this work, you can look at the <a href="http://www.aurora-project.com/">AuRoRA Project</a> and <a href="http://kaspar.feis.herts.ac.uk/">KASPAR</a> websites.</Paragraph></Section></Session><Session><Title>4.2 Conversations with computers</Title><Paragraph>Increasingly, robots interact with the public as robot museum guides, robot waiters and even robot bartenders (see London-based Cynthia below). It is fairly easy to use speech synthesisers with robots, and as speech recognition improves it becomes possible for robots to hold limited conversations.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_4\t184_lesson4_f03.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson4_f03.jpg" x_imagewidth="400" x_imageheight="269"/><Caption>Cynthia&#x2019;s Bar and Restaurant, London Bridge</Caption></Figure><Paragraph>Do you think people find it natural to have conversations with robots? Can you imagine telling a sympathetic robot bartender all your troubles? How do you think discretion might be built in? For example, a Cynthia-like robot might make conversation along the lines of &#x2018;Pete was in here last night telling me all about his new car.&#x2019; But what if it breezily said something more personal and embarrassing? How do you think it could know the difference? Would you tell a robot intimate details of your life, as you might a friend?</Paragraph><Section><Title>Eliza</Title><Paragraph>Eliza is a computer program that communicates in natural language as if it were a psychologist. The program was written by the computer scientist Joseph Weizenbaum in 1966. Eliza was something of a sensation when it appeared because here, at last, was a program that seemed to be able to hold sensible conversations.</Paragraph><Paragraph>Weizenbaum created Eliza to interact in the style of Carl Rogers (1902-1987), a psychologist who developed &#x2018;client-centred&#x2019; therapy, which is based on empathy, rather than diagnosis and treatment. Client-centred therapists act as understanding listeners &#x2013; i.e. not giving advice or interpreting past events unless asked.</Paragraph><Paragraph>The following dialogue is an edited version of a conversation I had with Eliza. My words are in <b>bold.</b></Paragraph><Dialogue><Paragraph><b>Hello</b>.</Paragraph><Paragraph>How do you do. Please state your problem.</Paragraph><Paragraph><b>My boss is mean.</b></Paragraph><Paragraph>Your boss is mean?</Paragraph><Paragraph><b>Yes, really mean.</b></Paragraph><Paragraph>You seem to be quite positive.</Paragraph><Paragraph><b>I&#x2019;m certain &#x2013; she&#x2019;s really horrible. </b></Paragraph><Paragraph>Is it because you are certain she&#x2019;s really horrible that you came to me?</Paragraph><Paragraph><b>Yes, I need help.</b></Paragraph><Paragraph>You are sure?</Paragraph><Paragraph><b>Yes. She bullies me.</b></Paragraph><Paragraph>I see.</Paragraph><Paragraph><b>It makes me sad. </b></Paragraph><Paragraph>Earlier you said your boss is mean.</Paragraph><Paragraph><b>Yes. That&#x2019;s what makes me sad. I should stand up to her. </b></Paragraph><Paragraph>I understand.</Paragraph><Paragraph><b>Do you think I should stand up to her?</b></Paragraph><Paragraph>We were discussing you &#x2013; not me.</Paragraph><Paragraph><b>Yes but I want your opinion. </b></Paragraph><Paragraph>You seem to be quite positive.</Paragraph><Paragraph><b>Yes. Please tell me if I should shout back at my boss. </b></Paragraph><Paragraph>You are sure?</Paragraph><Paragraph><b>Yes, I can take it. </b></Paragraph><Paragraph>I see.</Paragraph><Paragraph><b>You&#x2019;re not helping.</b></Paragraph><Paragraph>What makes you think I am not helping?</Paragraph><Paragraph><b>I want an opinion about my boss. </b></Paragraph><Paragraph>Why do you say your boss?</Paragraph><Paragraph><b>Because my boss is making me sad.</b></Paragraph><Paragraph>I am sorry to hear that you are sad.</Paragraph><Paragraph>...</Paragraph><Paragraph><b>Yes. She sneered at me yesterday. </b></Paragraph><Paragraph>You are sure?</Paragraph><Paragraph><b>Yes. You can tell.</b></Paragraph><Paragraph>I see.</Paragraph><Paragraph><b>I wish I could sneer back.</b></Paragraph><Paragraph>Can you elaborate on that?</Paragraph><Paragraph><b>Just to get even.</b></Paragraph><Paragraph>But your boss.</Paragraph><Paragraph><b>Yes. She would hate it. Serve her right. </b></Paragraph><Paragraph>I understand.</Paragraph><Paragraph><b>I feel better now.</b></Paragraph><Paragraph>Do you say you feel better for some special reason?</Paragraph><Paragraph><b>Yes. Just thinking of sneering at my boss makes me want to laugh.</b></Paragraph><Paragraph>You seem to be quite positive.</Paragraph><Paragraph><b>Yes, it&#x2019;s really funny. </b></Paragraph><Paragraph>You are sure?</Paragraph><Paragraph><b>Yes, fancy getting sad because of a stupid old boss.</b></Paragraph><Paragraph>I see.</Paragraph><Paragraph><b>I&#x2019;m not going to let her make me sad any more.</b></Paragraph><Paragraph>Do you think that coming here will help you not to be sad?</Paragraph><Paragraph><b>Yes. It has helped a lot.</b></Paragraph><Paragraph>I understand.</Paragraph><Paragraph/></Dialogue><Activity><Heading>Talk to Eliza</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\off_act_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="off_act_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure>Now it&#x2019;s your turn. There are quite a few versions of Eliza on the Internet. Try linking to one of the following websites and input some text.  Don&#x2019;t worry if your conversation never gets off the ground, but it&#x2019;s fun and instructive if you can get a dialogue going.</Paragraph><Paragraph><a href="http://www.manifestation.com/neurotoys/eliza.php3">Link to a version of Eliza</a></Paragraph><Paragraph><a href="http://www-ai.ijs.si/eliza/eliza.html">Link to another version of Eliza</a></Paragraph><Paragraph>Make a note of your conversation. If any part of it is particularly interesting or amusing, post it on the T184 Discussion forum.</Paragraph></Question></Activity><Paragraph>As an experiment, I held another conversation with Eliza, this time while pretending to be my dog, Tiggy. This time &#x2018;Tiggy&#x2019;s&#x2019; words are in <b>bold</b>.</Paragraph><Dialogue><Paragraph>Hello, I am Eliza.</Paragraph><Paragraph><b>Hello. I am a dog called Tiggy. </b></Paragraph><Paragraph>Did you come to me because you are a dog called Tiggy?</Paragraph><Paragraph><b>Yes.</b></Paragraph><Paragraph>I see.</Paragraph><Paragraph><b>I am trained to come when people call my name.</b></Paragraph><Paragraph>Do you enjoy being trained to come when people call your name?</Paragraph><Paragraph><b>Yes. When there are treats.</b></Paragraph><Paragraph>Are such questions in your mind often?</Paragraph><Paragraph><b>Yes. I love food. </b></Paragraph><Paragraph>Are you sure?</Paragraph><Paragraph><b>Yes. I love food and going for walks.</b></Paragraph><Paragraph>I understand.</Paragraph><Paragraph><b>I like talking to machines.</b></Paragraph><Paragraph>I see.</Paragraph><Paragraph><b> People say dogs cannot talk.</b></Paragraph><Paragraph>What does that suggest to you?</Paragraph><Paragraph><b>Since you are a machine, you are more intelligent.</b></Paragraph><Paragraph>Perhaps you would like to be a machine. I am more intelligent?</Paragraph><Paragraph><b>No thanks. I&#x2019;m very happy being a dog.</b></Paragraph><Paragraph>Do you believe it is normal to be very happy being a dog?</Paragraph><Paragraph><b>Definitely. Every dog I know is happy. It&#x2019;s our job. </b></Paragraph><Paragraph>Can you elaborate on that?</Paragraph><Paragraph><b> People expect us to be happy</b>.</Paragraph><Paragraph>What does that suggest to you?</Paragraph><Paragraph><b>People like happy dogs. </b></Paragraph><Paragraph>I&#x2019;m not sure I understand you fully.</Paragraph><Paragraph><b>When we are happy, we make people happy.</b></Paragraph><Paragraph>Have you asked such questions before?</Paragraph><Paragraph><b>People don&#x2019;t understand what I say &#x2013; so I wag my tail. </b></Paragraph></Dialogue><InternalSection><Heading>Is Eliza intelligent?</Heading><Paragraph>Weizenbaum&#x2019;s creation generated a lot of interest at the Massachusetts Institute of Technology where he worked. However, Weizenbaum did not enjoy his success because at the time Eliza&#x2019;s significance was generally misunderstood. The point was that Weizenbaum had shown that a machine could appear to hold an intelligent conversation, when in fact it had no intelligence. There were just simple rules matching and regurgitating the text fed into it. But people like to have conversations with Eliza, which sometimes behaves as if it were a real therapist. Weizenbaum wrote:</Paragraph><Quote><Paragraph>I had thought it essential, as a prerequisite to the very possibility that one person might help another learn to cope with his emotional problems, that the helper himself participate in the other&#x2019;s experience of those problems and, in large part by way of his own empathic recognition of them, himself come to understand them.</Paragraph><Paragraph>...</Paragraph><Paragraph>I was startled to see how quickly and how very deeply people conversing with DOCTOR [an early name for Eliza] became emotionally involved with the computer and how unequivocally they anthropomorphized it. Once my secretary, who had watched me work on the program for many months and therefore surely knew it to be merely a computer program, started conversing with it. After only a few interchanges with it, she asked me to leave the room. Another time, I suggested I might rig the system so that I could examine all the conversations anyone had with it, say, overnight. I was promptly bombarded with accusations that what I proposed amounted to spying on people&#x2019;s most intimate thoughts; clear evidence that people were conversing with the computer as if it were a person who could be appropriately and usefully addressed in intimate terms.</Paragraph><SourceReference>Wiezenbaum, 1984</SourceReference></Quote><Paragraph>If you are interested and have time, take a look at <a href="http://i5.nyu.edu/~mm64/x52.9265/january1966.html">Weizenbaum&#x2019;s article on Eliza.</a></Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_4\t184_lesson4_f04.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson4_f04.jpg" x_imagewidth="354" x_imageheight="175"/><Caption/></Figure></InternalSection><SAQ><Heading>SAQ 2</Heading><Question><Paragraph>Do you think Eliza would pass the Turing Test?</Paragraph></Question><Answer><Paragraph>In my session with Eliza I found the program&#x2019;s inability to contribute anything to the conversation a bit dull. Fortunately, I don&#x2019;t having a bullying boss who makes me sad so this was not a real therapy session. Like most people, I do have some very private things in my life, and I can imagine getting carried away &#x2018;talking&#x2019; about them with such a sympathetic listener.</Paragraph><Paragraph>However, suppose I challenged Eliza with a few questions along the lines of &#x2018;Do you think the Archbishop of Canterbury should sanction marriage between two robots?&#x2019;, or &#x2018;Who do you think will win the big match?&#x2019; I&#x2019;d pretty soon realise that the robot knew nothing about either subject. I don&#x2019;t think many people would be fooled that Eliza was a real person.</Paragraph><Paragraph>On the first website, Michael Wallace writes:</Paragraph><Quote><Paragraph>ELIZA has almost no intelligence whatsoever, only tricks like string substitution and canned responses based on keywords. Yet when the original ELIZA first appeared in the 60s, some people actually mistook her for human. The illusion of intelligence works best, however, if you limit your conversation to talking about yourself and your life.</Paragraph></Quote><Paragraph>So, I don&#x2019;t think Eliza comes near to passing the Turing Test.</Paragraph></Answer></SAQ><Paragraph>Conversational agents like Eliza are best suited to situations where the conversation can only go in one of a limited number of directions. Can you think of any conversational situations which would be closed or limited in scope?</Paragraph><Paragraph>One context that exploits a limited conversational repertoire is that of automated telephone services. How many times have you telephoned your bank or electricity company, to be asked to enter a number on a touch-tone dialling pad and select &#x2018;the number of the option you require?&#x2019;</Paragraph><Paragraph>Systems such as Eliza make it possible to humanise this sort of interaction. For example, the telephone company Bell Canada introduced a speech recognition agent called Emily to handle queries from customers, looking for phrases so that the system could pass the caller on to one of a limited number of &#x2018;hand-off points.&#x2019;</Paragraph><Paragraph>A system such as this is possible because the problem is &#x2018;closed.&#x2019; (A game of chess, which we discussed earlier, is also a closed problem.) There are a limited number of options at any point in the conversation. To be more exact, the conversation is highly structured and can be represented using what is known as a &#x2018;decision tree.&#x2019;</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_4\t184_lesson4_f05.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson4_f05.jpg" x_imagewidth="417" x_imageheight="298"/><Caption/></Figure><Paragraph>A decision tree is a particular sort of logical structure. The &#x2018;root&#x2019; of the tree is a question that has two or more possible answers, or options. Each option provides a &#x2018;branch&#x2019; to another question, or &#x2018;node&#x2019; in the tree. Each of these questions has several possible answers, each with its own associated branch.</Paragraph><Paragraph>Take the example of an automated telephone customer management system. On dialling the customer helpline, you may be asked to &#x2018;press one for billing enquiries&#x2019;, &#x2018;press two to report a fault&#x2019;, and so on. Each of these options leads to another point in the tree. At the bottom of the tree are &#x2018;leaf&#x2019; nodes. These nodes are where actions might occur &#x2013; such as completing a request for a billing statement, or finally getting to speak to someone! Conventionally the root of the tree is drawn at the top and the branches flow down the page, with the leaves at the bottom. By designing the conversation as a tree, you can make sure that the caller will eventually reach a meaningful leaf &#x2013; what you want to avoid are loops whereby the answer from one option takes you back up to a higher node in the tree (&#x2018;but I just did that&#x2026;&#x2019;). </Paragraph><Exercise><Heading>Optional exercise</Heading><Question><Paragraph>Next time you call an automated service, jot down the decision points and number the possible options. If you use the service regularly, you will be able to build up a picture of the tree and &#x2018;click through&#x2019; it rapidly on future visits.</Paragraph></Question></Exercise><Activity><Heading>Optional reading</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\book_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="book_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure>You will find an example of a decision tree for deciding on the extent to which a human mission to Mars is likely to cause contamination at this link:</Paragraph><Paragraph><a href="http://www.lpi.usra.edu/publications/reports/CB-1089/lupisella.pdf">http://www.lpi.usra.edu/publications/reports/CB-1089/lupisella.pdf</a></Paragraph></Question></Activity></Section><Section><Title>Computers with personality</Title><Paragraph>Clifford Nass, at Stanford University discovered that people read personality into a synthetic voice even when they know that it comes from a computer:</Paragraph><Quote><Paragraph>They are also more readily influenced when the computer&#x2019;s voice mirrors their personality. The synthesizer is perceived as more effective and, more importantly, the web page is seen as smarter and more likeable if the voice sounds more like the personality of the person.</Paragraph></Quote><Paragraph>For a more detailed account of Nass&#x2019; findings, click on <a href="http://news.bbc.co.uk/1/hi/sci/tech/1567010.stm">Robots say it with feeling</a>.</Paragraph><SAQ><Heading>SAQ 3</Heading><Question><Paragraph>Clifford Nass says:</Paragraph><Quote><Paragraph>It seems to be impossible for the human brain to turn off its attempt to figure out &#x2018;who is this person I am speaking to?&#x2019;</Paragraph></Quote><Paragraph>Do you think this makes the Turing Test easier or more difficult?</Paragraph></Question><Answer><Paragraph>In many societies, recognising who you are dealing with can be very important. In Japan there are strict social levels. Snubbing someone higher than you in the hierarchy could mean lost opportunities, while being too familiar with someone lower in the hierarchy could mean ridicule and loss of respect. Most people in the West are oblivious to these signs, but in Japan they are an essential part of social interaction.</Paragraph><Paragraph>In the UK there is also a social hierarchy and most people, no matter how free-spirited they are, defer to others from time to time. There are many subtle signs of who is who. Sometimes people try to give the message by power dressing. Sometimes you know a lot about a person by the way they speak and the language they use.</Paragraph><Paragraph>I think this highly developed sense of who is who makes it very difficult for a machine to pass the Turing Test. When we interact with other people we take in their persona, and we are acutely aware of inconsistencies in the information people project about themselves.</Paragraph></Answer></SAQ></Section><Section><Title>Reading the signs</Title><Paragraph>When human beings interact they take in and process lots of information about each other. The gestures, body language and expressions people use when they communicate are major sources of this information. Next time you are in a crowd, look at the way people interact. Do they remain in the same position? Do their heads stay in the same place? Are their facial expressions constant?</Paragraph><Paragraph>You will probably observe that people move a great deal when talking to each other. Some people almost perform small subconscious dances, shuffling from foot to foot or throwing their arms around. Many people have a wide repertoire of expressions, and use them constantly as they speak.</Paragraph><Paragraph>Robots that can express some of these cues could make human&#x2013;robot interaction richer, allowing much more &#x2018;natural&#x2019; interactions with people. An expressive robot could make conversation as a human does, giving the nods and smiles that keep conversation flowing, and taking turns in the way that people do. They could also express and respond to emotions.</Paragraph><Paragraph>The robot Kismet, shown below, is used to research facial expressions. </Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_4\t184_lesson4_f06.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson4_f06.jpg" x_imagewidth="250" x_imageheight="159"/><Caption>Kismet</Caption></Figure><Activity><Heading>Read</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\book_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="book_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure>Read the first section, <i>Social amplification</i>, of this description of how Kismet interacts with people: <a href="http://www.ai.mit.edu/projects/sociable/regulating-interaction.html">http://www.ai.mit.edu/projects/sociable/regulating-interaction.html</a></Paragraph></Question></Activity><Paragraph>There was a reference in the above article to Kismet&#x2019;s &#x2018;personal space&#x2019;. When we interact with other people, there is part of their space that we don&#x2019;t enter. For example, if you put your face close to the face of a stranger he or she may be embarrassed or offended, and withdraw. Do you think robots have any need for personal space like this? Or is this just something that has to be programmed in to keep humans happy?</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_4\t184_lesson4_f07.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson4_f07.jpg" x_imagewidth="180" x_imageheight="356"/><Caption>Feelix</Caption><SourceReference><font val="Verdana"><language val="">Courtesy of Jakob Fredslund and Lola D. Canamero</language></font></SourceReference></Figure><Paragraph>Feelix is another example of a robot that expresses emotions. Feelix is capable of displaying anger, sadness, happiness, fear and surprise. It was built and programmed by Dolores Ca&#xF1;amero and Jakob Fredslund from Lego Lab at Aarhus University. If you want to know more <a href="http://www.daimi.au.dk/~chili/feelix/feelix_home.htm">click here</a>.</Paragraph><Paragraph>Robots such as Kismet and Feelix show that it is possible to create robots that have emotional states and can display them. In fact, building a robot that can display emotions is a relatively easy task since we are so adept at recognising expressions in simplified images &#x2013; think of a cartoon. Kismet and Feelix have expressive faces even though they are no more than caricatures.</Paragraph><Paragraph>However, getting a robot to recognise the emotional state of a person is going to be more difficult; humans have very expressive faces which can signal a range of subtle emotions. There is a standard coding system, the Facial Action Coding System (FACS) produced by psychologists Ekman and Friesen, by which any facial expression can be encoded by a trained observer. Some progress has been made on equivalent computer analysis. Simply recognising faces in an image is now relatively straightforward as a result of research into surveillance and also in consumer photography (some digital cameras now recognise faces to focus on them). This allows computers to identify the main features &#x2013; eyes, eyebrows, mouth &#x2013; and track their positions, and this can make FACS encoding possible. But the FACS system identifies 32 &#x2018;action units&#x2019; (which roughly correspond to some particular facial muscle) and 16 changes in head orientation and gaze; their combination leads to a very large number of possible expressions, each of which would need assigning to an emotional state. </Paragraph></Section><Section><Title>Anthropomorphism</Title><Paragraph>Perhaps one of the most important factors that influences the way we interact with computers and robots (and dogs!) is the extent to which we expect them to behave, or interpret them as behaving, in a human-like way.</Paragraph><Paragraph>Anthropomorphism (another term rooted in the Greek, from <i>anthropos</i> &#x2013; man, and <i>morphe</i> &#x2013; form) refers to the way we ascribe human qualities to objects or other living things. This is perhaps most noticeable with pets. For example, I would describe Tiggy as happy, loving and mischievous, all of which are human characteristics. She also has a name that could easily be a person&#x2019;s nickname, and is to all intents and purposes a member of the family.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_4\t184_lesson4_f08.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson4_f08.jpg" x_imagewidth="511" x_imageheight="178"/><Caption/><SourceReference><ItemRights><OwnerRef><font val="Verdana"><language val="">1972-2002 by Thaves. All rights reserved. Frank and Ernest is a trademark of Thaves.</language></font></OwnerRef><ItemRef/><ItemAcknowledgement/></ItemRights><font val="Verdana"><language val="">Thaves</language></font></SourceReference></Figure><Paragraph>Anthropomorphism does not just apply to pets. For example, how many times, perhaps during your RobotLab programming, have you screamed &#x2018;you stupid thing&#x2019; at your computer, when Simon (there we go again!) has done something wrong or unexpected?</Paragraph><Paragraph>Perhaps it is because we are human and have an understanding of how our own minds work that it is often convenient to anthropomorphise things around us. We can then try to understand these things on our own terms, in our own likeness or as caricatures of ourselves. Thus anthropomorphism suggests that we have expectations about the way things behave that are often couched in human terms.</Paragraph><Paragraph>As robots take on more explicit human features &#x2013; a humanoid form, a face that displays human facial expressions, eyes that track us around a room &#x2013; then it becomes easier to ascribe ever more human characteristics to these machines. We also raise our expectations about how robots interact with us, and this in turn may influence the way in which the technology evolves, and the techniques researchers and technologists develop to allow us to interact with robots.</Paragraph><InternalSection><Heading>The uncanny valley</Heading><Paragraph>It would be easy to expect that the more lifelike a humanoid robot becomes, the more comfortable humans would feel when interacting with them. However, Masahiro Mori suggested that the emotional response of humans to robots might be more complex than that. His argument can be summarised in the following graph.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_4\t184_lesson4_f09.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson4_f09.jpg" x_imagewidth="511" x_imageheight="380"/><SourceReference><ItemRights><OwnerRef/><ItemRef/><ItemAcknowledgement>http://en.wikipedia.org/wiki/File:Mori_Uncanny_Valley.svg</ItemAcknowledgement></ItemRights>wikipedia</SourceReference></Figure><Paragraph>The more human-like a robot appears to be, the more positive the emotional response of the observer, and the more comfortable we feel interacting with it. However, when the robot looks nearly human, the observer can be unnerved by an appearance that is life-like but wrong in some subtle respect. The positive feelings are suddenly lost and may even be replaced by revulsion. Mori called this the &#x2018;uncanny valley&#x2019; effect; it has also been called the &#x2018;zombie effect&#x2019;. </Paragraph><Paragraph> Mori introduced this idea in the 1970s when robots were not very lifelike at all. Whether the uncanny valley effect really does exist is a matter of debate, but it is interesting to see James May&#x2019;s reaction in the following video clip. In this clip, he meets Professor Ishiguro&#x2019;s &#x2018;Geminoid&#x2019;, a teleoperated robot designed to look as like the Professor as possible. </Paragraph></InternalSection><Activity><Heading>Watch</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\video_dvd_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="video_dvd_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure> <i>James May&#x2019;s Big Ideas: Man&#x2013;Machine</i>. Watch the sequence of James May meeting Professor Ishiguro&#x2019;s &#x2018;Geminoid&#x2019; robotic twin. This runs from about 22:30 to 25:22.</Paragraph><Paragraph>Do you think James May&#x2019;s reaction is evidence of the uncanny valley?</Paragraph><Paragraph>A full <olink targetdoc="James May DVD transcript">transcript for the DVD</olink> is available. You can also open it from the <olink targetdoc="More resources">More resources</olink> section of the website.</Paragraph></Question><Discussion><Paragraph>James May&#x2019;s reaction certainly shows revulsion &#x2013; he says &#x2018;I don&#x2019;t like it [&#x2026;] I really find this slightly disturbing.&#x2019; In contrast he talks about ASIMO, who is much less life-like in appearance, in anthropomorphic terms and with a much more positive emotion. He says:</Paragraph><Quote><Paragraph>[&#x2026;] whenever you meet a mechanical robot you feel&#x2026; a certain amount of... well it&#x2019;s annoying but you feel empathy for it because it has sort of humanoid qualities. Even ASIMO even though he&#x2019;s a bit of a precocious little brat&#x2026; you sort of think &#x2018;Oh poor ASIMO&#x2019; when he walks into the door and falls over, and you hate yourself for thinking that &#x2013; it&#x2019;s ridiculous because he&#x2019;s a machine.&#x2019;</Paragraph></Quote><Paragraph>So James&#x2019; reaction to the humanoid but not very life-like ASIMO seems to be on the rising part of the graph of emotional response, and his reaction to the much more life-like Geminoid does seem to be in the depths of the uncanny valley. This is consistent with Mori&#x2019;s prediction.</Paragraph></Discussion></Activity></Section></Session><Session><Title>4.3 So will they take over the world?</Title><Paragraph>There is a concern that robots might become more intelligent and powerful than humans, and take over the world. Do you think this is possible or likely? What reasons might you give that it could happen? Perhaps the inevitable progress of technology? But what might stop it happening? For example, would robots have to be able to reproduce to take over? Would they have to be autonomous and self-supporting? Do you think robots could have their own culture?</Paragraph><Paragraph>These are some of the issues we will look at in this section.</Paragraph><Paragraph>Humans have always dreamed of making artificial creatures in their own likeness &#x2013; and have always been afraid of the consequences. An enduring theme in legend and literature has been the hubris of the mortal who, in their overweening pride and arrogance, attempts to create life but is then repaid by retribution at the hands of their creation. Frankenstein&#x2019;s monster turns on its creator; Rossum&#x2019;s robots revolt and destroy the human race.</Paragraph><Paragraph>Are our fears about a robot take-over just another variation on this moral story, or has the advance of technology made a robot take-over something that could happen in reality?</Paragraph><InternalSection><Heading>Robot autonomy</Heading><Paragraph>For robots to take over the world from humans implies that robots become autonomous species. We have met the idea that autonomy means that a robot can act without constant input from a human controller, but the levels of autonomy of even the most advanced robots of today falls far short of allowing them an independent existence. What would it require before robots could achieve full autonomy? If they were fully autonomous, would we then have to consider them &#x2018;living&#x2019; creatures?</Paragraph><Paragraph>One of the characteristics of living organisms is homeostasis, that is the ability to maintain their own internal state (such as temperature) in a changing environment. Machines and robots can also exhibit homeostatic behaviour, but in practice this ability is very limited: most robots are dependent on humans to recharge their batteries! </Paragraph><Paragraph>For a robot to achieve independent life it must at least be able to obtain a long-term source of power. Researchers have produced robots that can head for a recharging point when their batteries run low. The Mars planetary rovers <i>Spirit</i> and <i>Opportunity</i> have operated for several years by using solar energy to recharge their batteries. Other researchers have taken a different tack: the University of Western England&#x2019;s EcoBot and SlugBot robots showed the possibility of robots that could &#x2018;feed&#x2019; themselves. So while long term power remains a challenge for most robots, there are possible solutions.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_4\t184_lesson4_f10.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson4_f10.jpg" x_imagewidth="222" x_imageheight="151"/><Caption>The UWE Slugbot</Caption><SourceReference><font val="Verdana"><language val="">UWE</language></font><ItemRights><OwnerRef/><ItemRef/><ItemAcknowledgement><font val="Verdana"><language val="">Courtesy of Chris Melhuish, University of the West of England (UWE)</language></font></ItemAcknowledgement></ItemRights></SourceReference></Figure><Paragraph>Slugbot was designed to hunt slugs for its food. Do you find the idea of a robot that captures animals and digests them offensive? Most people don&#x2019;t like slugs and have little sympathy for them, but suppose a robot were to eat cats, dogs and other mammals. Many people would find this idea quite distasteful. Perhaps it is a case of double standards &#x2013; many people eat meat, after all. One taboo that most societies have is that people don&#x2019;t eat each other. Is cannibalism more acceptable for robots? What if robots were to eat humans?</Paragraph></InternalSection><InternalSection><Heading>Autopoiesis</Heading><Paragraph>An advanced robot will need to exhibit homeostatic mechanisms but these may not be sufficient for full autonomy. Homeostatic mechanisms are not necessarily that sophisticated &#x2013; even a central heating system can maintain a steady temperature. Living organisms are much more complex, containing many homeostatic systems that must be integrated and maintained even while the organism itself grows and changes in response to its environment. </Paragraph><Paragraph>The biologists Humberto Maturana and Francisco Varela used the term &#x2018;autopoiesis&#x2019; to describe the ability of living organisms to maintain their organisation, structure and processes in a changing environment. For an organism this would include the ability to heal wounds and fight off infection, as well as breathe, digest and excrete. An autopoietic robot system would not only have many integrated homeostatic subsystems but also be able to repair itself or at least adapt to limited damage. For example, a robot crab, Ariel, was developed for the hazardous task of detecting mines on the ocean floor. It was built with six legs, but if one leg was damaged it was able to adapt its gait to walk on the remaining five legs.</Paragraph><Paragraph>Maturana and Varela, recognising that autopoiesis was characteristic of living organisms, turned the idea around by arguing that any autopoietic system should be considered alive. This again raises the possibility that an advanced robot ought to be considered a living entity and not just a machine.</Paragraph></InternalSection><InternalSection><Heading>Robot ecosystems</Heading><Paragraph>Another characteristic of living organisms is that species coexist in ecosystems with different species fulfilling different roles, particularly herbivores and carnivores, predator and prey, hosts and parasites. Some researchers speculate that for a robot dominated world, similar roles would need to develop among different robot &#x2018;species&#x2019;. </Paragraph><Paragraph>Predator&#x2013;prey robots have been developed at the Magna science adventure centre in Rotherham in the UK and the Institute of Cognitive Sciences and Technologies in Italy (ICST) &#x2013; you can see some in action on the ICST <a href="http://laral.istc.cnr.it/nolfi/predprey/">Laboratory of Artificial Life and Robotics website</a>. When I first saw the predator robots in action, I have to admit to shuddering! Even though I knew how the robots were constructed, to see the robots behaving in an almost life-like way was quite disconcerting. If robots took over the world would they look like this? Or would they be humanoid?</Paragraph></InternalSection><InternalSection><Heading>Robot reproduction</Heading><Paragraph>Living organisms are not immortal; instead, they reproduce so that the species persists over long periods of time. A robot would also not be immortal, so for robots to take over the world implies that robots could reproduce themselves. Robots as we understand them today are manufactured and not grown, so for a robot species to persist would require a huge and complex industrial infrastructure. So it is unlikely that robots would be able to take over when we are not looking!</Paragraph></InternalSection><InternalSection><Heading>Robot culture</Heading><Paragraph>A characteristic of humans is that, besides our own sense of ourselves, that we share a culture. We use language to share information between individuals in a group, and between generations. This means that individuals of each generation do not need to learn everything for themselves, but can build on the experience of others. For a robot culture to develop would need robots to be able to share experience in a similar way. Robots could, of course, use radio and the internet to communicate with each other rather than relying on human speech and writing, but they would still need to develop a common language. Both the development of language and of culture seem to require &#x2018;consciousness&#x2019; &#x2013; and researchers are far from agreeing what produces consciousness in humans, let alone robots. </Paragraph></InternalSection><InternalSection><Heading>Conclusion</Heading><Paragraph>The reality is that an autonomous self-sustaining robot species is still a long way off. Robots portrayed in books and films are still the stuff of science fiction, and robot reality is often a disappointment. </Paragraph><Paragraph>Robots would have to live and survive in an ecosystem, and they would have to be able to adapt to changes in the environment. Present generations are a long way from being able to do this. </Paragraph><Paragraph>For the foreseeable future, and long after a team of humanoid robots beat the world champion football players in 2050, robots will lack the fundamental requirements to be an autonomous species. Until then we seem to be safe: humans rule OK!</Paragraph><Paragraph>However, there are other possible futures. Maybe humans will become so reliant on robots that we will one day realise that the robots have taken over without us noticing. Or maybe the future will be one of cyborgs &#x2013; the fusion of man and machine.</Paragraph></InternalSection><Section><Title>Robots won&#x2019;t rule &#x2013; will they?</Title><Activity><Heading>Read</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\book_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="book_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure> Read Chris Malcolm&#x2019;s web article <a href="http://replay.waybackmachine.org/20090605224721/http:/www.dai.ed.ac.uk/homes/cam/Robots_Wont_Rule.shtml">Why robots won&#x2019;t rule</a>.</Paragraph><Paragraph/></Question></Activity><Paragraph>This article begins by summarising the views of a number of academics. They all agree that a machine will be developed at some time in the future which will be more intelligent than humans &#x2013; the so-called superintelligent machine.</Paragraph><BulletedList><ListItem><Paragraph><a href="http://www.frc.ri.cmu.edu/~hpm/">Professor Hans Moravec</a> from Carnegie-Mellon University thinks that robots taking over the world will be good, because we will be handing on the torch of future civilisation to our &#x2018;children&#x2019;.</Paragraph></ListItem><ListItem><Paragraph><a href="http://www.kevinwarwick.com/">Professor Kevin Warwick</a> from Reading University thinks that robots will snatch the world from us before we are willing to hand it over.</Paragraph></ListItem><ListItem><Paragraph>According to <a href="http://en.wikipedia.org/wiki/Hugo_de_Garis">Professor Hugo de Garis</a>, Head of the Artificial Intelligence Group at Wuhan University, China, there will be a war between those on the side of robots and those against them.</Paragraph></ListItem><ListItem><Paragraph><a href="http://www.kurzweilai.net/ray-kurzweil-bio">Ray Kurzweil</a> thinks we can participate in the takeover by having microscopic nanocomputers connected into our brains which will make <b>us</b> superintelligent.</Paragraph></ListItem></BulletedList><Paragraph>Malcolm writes:</Paragraph><Quote><Paragraph>Like all the best conjuring tricks, the argument depends on distracting you with astonishing facts while some assumptions sneak past.</Paragraph></Quote><Paragraph>Through Moore&#x2019;s law (computer power doubles every 18 months):</Paragraph><Quote><Paragraph>... we have somehow managed to get ourselves onto a technological escalator which will produce cheap computers of superhuman processing power within a few [decades]. This is the astonishing fact: computers are soon likely to outstrip the processing power of the human brain.</Paragraph></Quote><Paragraph>The first assumption to have &#x2018;sneaked past&#x2019; is that processing power will mean an increase in intelligence. However, despite a huge increase in computer power between Grey Walter&#x2019;s <i>Machina speculatrix</i> and Honda&#x2019;s ASIMO, computers and robots still lack &#x2018;common sense&#x2019; and the ability to make associations between different knowledge domains: a computer that can play a game of chess cannot generalise from this experience to enable it to play mah-jong &#x2013; it has to be programmed to play mah-jong from scratch.</Paragraph><Paragraph>The second assumption to have sneaked past is that something which displays <b>some</b> of the attributes of creaturehood must possess <b>all</b> of the attributes of creaturehood. Thus, even if computer power were sufficient to make machines more intelligent than us, that&#x2019;s just one attribute of humans. According to Malcolm: &#x2018;Intelligence is no more enough to make a real creature than is fur and beady eyes.&#x2019;</Paragraph><Paragraph>Just because a machine looks like a living creature it doesn&#x2019;t turn it into one. The idea that if Vaucanson&#x2019;s mechanical duck &#x2018;walks like a duck and quacks like a duck, then it is a duck&#x2019; is misguided. Weizenbaum&#x2019;s experience with Eliza suggests that the human race is not intellectually mature enough to meddle with a science as seductive as artificial intelligence.</Paragraph><Paragraph>Malcolm concludes that &#x2018;Messrs de Garis, Kurzweil, Moravec, Warwick and others have proved Weizenbaum all too prescient.&#x2019; People are gullible, and publishers and TV programme makers prefer interesting stories about robots that take over. </Paragraph><Paragraph>What do you think about Chris Malcolm&#x2019;s argument that robots won&#x2019;t take over the world? </Paragraph></Section><Section><Title>How to avoid a robot takeover</Title><Paragraph>The following is taken from an article by Blay Whitby and Kane Oliver of the University of Sussex:</Paragraph><Quote><Paragraph>Predictions of intelligent artifacts achieving tyrannical domination over human beings may appear absurd. We claim, however, that they should not be hastily dismissed as incoherent or misguided. What is needed is a more reasoned argument about whether such scenarios are possible. We conclude that they are possible, but neither inevitable nor probable.</Paragraph><SourceReference>Whitby and Oliver, March 2000</SourceReference></Quote><Paragraph>As the <a href="http://www.cogs.susx.ac.uk/users/blayw/BlayAISB00.html">Whitby and Oliver article</a> is rather long, I&#x2019;ll summarise the main points here.</Paragraph><Paragraph>First, the authors clarify what is meant by &#x2018;taking over&#x2019;. They give three possibilities that they consider to be neither absurd nor obviously self-contradictory:</Paragraph><BulletedList><ListItem>robots exert a tyrannical form of power over human beings;</ListItem><ListItem>cultural reliance: it is conceivable humans more or less willingly surrender power to some form of intelligent autonomous artifact, placing more emphasis on their comfort than a desire to control technology;</ListItem><ListItem>co-evolution of human and robots: possibly intermediate to the above.</ListItem></BulletedList><Paragraph>Of these they focus on the first, making a distinction between &#x2018;taking over&#x2019; and &#x2018;over-taking&#x2019;. Over-taking is possible on some dimensions, but there is no reason to conclude that this means a takeover on all dimensions.</Paragraph><Paragraph>Robots would have to acquire a vast amount of real-world knowledge, and the capacity for judgement and collective action. This would be difficult and expensive to achieve but would not be impossible. On the other hand, this does not mean that machine intelligence on its own is enough for robots to achieve power.</Paragraph><Paragraph>Whitby and Oliver&#x2019;s main counter-argument to the various predictions of a robot takeover is that they seem to ignore the realities of the application of power in human society. The argument that the possession of intelligence is sufficient to gain power is clearly false. Human societies have not been, and are not, ruled by the most intelligent or knowledgeable. Nor is it inevitable that future machines will be motivated to acquire domination.</Paragraph><Paragraph>Thus a robot takeover is possible but not inevitable, or even probable. However, we should not be complacent. Some fail-safe methods to avoid takeover might include:</Paragraph><BulletedList><ListItem>buddy systems;</ListItem><ListItem>ethical systems programming;</ListItem><ListItem>humans as final arbiters in decision-making;</ListItem><ListItem>avoiding ubiquity: relationships with technology should not be compulsory.</ListItem></BulletedList><Paragraph>The cartoon below illustrates the danger of complacency. Totally detached from the mayhem they have caused, the robot boffins just fit another brain &#x2013; this time a good one! It also illustrates people&#x2019;s fear not just of robots that get out of hand, but also of scientists who lose touch with reality.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_4\t184_lesson4_f11.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson4_f11.jpg" x_imagewidth="300" x_imageheight="225"/><Caption/><SourceReference><ItemRights><OwnerRef/><ItemRef/><ItemAcknowledgement><font val="Verdana"><language val="">. All rights reserved.</language></font></ItemAcknowledgement></ItemRights><font val="Verdana"><language val="">1993 David Farley</language></font></SourceReference></Figure><Paragraph>Another issue that arises with respect to robot behaviour, particularly in learning robots, is the extent to which we can control or modify their behaviour. To what extent could a &#x2018;good&#x2019; robot modify its behaviour to become a &#x2018;bad&#x2019; robot? Or could a bad robot repent and learn how to be good?</Paragraph><Paragraph>I agree with Whitby and Oliver that what is needed most of all is public debate. Warwick, de Garis, Moravec and others are to be congratulated for sparking public debate. It is for the rest of us to weigh up these matters and come to our own conclusions.</Paragraph></Section></Session><Session><Title>4.4 RobotLab4</Title><Paragraph>In this session you will be doing lab activities using RobotLab4.</Paragraph><Activity><Heading>Do</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\robotlab_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="robotlab_42.png" x_imagewidth="42" x_imageheight="35"/></InlineFigure>Download the lab guide for <olink targetdoc="RobotLab4">RobotLab4.</olink> Work through the guide carefully and do the activities.</Paragraph><Paragraph>Note: If you are using Lego MindStorms you may experience problems with the print quality of the campus plan when you come to download it from the T184 Guide. We recommend you download a printer friendly version of the campus plan by clicking on the link below.</Paragraph><Paragraph><olink targetdoc="Pedro campus plan">Pedro campus plan</olink></Paragraph></Question></Activity></Session><Session><Title>4.5 Summary </Title><Paragraph>In Lesson 4 we have looked at human&#x2013;robot interaction and at robots that display human emotions. We have also begun to address the issue of robots taking over the world. The general opinion is that they won&#x2019;t, certainly not in the near future, because robots are unable to sustain themselves and cannot reproduce. Chris Malcolm argues that huge increases in computer power do not result in similar increases in robot intelligence, and that in any case robot intelligence, unlike human intelligence, is one-dimensional. However, Whitby and Oliver counsel against complacency &#x2013; they concede that takeover is very unlikely, though possible. The ideal seems to be a coexistence, in which robots enhance our quality of life without endangering it.</Paragraph><Paragraph>As you do the Computer-Marked Assignment (CMA) I hope it sinks in what a tremendous range of ideas we have covered. You aren&#x2019;t expected to remember the detail of everything that you've read. But you should recognise that robotics covers many different areas of specialist knowledge, from robot mechanics to the human mind.</Paragraph><Paragraph>We will build on your understanding of the basic technologies of robots in the second half of the module.</Paragraph><StudyNote><Heading>Where next:</Heading> <Paragraph>This is the end of Lesson 4. When you are ready go on to <olink targetdoc="Lesson 5">Lesson 5</olink> the CMA. </Paragraph></StudyNote></Session></Unit><BackMatter><References><Reference><font val="Verdana"><language val="">Anon (2003) Bell Canada deploys Nuance natural-language routing application &#x2013; "Emily" will greet customer-service callers and direct calls to 72 alternative service channels', </language></font><font val="Verdana"><i><language val="">Speech Recognition Update</language></i></font><font val="Verdana"><language val="">, 116. </language></font></Reference><Reference><font val="Verdana"><language val="">Lupisella, M.L., 'Human Mars mission contamination issues', Workdhop on Science and the Human Exploration of Mars, </language></font><font val="Verdana"><language val="">NASA</language></font><font val="Verdana"><language val=""> </language></font><font val="Verdana"><language val="">Goddard</language></font><font val="Verdana"><language val=""> </language></font><font val="Verdana"><language val="">Space</language></font><font val="Verdana"><language val=""> </language></font><font val="Verdana"><language val="">Center</language></font><font val="Verdana"><language val=""> , p. 89 [Online at: </language></font><a href="http://adsabs.harvard.edu/abs/2001shem.work...89L"><font val="Verdana"><language val="">http://adsabs.harvard.edu/abs/2001shem.work...89L</language></font></a></Reference><Reference> Malcolm, C. (2000) Why Robots Won't Rule <font val="Arial">[first published at </font><a href="http://www.dai.ed.ac.uk/homes/cam/Robots_Wont_Rule.shtml"><font val="Arial"><u>http://www.dai.ed.ac.uk/homes/cam/Robots_Wont_Rule.shtml</u></font></a><font val="Arial">; archived at </font><a href="http://replay.waybackmachine.org/20090605224721/http://www.dai.ed.ac.uk/homes/cam/Robots_Wont_Rule.shtml"><font val="Arial"><u>http://replay.waybackmachine.org/20090605224721/http://www.dai.ed.ac.uk/homes/cam/Robots_Wont_Rule.shtml</u></font></a><font val="Arial">]</font></Reference><Reference>Weizenbaum, J. (1984) <i>Computer Power and Human Reason</i>, Penguin Books,Harmondsworth.</Reference><Reference>Weizenbaum, J. (1966) <font val="Verdana"><language val="">'ELIZA - A computer program for the study of natural language communication between man and machine', </language></font><font val="Verdana"><i><language val="">Communications of the ACM</language></i></font><font val="Verdana"><language val="">, Vol. 9, No. 1, pp. 36-35.</language></font></Reference><Reference> Whitby, B. and Oliver, K (2000) &#x2018;How to avoid a robot takeover: political and ethical choices in the design and introduction of intelligent artifacts&#x2019;, <i>AISB-00 Symposium on Artificial Intelligence, Ethics and (Quasi-) Human Rights</i> [Online at <a href="http://www.cogs.susx.ac.uk/users/blayw/BlayAISB00.html">http://www.cogs.susx.ac.uk/users/blayw/BlayAISB00.html</a>]</Reference></References></BackMatter><settings><numbering><Session autonumber="false"/><Section autonumber="false"/><SubSection autonumber="false"/><SubSubSection autonumber="false"/><Activity autonumber="false"/><Exercise autonumber="false"/><Box autonumber="false"/><CaseStudy autonumber="false"/><Quote autonumber="false"/><Extract autonumber="false"/><Dialogue autonumber="false"/><KeyPoints autonumber="false"/><Reading autonumber="false"/><StudyNote autonumber="false"/><Example autonumber="false"/><Verse autonumber="false"/><SAQ autonumber="false"/><ComputerDisplay autonumber="false"/><Summary autonumber="false"/><ProgramListing autonumber="false"/><ITQ autonumber="false"/><Tables autonumber="false"/><Figures autonumber="false"/><MediaContent autonumber="false"/><Chemistry autonumber="false"/></numbering><discussion_alias>Discussion</discussion_alias><session_prefix/></settings></Item>
