<?xml version="1.0"?>
<Item TextType="CompleteItem" SchemaVersion="1.2" id="WEB012106" ACSPageNumber="1" ACSTemplate="generic_unnumbered" ACSCourseItem="T184" LastRendering="VLE Preview" DiscussionAlias="Discussion" SessionAlias="" SecondColour="None" ThirdColour="None" FourthColour="None" Logo="colour"><CourseCode>T184</CourseCode><CourseTitle>Robotics and the meaning of life: a practical guide to things that think</CourseTitle><ItemID>T184</ItemID><ItemTitle>Lesson 3 </ItemTitle><Unit><UnitID>Lesson 3 Introduction</UnitID><UnitTitle>Lesson 3 Introduction</UnitTitle><ByLine>Prepared for the module team by Jeffrey Johnson and Tony Hirst</ByLine><Session><Title>Lesson 3: Things that think</Title><Paragraph><i>Prepared for the module team by Jeffrey Johnson and Tony Hirst, updated by Jon Rosewell</i></Paragraph><Paragraph>Welcome to Lesson 3. Having looked at how machines sense and move in Lesson 2, this week we will see the implications of trying to add the magic ingredient &#x2013; intelligence!   We will extend the sense&#x2013;act model developed in Lesson 2 to the sense&#x2013;think&#x2013;act model of intelligent machines.  Sense&#x2013;think&#x2013;act robots act &#x2018;deliberatively&#x2019;, that is they &#x2018;think&#x2019; about what to do before performing an action.  In this lesson we&#x2019;ll also consider the constraints and opportunities related to building the physical bodies of robots, but the main emphasis will be on things that think.</Paragraph><Paragraph>Lesson 3 follows the same pattern of work you met in Lessons 1 and 2.  It comprises a number of study sessions listed to the left of this page.</Paragraph><Section><Title>Learning outcomes</Title><Paragraph>At the end of Lesson 3 you should be able to:</Paragraph><BulletedList><ListItem>explain the place of cognition in the sense&#x2013;think&#x2013;act model;</ListItem><ListItem>explain how new facts can be deduced from existing facts;</ListItem><ListItem>explain what it means for a robot to plan its actions;</ListItem><ListItem>state some of the limitations on robot bodies;</ListItem><ListItem>know of new power technologies for robots; </ListItem><ListItem>write simple RobotLab programs;</ListItem><ListItem>explain how branches and loops are used in RobotLab.</ListItem></BulletedList></Section></Session><Session><Title>3.1 The sense&#x2013;think&#x2013;act model</Title><Paragraph>In Lesson 2 the perception subsystem of a robot is defined to be the sensors which collect sensory information; the actuation subsystem is the means by which the machine interacts with its environment.  In Lesson 3 we will add another possibility, in which the robot is capable of &#x2018;thinking&#x2019; about its actions before performing them.</Paragraph><Paragraph>The <b>sense</b>&#x2013;<b>think</b>&#x2013;<b>act model</b> shown below extends the sense&#x2013;act model that you met in Lesson 2, and can be described in terms of the &#x2018;cognition&#x2019; subsystem placed between the perception and actuation subsystems.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f01.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f01.jpg" x_imagewidth="474" x_imageheight="244"/><Caption/></Figure><Section><Title>The cognition subsystem</Title><Paragraph>Even though there is no general agreement on what intelligence is, human intelligence is characterised by what we call &#x2018;thinking&#x2019;. The part of a thinking machine that does the thinking is called its &#x2018;cognition subsystem&#x2019;, popularly thought of as the robot&#x2019;s brain.</Paragraph><Paragraph>The cognition subsystem has the important job of deciding what to do next, and telling the various actuators in the actuation subsystem what they should be doing. Compared to reflex actions or reactions, the cognition subsystem allows more complex decisions to be made. This requires more information to be processed.</Paragraph><Paragraph>The cognition subsystem will be characterised by the way a robot uses its knowledge.  This includes factual information about itself and its environment, both of which are stored in its memory.</Paragraph><Paragraph>The cognition subsystem is often referred to as an information processing system. It takes information about the world and processes it in some way to produce new information. This new information may then initiate some particular action or actions via the actuation subsystem.</Paragraph><Paragraph>An important part of cognition is &#x2018;reasoning&#x2019;.  This is the process by which humans combine knowledge and facts to deduce new facts.  In RobotLab3 you will see a simple mechanism that supports reasoning, the if-then-else construct.  This characterises the so-called &#x2018;logical-deductive&#x2019; style of reasoning, which is very powerful.  But there are also other modes of thinking.</Paragraph><Paragraph>A robot&#x2019;s cognition system is often engaged in solving problems.  For example, a robot may have the constant problem of &#x2018;what shall I do next?&#x2019; in the context of &#x2018;how can I achieve my mission?&#x2019;.</Paragraph><Exercise><Heading>Exercise &#x2013; crossing a river</Heading><Question><Paragraph>Suppose the robot below has to cross a river. It must decide at which of three places it can cross safely.  Which do you think it would choose:</Paragraph><NumberedList class="lower-alpha"><ListItem>If it uses the rule &#x2018;cross at the narrowest point&#x2019;?</ListItem><ListItem>If it knows that the crossing at A is shallow?</ListItem><ListItem>If it has seen similar crossings to C that are dangerous?</ListItem><ListItem>If it has always in the past crossed safely at B?</ListItem></NumberedList><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f02.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f02.jpg" x_imagewidth="256" x_imageheight="196"/><Caption/></Figure></Question><Answer><Paragraph>The answers to these questions illustrate different ways of thinking:</Paragraph><NumberedList class="lower-alpha"><ListItem>The robot could combine the fact &#x2018;C is the narrowest point&#x2019; with the rule that says &#x2018;cross at the narrowest point&#x2019; to deduce that it should cross at C.  This decision is a result of a logical deduction.</ListItem><ListItem>The robot might reason that if the water is shallow it is safe to cross at A.  This decision is also a result of a logical deduction.</ListItem><ListItem>The robot makes an association: &#x2018;C looks dangerous!  I won&#x2019;t choose it&#x2019;.  The association may not be with any particular dangerous crossing, but the robot has abstracted a common characteristic from the previous crossings causing it to reject the crossing at C.</ListItem><ListItem>The robot has previously learnt to cross at B, and it already knows the answer to the question &#x2018;where shall I cross?&#x2019;.</ListItem></NumberedList></Answer><Discussion><Paragraph>Here we have abstracted three ways of thinking:</Paragraph><BulletedList><ListItem>logical reasoning;</ListItem><ListItem>association with an abstracted characteristic that the robot has previously learnt to be dangerous;</ListItem><ListItem>conforming to habit.</ListItem></BulletedList><Paragraph>In humans these modes may be used together.  For example, the rejection of C could be a combination of the association that suggests this is a dangerous option, together with reasoning that there are less dangerous options that should be selected.  In other situations an option might be chosen despite it being associated with something dangerous, because reasoning or learning suggests it is the best option.</Paragraph></Discussion></Exercise><InternalSection><Heading>The Towers of Hanoi</Heading><Paragraph>There are many so-called &#x2018;toy problems&#x2019; used to test the basic reasoning capabilities of AI systems. The Towers of Hanoi is one such example.</Paragraph><Paragraph>As with all the best problems, the Towers of Hanoi is rooted in legend. In this case, it is said that somewhere in the East there is a temple. In the temple are three towers. At the moment of Creation, God placed a stack of sixty-four golden rings, one on top of another, on one of the three towers. Each ring was slightly smaller than the one below it. The temple monks were given the task of moving all the rings from one tower to another, with three provisos:</Paragraph><BulletedList><ListItem>only one ring can be moved at a time;</ListItem><ListItem>rings can only be put on the towers, not on the ground;</ListItem><ListItem>a larger ring cannot be put on top of a smaller ring.</ListItem></BulletedList><Paragraph>When the task is completed, so the legend goes, the rings and the temple will crumble to dust and the world will come to an end.</Paragraph><Paragraph>The legend was discovered, or perhaps originated, by Edouard Lucas in 1883. Since then, it has been used by psychologists studying human problem-solving behaviour, and as the basis for a popular children&#x2019;s puzzle.</Paragraph><Paragraph>Other toy problems include Missionaries and Cannibals, and the Bridges of Konigsberg. If you are interested, you could try to track some of these down via the Web.</Paragraph><Paragraph>The reality of operating intelligently in the real world is, of course, another matter. In this lesson you will begin to see the enormous gap between the theoretical possibilities of AI and robotics, and the realities of implementation.</Paragraph></InternalSection></Section></Session><Session><Title>3.2 Thinking and learning</Title><Paragraph>The ability to think about thinking may distinguish humans from all other animals.  It has certainly engaged the attention of many philosophers over many centuries.   Rather than get bogged down in the philosophical details, we will take a practical attitude in this lesson towards &#x2018;thinking&#x2019; in machines.  Neither Simon nor Pedro, our RobotLab robots, have an identity crisis about their existence.  Can they think at all? We&#x2019;ll review this question during the lesson.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f03.gif" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f03.gif" x_imagewidth="512" x_imageheight="424"/><SourceReference><ItemRights><OwnerRef/><ItemRef/><ItemAcknowledgement><font val="Verdana"><i><language val="">St Petersburg Times</language></i></font></ItemAcknowledgement></ItemRights><font val="Verdana"><language val="">Don Addis</language></font></SourceReference></Figure><Paragraph>As the example of crossing a river showed, the cognition system of a human or an artificially intelligent robot could use several different types of &#x2018;thinking&#x2019;. In the lesson we will consider some of them in more detail.</Paragraph><Paragraph>Reasoning and deduction are important parts of human thinking, and also of artificial intelligence. We will look at how robots can represent facts and rules which they use to deduce appropriate behaviour. In RobotLab sessions, you will see how these can be implemented in simple robot programs. </Paragraph><Paragraph>This, of course, implies that we have designed our robot to embody appropriate rules for all eventualities. This is a difficult task, and an alternative approach is to design robots that can learn. Artificial neural networks offer one approach to training robots. We will consider neural nets in more detail here; you will see them in action in later RobotLab sessions.</Paragraph><Paragraph>A neural network has to be trained, which means there has to be a human supervisor involved in the training phase. But animals can learn without being taught by a human; a young chick will peck indiscriminately at anything, but learns to peck at seeds rather than gravel. When the chick pecks at a seed, it is &#x2018;rewarded&#x2019; by obtaining food and this reinforces the correct behaviour of pecking only at seeds. Reinforcement learning is also a possible strategy for creating a robot that can learn.</Paragraph><Paragraph>In a human-designed robot it is easy to see that someone has &#x2018;designed&#x2019; the behaviour embodied into the robot. For example, I can design and make a robot with a touch sensor that will follow the edge of a wall; such a robot can find its way out of a maze. A mouse shows similar behaviour; it will run with its whiskers in contact with a wall. Why does it do this? One reason might be that it keeps the mouse in cover and out of the reach of cats. Over many generations, mice will vary in the tendency to either stay close to a wall or wander into the open, but those that stick to the wall might be less vulnerable to predation and so survive and reproduce better than those who stray. By this process of natural selection, the mouse species will evolve into one that shows this characteristic behaviour.</Paragraph><Paragraph>Similar ideas have been used by AI researchers. Rather than designing behaviour, they simulate the process of natural selection among different behaviours, looking for the behaviour that works best. This technique, of evolving a solution using a &#x2018;genetic algorithm&#x2019;, is one we will come back to in a later lesson.</Paragraph><Paragraph>What this introduction should have shown is that there are many different models for what the cognition system of a robot could be like. </Paragraph><Section><Title>Reasoning &#x2013; facts, rules and deduction</Title><Paragraph>Although it is difficult to define and measure intelligence, the ability to reason is certainly a major factor.  &#x2018;Reasoning&#x2019; is the process of deducing new facts from those that are already known.  For example, suppose I have observed that my dog Tiggy always wags her tail when she eats her dinner.</Paragraph><Paragraph>To see how it is possible to use the given information to reach this conclusion, I can write down what I know Tiggy is doing in the following way:</Paragraph><Quote><Paragraph>Tiggy is eating her dinner.</Paragraph></Quote><Paragraph>There is a &#x2018;hypothesis&#x2019; I have about Tiggy, which is:</Paragraph><Quote><Paragraph>If Tiggy is eating her dinner then Tiggy is wagging her tail.</Paragraph></Quote><Paragraph>Taken together, the fact that &#x2018;Tiggy is eating her dinner&#x2019; satisfies the conditional part of my hypothesis and I can conclude (or deduce) that Tiggy is wagging her tail. (If Tiggy&#x2019;s tail isn&#x2019;t wagging hit your Refresh page button.)</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f04.gif" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f04.gif" x_imagewidth="480" x_imageheight="316"/><Caption/></Figure><SAQ><Heading>SAQ 2</Heading><Question><NumberedList class="lower-alpha"><ListItem><Paragraph>If the light sensor on my moving robot measures less than 30% it will stop.  The light sensor on my robot measures 27%.  Which of the following can you deduce?</Paragraph><NumberedSubsidiaryList class="lower-roman"><SubListItem>My robot will stop.</SubListItem><SubListItem>My robot will turn to the left.</SubListItem><SubListItem>My robot will continue moving.</SubListItem></NumberedSubsidiaryList></ListItem><ListItem><Paragraph>If the left touch sensor on my robot is pressed and the right touch sensor on my robot is pressed, then my robot will go backwards.  Both the touch sensors on my robot are pressed.  Which of the following can you deduce?</Paragraph><NumberedSubsidiaryList class="lower-roman"><SubListItem>My robot will go forwards.</SubListItem><SubListItem>My robot will go anticlockwise.</SubListItem><SubListItem>My robot will stop.</SubListItem><SubListItem>My robot will go backwards.</SubListItem></NumberedSubsidiaryList></ListItem><ListItem><Paragraph>If the temperature of the engine is greater than 85 &#xB0;C then the engine is in danger.  The temperature of the engine is 90 &#xB0;C.  What can you deduce about the engine?</Paragraph></ListItem></NumberedList></Question><Answer><NumberedList class="lower-alpha"><ListItem>The correct answer is  (i).</ListItem><ListItem>The correct answer is (iv).</ListItem><ListItem>You can deduce that the engine is in danger.</ListItem></NumberedList></Answer></SAQ><Paragraph>In SAQ 2, instead of &#x2018;the engine is in danger&#x2019; it would be more precise to say &#x2018;the engine is in danger is true&#x2019;, but  most of us don&#x2019;t tag on the words &#x2018;is true&#x2019; to everything we say. This illustrates how subtle the language we use is. However, it is necessary to be much more explicit where machines are involved.  It is helpful to define what we mean by a &#x2018;fact&#x2019;.</Paragraph><InternalSection><Heading>Facts</Heading><Paragraph>Let&#x2019;s start off by saying that a &#x2018;fact&#x2019; has two parts.  The first part conveys the meaning, and the second states whether the fact is true or not.  For example, the following statements are all facts.</Paragraph><UnNumberedList><ListItem><font val="Courier New">The Queen of England is a woman </font><font val="Courier New"><b>is true.</b></font><font val="Courier New"> </font></ListItem><ListItem><font val="Courier New">The Queen of England is a man </font><font val="Courier New"><b>is false.</b></font><font val="Courier New"> </font></ListItem><ListItem><font val="Courier New">The Queen of England is a robot </font><font val="Courier New"><b>is false.</b></font><font val="Courier New"> </font></ListItem></UnNumberedList><Paragraph>We will call the first part of the fact a &#x2018;phrase&#x2019;. The second part of the fact, &#x2018;is true&#x2019; or &#x2018;is false&#x2019;, is called its &#x2018;truth value&#x2019;.</Paragraph></InternalSection><SAQ><Heading>SAQ 3</Heading><Question><Paragraph>Split each of the following facts into a phrase and a truth value.</Paragraph><UnNumberedList><ListItem><font val="Courier New">The headquarters of the Open University is based in London is false. </font></ListItem><ListItem><font val="Courier New">The OU student newspaper is called Sesame is true. </font></ListItem><ListItem><font val="Courier New">200,000 people study with the OU each year is true. </font></ListItem><ListItem><font val="Courier New">The OU Regional Office for Northern Ireland is in Netherwallop is false. </font></ListItem><ListItem><font val="Courier New">Some Members of Parliament have OU degrees is true. </font></ListItem></UnNumberedList></Question><Answer><Paragraph>The phrase is shown below in normal text, and the truth values in bold:</Paragraph><UnNumberedList><ListItem><font val="Courier New">The headquarters of the Open University is based in London</font><font val="Courier New"><b> is false.</b></font><font val="Courier New"> </font></ListItem><ListItem><font val="Courier New">The OU student newspaper is called Sesame</font><font val="Courier New"><b> is true.</b></font><font val="Courier New"> </font></ListItem><ListItem><font val="Courier New">200,000 people study with the OU each year </font><font val="Courier New"><b>is true.</b></font><font val="Courier New"> </font></ListItem><ListItem><font val="Courier New">The OU Regional Office for </font><font val="Courier New">Northern Ireland</font><font val="Courier New"> is in Netherwallop  </font><font val="Courier New"><b>is false.</b></font><font val="Courier New"> </font></ListItem><ListItem><font val="Courier New">Some members of parliament have OU degrees </font><font val="Courier New"><b>is true.</b></font><font val="Courier New"> </font></ListItem></UnNumberedList></Answer></SAQ><Paragraph>To define &#x2018;fact&#x2019; more precisely, we can say that a &#x2018;phrase&#x2019; is any collection of words,   and that a fact is a phrase that can be recognised and be given a truth value.</Paragraph><Paragraph>By defining a fact in this way we avoid a long digression into mathematical logic, and complex philosophical discussions about &#x2018;well formed&#x2019; phrases and &#x2018;meaning&#x2019;.  If a robot can recognise the phrase, and give it a truth value, the phrase has meaning to the robot.</Paragraph><SAQ><Heading>SAQ 4</Heading><Question><Paragraph>Deduce how this robot is moving from the following two facts. </Paragraph><UnNumberedList><ListItem><font val="Courier New">The left robot wheel is going forward is true. </font></ListItem><ListItem><font val="Courier New">The right robot wheel is going forward is false. </font></ListItem></UnNumberedList></Question><Answer><Paragraph>From these facts I can deduce that the robot is turning to the right.  The second fact tells me that the right wheel is either stationary or going backwards.  Either way, the resultant motion is to turn to the right.  If the right wheel is going at the same speed as the left wheel, the robot will be rotating clockwise on its axis.</Paragraph></Answer></SAQ><Paragraph>You probably deduced correctly which way the robot moves.  How did you do this?  I have tried to understand how I did it.  In my head there was a picture of a robot like that in RobotLab. I pictured the left wheel going forward while the right wheel was stationary.  It seemed &#x2018;obvious&#x2019; that the robot would turn to the right.</Paragraph><Paragraph>This is an example of the mysterious nature of the human process that we find so hard to build in to robots.  It is amazing that most ordinary people can perform quite complex logical calculations like these.</Paragraph><InternalSection><Heading>Rules</Heading><Paragraph>In addition to facts, logical deductions need &#x2018;rules of inference&#x2019;.  Rules have a very particular structure or form, for example:</Paragraph><ComputerDisplay><Paragraph><br/><font val="Courier New"><b>if</b></font><font val="Courier New">   Tiggy is eating her dinner is true</font><br/><font val="Courier New"><b>then</b></font><font val="Courier New"> Tiggy is wagging her tail is true </font><br/></Paragraph></ComputerDisplay><Paragraph>The first fact, <ComputerCode><font val="Courier New">&#x2018;Tiggy is eating her dinner is true&#x2019;</font></ComputerCode>, is often called the &#x2018;conditional&#x2019; or &#x2018;antecedent&#x2019;  part of the rule.</Paragraph><Paragraph>The second fact, <ComputerCode><font val="Courier New">&#x2018;Tiggy is wagging her tail is true&#x2019;</font></ComputerCode>, is often called the &#x2018;consequent&#x2019; part of the rule.</Paragraph></InternalSection><SAQ><Heading>SAQ 5</Heading><Question><NumberedList class="lower-alpha"><ListItem><Paragraph>What are the antecedent and consequent facts of these rules:</Paragraph><Paragraph><ComputerCode><font val="Courier New"><b>if</b></font><font val="Courier New">    the engine is hot is true</font></ComputerCode></Paragraph> <Paragraph><ComputerCode><font val="Courier New"><b>then</b></font><font val="Courier New">  the engine may fail is true?</font></ComputerCode></Paragraph> </ListItem><ListItem><Paragraph>What are the antecedent and consequent facts of these rules:</Paragraph><Paragraph><ComputerCode><font val="Courier New"><b>if</b></font><font val="Courier New">    the oil is low is true</font></ComputerCode></Paragraph> <Paragraph><ComputerCode><font val="Courier New"><b>then</b></font><font val="Courier New">  the engine is hot is true?</font></ComputerCode></Paragraph></ListItem><ListItem><Paragraph>What can you deduce from these two rules and the fact <ComputerCode><font val="Courier New">&#x2018;the oil is low is true&#x2019;</font></ComputerCode>?</Paragraph></ListItem></NumberedList></Question><Answer><NumberedList class="lower-alpha"><ListItem><Paragraph>The antecedent fact is <ComputerCode><font val="Courier New">&#x2018;the engine is hot is true&#x2019;</font></ComputerCode>, and the consequent fact is <ComputerCode><font val="Courier New">&#x2018;the engine may fail is true&#x2019;</font></ComputerCode>.</Paragraph></ListItem><ListItem><Paragraph>The antecedent fact is <ComputerCode><font val="Courier New">&#x2018;the oil is low is true&#x2019;</font></ComputerCode>, and the consequent fact is <ComputerCode><font val="Courier New">&#x2018;the engine is hot is true&#x2019;</font></ComputerCode>.</Paragraph> </ListItem><ListItem><Paragraph>The fact <ComputerCode>&#x2018;the oil is low is true&#x2019;</ComputerCode> combined with the second rule leads me to deduce that <ComputerCode><font val="Courier New">&#x2018;the engine is hot&#x2019;</font></ComputerCode> is true. This new fact combined with the first rule leads me to deduce that <ComputerCode><font val="Courier New">&#x2018;the engine may fail&#x2019;</font></ComputerCode> is true.</Paragraph></ListItem></NumberedList></Answer></SAQ><InternalSection><Heading>The T184 fact and rule writing convention</Heading><Paragraph>To make it clear which part of a fact is the phrase and which is the truth value, in RobotLab we use an underscore character instead of a space between the words in a  phrase, and we write the truth value as <ComputerCode><font val="Courier New">&#x2018;= TRUE&#x2019;</font></ComputerCode> or <ComputerCode><font val="Courier New">&#x2018;= FALSE&#x2019;</font></ComputerCode>. For example, facts may take the form:</Paragraph><ComputerDisplay><Paragraph><font val="Courier New">the_light_sensor_detects_white = TRUE</font></Paragraph><font val="Courier New"> </font><Paragraph><font val="Courier New">the_touch_sensor_is_activated = FALSE</font></Paragraph><font val="Courier New"> </font><Paragraph><font val="Courier New">the_battery_level_is_low = TRUE</font></Paragraph><font val="Courier New"> </font></ComputerDisplay><Paragraph>Similarly, rules take the form:</Paragraph><ComputerDisplay><Paragraph><br/><font val="Courier New"><b>if</b></font><font val="Courier New"> the_battery_level_is_low = TRUE </font><br/><font val="Courier New"><b>then</b></font><font val="Courier New"> the_battery_needs_charging = TRUE </font><br/></Paragraph></ComputerDisplay></InternalSection><InternalSection><Heading>Deduction</Heading><Paragraph>By matching a known fact to the antecedent part of a rule, the consequent part of the rule can be deduced, making it a known fact also. So, from a rule such as:</Paragraph><ComputerDisplay><Paragraph><br/><font val="Courier New"><b>if</b></font><font val="Courier New"> the_battery_level_is_low = TRUE</font><br/><font val="Courier New"><b>then</b></font><font val="Courier New"> the_battery_needs_charging = TRUE</font><br/></Paragraph></ComputerDisplay><Paragraph>and the fact:</Paragraph><ComputerDisplay><Paragraph><font val="Courier New">the_battery_level_is_low = TRUE</font></Paragraph><font val="Courier New"> </font></ComputerDisplay><Paragraph>it can be deduced that <ComputerCode><font val="Courier New">the_battery_needs_charging = TRUE</font></ComputerCode></Paragraph><Paragraph>Generally, the rules and known facts (i.e. the facts that the robot &#x2018;believes&#x2019;) are stored in a database. New facts can be generated from old facts by the application of the rules. In some systems, the robot may be able to learn new rules, but in many cases, it cannot &#x2013; all the rules must be provided at the start by the programmer.</Paragraph><Paragraph>This method of generating new facts from old facts is called reasoning. There are various other rules of deduction used in reasoning, and many other subtleties.  However, this form of generating new facts from &#x2018;if-then&#x2019; rules is as far as we&#x2019;ll go on this module.</Paragraph><Paragraph>Since an understanding of deduction is central to the idea of reasoning, it is worthwhile working through a few more examples. Consider the following:</Paragraph><ComputerDisplay><Paragraph><br/><font val="Courier New"><i>Rule:</i></font><font val="Courier New">       </font><font val="Courier New"><b>if</b></font><font val="Courier New"> Tiggy_eating = TRUE </font><font val="Courier New"><b>then</b></font><font val="Courier New"> Tiggy_wagging_tail = TRUE</font><br/><font val="Courier New"><i>Fact:</i></font><font val="Courier New">       Tiggy_eating = TRUE</font><br/><font val="Courier New"><i>Deduction:</i></font><font val="Courier New">  Tiggy_wagging_tail = TRUE</font><br/></Paragraph></ComputerDisplay><ComputerDisplay><Paragraph><br/><font val="Courier New"><i>Rule:</i></font><font val="Courier New">       </font><font val="Courier New"><b>if</b></font><font val="Courier New"> temperature_&gt;_90 = TRUE </font><font val="Courier New"><b>then</b></font><font val="Courier New"> engine_too_hot = TRUE </font><br/><font val="Courier New"><i>Fact:</i></font><font val="Courier New">       temperature_&gt;_90 = TRUE</font><br/><font val="Courier New"><i>Deduction:</i></font><font val="Courier New">  engine_too_hot = TRUE</font><br/></Paragraph></ComputerDisplay><ComputerDisplay><Paragraph><br/><font val="Courier New"><i>Rule:</i></font><font val="Courier New">       </font><font val="Courier New"><b>if</b></font><font val="Courier New"> sensor_touched = TRUE </font><font val="Courier New"><b>then</b></font><font val="Courier New"> stop_the_robot = TRUE</font><br/><font val="Courier New"><i>Fact:</i></font><font val="Courier New">       sensor_touched = TRUE</font><br/><font val="Courier New"><i>Deduction:</i></font><font val="Courier New">  stop_the_robot = TRUE</font><br/></Paragraph></ComputerDisplay><Paragraph>When a rule is matched by a fact in this way, the rule is said to &#x2018;fire&#x2019;.  Now  consider the following example.  What is going on here?</Paragraph><ComputerDisplay><Paragraph><br/><font val="Courier New"><i>Rule:</i></font><font val="Courier New">       </font><font val="Courier New"><b>if</b></font><font val="Courier New"> sensor_touched = FALSE </font><font val="Courier New"><b>then</b></font><font val="Courier New"> keep_going = TRUE</font><br/><font val="Courier New"><i>Fact:</i></font><font val="Courier New">       sensor_touched = FALSE</font><br/><font val="Courier New"><i>Deduction:</i></font><font val="Courier New">  keep_going = TRUE</font><br/></Paragraph></ComputerDisplay><Paragraph>In this case the antecedent part of the rule (i.e. the antecedent fact) has to be false to make the rule fire. Since the fact in the database is false, it matches the requirement of the rule perfectly, and the rule can fire.</Paragraph><Paragraph>This shows that for a rule to fire, both parts of the antecedent fact have to be matched with a fact in the database.  First there is the phrase, or the bit which can have meaning.  Then there is the question of whether the truth values match. When both match, then the rule can fire.</Paragraph><Paragraph>Sometimes the phrase of a fact in the database matches a rule, but its truth value does not match.  In this case no deduction can be made and the rule does not fire, as illustrated below:</Paragraph><ComputerDisplay><Paragraph><br/><font val="Courier New"><i>Rule:</i></font><font val="Courier New">       </font><font val="Courier New"><b>if</b></font><font val="Courier New"> sensor_touched = TRUE </font><font val="Courier New"><b>then</b></font><font val="Courier New"> stop_the_robot = TRUE</font><br/><font val="Courier New"><i>Fact:</i></font><font val="Courier New">       sensor_touched = FALSE</font><br/><font val="Courier New"><i>Deduction:  </i></font><font val="Courier New">no deduction can be made</font><br/></Paragraph></ComputerDisplay></InternalSection></Section><Section><Title>Conflict resolution</Title><Paragraph>Human beings use hundreds of rules, and sometimes they clash or conflict with each other.  For example:</Paragraph><ComputerDisplay><Paragraph><font val="Courier New"><b>if</b></font><font val="Courier New"> it_is_Sunday = TRUE </font><font val="Courier New"><b>then</b></font><font val="Courier New"> visit_Aunty = TRUE</font></Paragraph><font val="Courier New"> </font><Paragraph><font val="Courier New"><b>if</b></font><font val="Courier New"> the_roof_is_leaking = TRUE </font><font val="Courier New"><b>then</b></font><font val="Courier New"> fix_it_immediately = TRUE</font></Paragraph></ComputerDisplay><Paragraph>Suppose that in your database you have the facts:</Paragraph><ComputerDisplay><Paragraph><font val="Courier New">it_is_Sunday = TRUE </font><br/><font val="Courier New"><i>and</i></font><font val="Courier New"> </font><br/><font val="Courier New">the_roof_is_leaking = TRUE</font></Paragraph></ComputerDisplay><Paragraph>It is not possible to visit Aunty and fix the roof at the same time, so what do you do?  Most people have methods for resolving such conflicts.  For example, some rules may have priority over others.  In this case fixing the roof might take priority.</Paragraph><Paragraph>What happens when a robot has rules that conflict with each other?  For example:</Paragraph><ComputerDisplay><Paragraph><font val="Courier New"><b>if</b></font><font val="Courier New"> light_sensor_&gt;_50  = TRUE </font><font val="Courier New"><b>then</b></font><font val="Courier New"> turn_left = TRUE</font></Paragraph><font val="Courier New"> </font><Paragraph><font val="Courier New"><b>if</b></font><font val="Courier New"> touch_sensor_pressed = TRUE </font><font val="Courier New"><b>then</b></font><font val="Courier New"> turn_right = TRUE</font></Paragraph><font val="Courier New"> </font></ComputerDisplay><Paragraph>when the fact database contains:</Paragraph><ComputerDisplay><Paragraph><font val="Courier New">light_sensor_&gt;_50 = TRUE </font><br/><font val="Courier New"><i>and</i></font><font val="Courier New"> </font><br/><font val="Courier New">touch_sensor_pressed = TRUE</font></Paragraph></ComputerDisplay><Paragraph>The robot has to have a &#x2018;conflict resolution&#x2019; strategy to decide which rule will fire.  Typical conflict resolution strategies include one or more of the following:</Paragraph><BulletedList><ListItem>the priority given to the rules;</ListItem><ListItem>the order in which the rules are written;</ListItem><ListItem>which rules fired least recently;</ListItem><ListItem>and so on.</ListItem></BulletedList><Paragraph>A rule that could have fired, but did not, is said to have been &#x2018;triggered&#x2019;.  That is, its conditional fact was known to be true, and in principle it could have fired, but it was prevented from doing so by a more favoured rule firing.</Paragraph><SAQ><Heading>SAQ 6</Heading><Question><Paragraph>As you probably guessed, Tiggy has quite a personality, and at one year old she is rather a handful.  Even so, there are rules that seem to govern her behaviour.  One of these is:</Paragraph><ComputerDisplay><Paragraph><font val="Courier New"><b>if</b></font><font val="Courier New"> Tiggy_is_on_the_sofa = TRUE </font><font val="Courier New"><b>then</b></font><font val="Courier New"> Tiggy_is_happy = TRUE  </font></Paragraph></ComputerDisplay><Paragraph>However, there&#x2019;s another rule:</Paragraph><ComputerDisplay><Paragraph><font val="Courier New"><b>if</b></font><font val="Courier New"> Tiggy_is_on_the_sofa = TRUE </font><font val="Courier New"><b>then</b></font><font val="Courier New"> Tiggy_is_told_off = TRUE</font></Paragraph></ComputerDisplay><Paragraph>Observing her demeanour when this rules fires, I think there is yet another rule:</Paragraph><ComputerDisplay><Paragraph><font val="Courier New"><b>if</b></font><font val="Courier New"> Tiggy_is_told_off = TRUE </font><font val="Courier New"><b>then</b></font><font val="Courier New"> Tiggy_is_happy = FALSE</font></Paragraph></ComputerDisplay><NumberedList class="lower-alpha"><ListItem>Is there any conflict in the rules that apply to Tiggy?</ListItem> <ListItem>Do you think that animals can have conflict resolution strategies?</ListItem> <ListItem>Do you think that dogs are capable of reasoning, as suggested here?</ListItem> </NumberedList></Question><Answer><NumberedList class="lower-alpha"><ListItem>There&#x2019;s no conflict if Tiggy stays off the sofa.  However if she gets on the sofa, the first rule will fire and she&#x2019;ll be happy.  However, if she is seen on it she&#x2019;ll be told off and the second rule will fire, which will make the third rule fire, and she&#x2019;ll be unhappy.  So getting on the sofa can make her both happy and unhappy.</ListItem> <ListItem>As time goes on, Tiggy is learning to give priority to the second rule.  Sometimes she gives priority to the first rule, since the sofa is just irresistible.</ListItem> <ListItem><Paragraph>I think Tiggy does &#x2018;work it out&#x2019;, although perhaps not in the way a human would.  She knows that she&#x2019;ll be told off if she gets on the sofa, and seems to apply rules such as <ComputerCode><font val="Courier New"><b>if</b></font><font val="Courier New"> I_get_off_the_sofa_before_I_am_seen = TRUE </font><font val="Courier New"><b>then</b></font><font val="Courier New"> I_won&#x2019;t_get_told_off = TRUE</font></ComputerCode>.  And she&#x2019;s right.  It doesn&#x2019;t seem fair to invoke complicated rules like <ComputerCode><font val="Courier New"><b>if</b></font><font val="Courier New"> the_cushions_are_warm = TRUE </font><font val="Courier New"><b>then</b></font><font val="Courier New"> Tiggy_has_been_on_the_sofa = TRUE</font></ComputerCode>.  So sometimes, the first rule still takes priority, and sometimes there are tell-tale black hairs on warm cushions!</Paragraph></ListItem> </NumberedList><Paragraph>It&#x2019;s important to realise that I don&#x2019;t know what Tiggy thinks and I can&#x2019;t know how she thinks.  First, I&#x2019;m not a dog and second, I&#x2019;m not Tiggy.  I can try to understand Tiggy&#x2019;s thought processes, but I can only interpret what I see through my own thought processes.</Paragraph></Answer></SAQ></Section><Section><Title>Is reasoning built into our brains?</Title><Paragraph>Some people believe that the rules for reasoning are &#x2018;hard-wired&#x2019; into our brains.  The evidence for this includes the way that almost every human being seems to accept the legitimacy of this way of reasoning.</Paragraph><Paragraph>Another remarkable feature of humans is that almost all human societies use language to communicate facts and ideas.  The linguist, Noam Chomsky, put forward a widely accepted theory that the ability to learn and use language &#x2013; any language &#x2013; is built into our brains.  Language enables us to represent and manipulate abstract concepts in our heads &#x2013; it is what enables us to think.</Paragraph><Paragraph>Do you think that the ability to reason logically is built into our brains? Try the experiments below which test people&#x2019;s reasoning powers.</Paragraph><Exercise><Heading>Exercise &#x2013; cards 1</Heading><Question><Paragraph>There are four cards on the table with a number on one side and a letter on the other.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f05.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f05.jpg" x_imagewidth="265" x_imageheight="101"/><Caption/></Figure><Paragraph>Which cards would you turn over to test the rule: &#x2018;If a card has a vowel on one side then it has an even number on the other&#x2019;?</Paragraph></Question><Answer><Paragraph>You turn over the E to check that it has an even number on the back.  You also have to turn over the 7 to make sure it has not got a vowel on the back, contradicting the rule.</Paragraph></Answer></Exercise><Exercise><Heading>Exercise &#x2013; cards 2</Heading><Question><Paragraph>Consider another set of cards. These have a city on one side and a mode of transport on the other.  Which cards would you turn over to test the rule: &#x2018;Every time I go to Paris I travel by plane&#x2019;?</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f06.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f06.jpg" x_imagewidth="265" x_imageheight="101"/><Caption/></Figure></Question><Answer><Paragraph>You turn over &#x2018;Paris&#x2019; to make sure that it has &#x2018;plane&#x2019; on the back.  You also turn over &#x2018;train&#x2019; to make sure that it has not got &#x2018;Paris&#x2019; on the back.</Paragraph></Answer></Exercise><Paragraph>The significance of these card experiments is that in tests only 12% of people got the answer to the first one right, which is more abstract. This compares with 60% for the second, which uses associations with the real world.    It is argued that if logic were hard-wired into the brain there would be no difference in the results of the two experiments.  That there is a difference suggests that humans use knowledge and experience about the world to answer the question.</Paragraph><Paragraph>One of the difficulties of all this for roboticists is that we don&#x2019;t really understand how our brains work, and it is very difficult to make robots with brains and modes of reasoning like ours.</Paragraph></Section></Session><Session><Title>3.3 Machine intelligence</Title><InternalSection><Heading>Neural networks</Heading><Paragraph>The main alternative to the logic-based calculations that we met on the previous page is calculations that use so-called neural networks.  The logical if-then approach to processing incoming information can be represented as follows, where all the logic inside the box is assumed to be known and explicit:</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f07.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f07.jpg" x_imagewidth="507" x_imageheight="205"/><Caption/></Figure><Paragraph>Computers are completely different from human brains.  They work much faster, but they cannot display intelligence like us &#x2013; a message that appears again and again throughout this module.  Human brains are made up of neurons, as you saw in Lesson 1.  They get their power through their number, and the astronomical  number of connections between them. For example:</Paragraph><BulletedList><ListItem>the average number of neurons in the brain = 10 billion to 100 billion;</ListItem><ListItem>the number of connections for a typical neuron = 1,000 to 10,000.</ListItem></BulletedList><Paragraph>Observations like these led scientists to design &#x2018;artificial neurons&#x2019;, computing devices with neuron-like properties.</Paragraph><Paragraph>Below is a typical picture of a neural network, with layers of processing elements called neurons.  The neurons have inputs, which are typically numbers.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f08.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f08.jpg" x_imagewidth="310" x_imageheight="273"/><Caption/></Figure><Paragraph>The main idea behind this kind of neural network is that whatever pattern of input numbers is given to the network, there will be an associated pattern of output numbers.   The network is capable of adjusting itself internally during a training process, in which the network is presented with a set of known inputs and a corresponding set of desired outputs.</Paragraph><Paragraph>In a robot, the inputs might come from sensors, and the outputs might encode a decision.  For example, if the top green output neuron is 1 and all the other outputs are 0, the robot might be wired up to turn left.  Then when trained, for a given set of sensor values, the top neuron might output 0.9 and all the others might output 0.1, and this could trigger the &#x2018;turn left&#x2019; mechanism.</Paragraph><Paragraph>A simple kind of artificial neuron typically adds the input numbers. If the sum is above a given threshold value the neuron fires, and outputs the number 1.  Each of the links shown in the network has a &#x2018;weight&#x2019;.  The output is passed from one neuron to the inputs of the next layer multiplied by the weight on its link.</Paragraph><Paragraph>For example, suppose the weight on the connection between two neurons is 0.78. When the first neuron fires, the value 0.78 will pass from it to the second neuron.  This second neuron sums all of its inputs, and if the result exceeds a threshold value it also fires.</Paragraph><Paragraph>Surprisingly, it is possible to &#x2018;train&#x2019; a neural network from data that sets appropriate values of the weights automatically.  The details don&#x2019;t matter here, but the important point is that the network &#x2018;learns&#x2019; how to process information by learning from examples.</Paragraph><Paragraph>Thus an artificial neural network (ANN) has inputs and outputs. The network is trained using &#x2018;training data&#x2019;, i.e. sets of inputs and known outputs. Initially when the inputs are given, the outputs are wrong. The errors, or differences between the desired outputs for a particular input pattern and the actual outputs that are generated from the input pattern, are used to change the weights in the network.  The set of data is repeatedly shown to the network, until the weights alter to give the correct outputs.</Paragraph><Paragraph>In this way, the logical information processing box seen earlier can be replaced by a neural information processing box, illustrated below.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f09.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f09.jpg" x_imagewidth="490" x_imageheight="193"/><Caption/></Figure><Paragraph>The power of a neural network lies in the fact that, once trained on a range of training data, the network can respond correctly to inputs it has not seen before.  This is called &#x2018;generalisation&#x2019;. </Paragraph><Paragraph>You can see an example of an ASIMO robot generalising to correctly recognise an object it has not previously seen in the following video clip. (Note that the programme does not make it clear whether ASIMO is using neural nets or some other approach to recognising objects.) </Paragraph><Activity><Heading>Watch</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\video_dvd_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="video_dvd_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure> <i>James May&#x2019;s Big Ideas: Man-Machine</i>. Watch the sequence of ASIMO recognising tables and chairs which runs from about 40:40 to 42:40 minutes. </Paragraph><Paragraph>A full <olink targetdoc="James May DVD transcript">transcript for the DVD</olink> is available. You can also open it from the <olink targetdoc="More resources">More resources</olink> section of the website.</Paragraph></Question><Discussion><Paragraph>Note that ASIMO identifies an office stool as a type of chair, even though it looks quite different to the previous example of a chair. ASIMO may be looking for characteristics such as a flat surface at a particular height above ground to recognise a chair. If so, although the table has a flat surface and four legs, it would not be recognised as a chair because the flat surface is too high.</Paragraph></Discussion></Activity><Paragraph>During training, the networks don&#x2019;t always learn to give the correct outputs for a given set of inputs. For example, the data may contain inconsistencies, with two training items having the same inputs but different outputs.  In this case, no network could ever be trained to give the &#x2018;right&#x2019; answer, since there are two different answers.</Paragraph><Paragraph>There are many kinds of neural computing, some of which are very powerful.  However, be warned that most artificial neural systems are as different from real brains as the processors inside a computer.  It is easy to hype up neural networks, but they remain far removed from the biological neurons that underpin human intelligence.</Paragraph><Paragraph>The following cartoon illustrates the difficulty we have in implementing the neural information processing box and getting &#x2018;consciousness&#x2019; to appear. The expression &#x2018;<i>cogito, ergo sum</i>&#x2019;, means &#x2018;I think, therefore I am&#x2019;. It is attributed to the 17th century philosopher, mathematician and soldier, Ren&#xE9; Descartes.  The joke here is that the operators only know what&#x2019;s going on inside this huge machine by the messages it prints out.  The suggestion is that this machine is thinking all kinds of thoughts unknown to its creators, and has its own autonomous intelligence to grapple with the conundrum of its own existence &#x2013; in Latin!</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f10.gif" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f10.gif" x_imagewidth="476" x_imageheight="378"/><Caption/><SourceReference><font val="Verdana"><language val="">&#xA9; Mischa Richter</language></font></SourceReference></Figure><Paragraph>The same is true of the information-in/information-out  diagrams shown above.  How can we know what the machine is thinking inside?  How can we know if a machine is &#x2018;intelligent&#x2019; except by asking it questions and examining the answers?</Paragraph></InternalSection><Section><Title>The Turing Test</Title><Paragraph>Machine intelligence is hard to define, but there have been many attempts to define tests to see how intelligent machines are.</Paragraph><Paragraph>Alan Turing was one of the greatest British mathematicians and scientists of the 20th century.  He had a brilliant mind and made many profound contributions to the fledgling computer science of the 1930s&#x2013;50s.  He is rightly considered to be one of the fathers of artificial intelligence.</Paragraph><Paragraph>Turing worked at Bletchley Park during the Second World War, where he helped crack the Enigma code, used by Germany to send military signals.  He was part of the team that built Colossus, one of the very first digital computers.</Paragraph><Paragraph>One of the things that Turing was interested in was the extent to which computers can mimic intelligent human behaviour.  He proposed the idea of an imitation game that has since come to be known as the Turing Test.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f11.gif" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f11.gif" x_imagewidth="434" x_imageheight="192"/><SourceReference><ItemRights><OwnerRef/><ItemRef/><ItemAcknowledgement>from Copeland, B. J., <i>Artificial Intelligence</i>, Blackwell Publishers, Oxford, 1993</ItemAcknowledgement></ItemRights>Ann Witbrock</SourceReference></Figure><Paragraph>The Turing Test involves a man and a woman answering questions by passing written messages to a questioner.  The questioner can ask any question in an attempt to tell the respondents apart. The man and woman can give any answer in order to fool the questioner.  You can imagine how hard it would be to tell the man and woman apart.  Turing then supposed that either the man or the woman is replaced by a computer. With the same restrictions as before the questioner now has to decide which one is the computer.  If the questioner can&#x2019;t tell the difference the computer is deemed to have passed the test.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f12.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f12.jpg" x_imagewidth="411" x_imageheight="459"/><SourceReference><ItemRights><OwnerRef><font val="Verdana"><language val="">Peter Steiner from cartoonbank.com. All Rights Reserved</language></font></OwnerRef><ItemRef/><ItemAcknowledgement/></ItemRights><font val="Verdana"><language val="">The New Yorker Collection, 1993</language></font></SourceReference></Figure><SAQ><Heading>SAQ 7</Heading><Question><Paragraph>If a dog could use the Internet, as in the cartoon above, do you think it could also pass the Turing Test?</Paragraph></Question><Answer><Paragraph>If a dog could use the Internet  it could fool those it interacted with while online into thinking that it was a person and not a dog. So if it took the place of the computer in the Turing Test it would pass.</Paragraph><Paragraph>There are numerous stories and even TV programmes about animals that talk and think like humans.  Of course, no one has yet demonstrated an animal that can pass the Turing Test.</Paragraph></Answer></SAQ><Activity><Heading>Optional reading</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\book_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="book_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure> If you have time and would like more information on Alan Turing or the Loebner Prize take a look at the following websites: </Paragraph><Paragraph><a href="http://www.turing.org.uk/turing/">http://www.turing.org.uk/turing/</a></Paragraph><Paragraph><a href="http://www.loebner.net/Prizef/loebner-prize.html">http://www.loebner.net/Prizef/loebner-prize.html</a></Paragraph></Question></Activity></Section><Section><Title>Intelligence and language</Title><Paragraph>John Searle objected that the Turing Test does not test understanding, only the ability to manipulate symbols.  His &#x2018;Chinese Room&#x2019; thought experiment involves a person in a tent sending messages backwards and forwards in Chinese. Initially you might think that the tent understands Chinese.  However, if you look inside there is a man looking up the symbols in a table and passing the appropriate responses back.  Although the man is correctly processing the Chinese symbols, he does not understand Chinese, and may have no understanding of their meaning.</Paragraph><Paragraph>This raises another issue &#x2013; what do we mean when we say that a human &#x2018;understands language&#x2019;? Even simple words such as &#x2018;moon&#x2019; or &#x2018;ball&#x2019; are just arbitrary symbols: the sound of the spoken word or the shape of the written one has no relation to the object itself. For two humans to agree what they are talking about, they must attach the word to sensory experiences, for example, the word &#x2018;moon&#x2019; corresponds to seeing a big white thing in the night sky. This is called &#x2018;symbol grounding&#x2019; and is thought to be central to the development of language. The human brain cannot compute language on its own &#x2013; it has to be connected to the world in a real body, with real senses.  Thus the &#x2018;meaning&#x2019; of the symbols that are manipulated inside our heads must somehow be &#x2018;grounded&#x2019; in reality.</Paragraph><Paragraph>Some robotic researchers believe that machine understanding of language must also be grounded in sensory experience. You can see a Honda ASIMO robot learning the meaning of words in the following clip from <i>Man&#x2013;Machine</i>. </Paragraph><Activity><Heading>Watch</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\video_dvd_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="video_dvd_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure> <i>James May&#x2019;s Big Ideas: Man&#x2013;Machine</i>. Watch the sequence of ASIMO recognising new objects, which runs from about 35:29 to 40:40. </Paragraph><Paragraph>A full <olink targetdoc="James May DVD transcript">transcript for the DVD</olink> is available. You can also open it from the <olink targetdoc="More resources">More resources</olink> section of the website.</Paragraph></Question><Discussion><Paragraph>Notice how ASIMO says &#x2018;unknown object&#x2019; when presented with an object such as the toy robot he has not seen before, accepts the word &#x2018;Grandpa&#x2019; given to him by James, and later can recall the word &#x2018;Grandpa&#x2019; when presented with the same object. </Paragraph><Paragraph>I think most people would have called this a &#x2018;toy robot&#x2019;, but ASIMO and James have agreed that it is a &#x2018;Grandpa&#x2019;. Do you think James and ASIMO share the same understanding of what is meant by a &#x2018;Grandpa&#x2019;?</Paragraph></Discussion></Activity><Paragraph>One of the big challenges in robotics is to get robots to understand symbols such as the words we use in speech. This is difficult enough for concrete objects such as &#x2018;ball&#x2019;, &#x2018;car&#x2019; or &#x2018;cow&#x2019;, but the challenge is even greater for the many words in everyday language which refer to complex and abstract things. For example, the words shopping, motoring, and gardening may evoke complex thoughts. The problem of designing robots that can understand human language is therefore still a long way from being solved.</Paragraph><Paragraph/></Section><Section><Title>Good old-fashioned AI</Title><Paragraph>There is artificial intelligence (AI) and there is good old-fashioned artificial intelligence (GOFAI).</Paragraph><Paragraph>Steve Grand is something of a maverick among robotics researchers. Working at home rather than in an academic or industrial research lab, he set out to create a robot of his own. He wrote about his experiences in his book <i>Growing up with Lucy: How to Build an Android in Twenty Easy Steps</i>. In the introduction he writes that GOFAI &#x2018;set out with the explicit aim of creating human-like intelligence, but got it all horribly wrong and wasted the best part of 50 years.&#x2019; He was referring to the fact that AI researchers used to concentrate on &#x2018;symbolic&#x2019; processes such as logic and reasoning; this is the approach summed up as &#x2018;GOFAI&#x2019;. Today many people are much more interested in non-symbolic intelligence based on biological analogies. Personally I don&#x2019;t think the researchers did waste 50 years completely, but it would have been better if a more open-minded approach had been taken.</Paragraph><Paragraph>Grand also suggests that, as conscious beings, we don&#x2019;t live in the real world, and that most of the time we live in a virtual world inside our heads.  What do you think?  What kind of virtual world could exist inside a robot&#x2019;s head?  How could you know what is in the mind of a robot?</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f13.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f13.jpg" x_imagewidth="314" x_imageheight="304"/><Caption/><SourceReference><font val="Verdana"><language val=""> </language></font><ItemRights><OwnerRef><font val="Verdana"><language val="">Gahan Wilson from cartoonbank.com. All Rights Reserved.</language></font></OwnerRef><ItemRef/><ItemAcknowledgement/></ItemRights><font val="Verdana"><language val="">The New Yorker Collection, 1990</language></font></SourceReference></Figure><Paragraph>In robotics there is a tension between what we strive to achieve and what we manage to achieve. On the one hand roboticists want to build human-like machines, but on the other, many robots are little more than computing boxes on wheels with primitive sensing mechanisms. This tension is also true of AI in general. A long-running joke among AI researchers is the alternative definition of AI &#x2013; &#x2018;Almost Invented&#x2019;.</Paragraph><Paragraph>In creating humanoid machines, we also have a desire to implant human-like intelligence. However, the result is a machine that doesn&#x2019;t understand human language very well and that is unable to reason about subtle, ambiguous and abstract things in the way humans can.</Paragraph><Paragraph>GOFAI is a symptom of interpreting &#x2018;intelligence&#x2019; as the ability to do numerical and logical calculations. In this, symbols play a key role. Explicit manipulation of symbols and reasoning are important for some things, like a legal argument, a newspaper article or a mathematical proof. But mapping sensor input onto symbols can be very difficult in practice.</Paragraph><Paragraph>The problem of symbol grounding is a hard one to solve. GOFAI can be criticised for paying too much attention to symbolic manipulation, and not enough to sensors and abstracting useful information from them. For example, one of the problems that GOFAI still has to solve is how to extract information from visual scenes in order to generate useful symbolic descriptions of them.</Paragraph></Section><Section><Title>Strong AI and weak AI</Title><Paragraph>In the artificial intelligence world, there is actually a considerable range of opinion as to the extent to which robots will be able to replicate human intelligence. For the sake of convenience, researchers often classify themselves as belonging to one of two camps: &#x2018;strong AI&#x2019; and &#x2018;weak AI&#x2019;.</Paragraph><Paragraph>Members of the strong AI camp believe that it will be possible to build machines that are capable of demonstrating human levels of intelligence, and ultimately, a full range of emotions, and even consciousness. Their arguments are based on the claim that there can be nothing mystical going on in our brains, and that, when all is said and done, our bodies and brains are just machines, albeit made out of biological &#x2018;wetware&#x2019;.</Paragraph><Paragraph>The weak AI group takes issue with these rather grandiose claims, and suggests that machines will only ever be able to act &#x2018;as if&#x2019; they had natural intelligence. Although a robot might appear to be able to understand what is going on in a conversation, for example, this would just be an illusion. Again, there is nothing mystical going on &#x2013; just a computer processing information. There is no emergence of feeling, or consciousness &#x2013; it&#x2019;s just machinery.</Paragraph><Paragraph>Of course, researchers are a long way away from creating conscious robots. As you have seen, there is still the challenge (particularly in the GOFAI tradition) of just how to implement an intelligent reasoning process between sensory inputs and behavioural outputs. The cartoon below captures the essence of the difficulty in trying to formally address this problem.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f14.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f14.jpg" x_imagewidth="304" x_imageheight="347"/><Caption/><SourceReference><font val="Verdana"><language val="">&#xA9; Sidney Harris</language></font></SourceReference></Figure><Paragraph>Human reasoning can involve long and complicated sequences of deductive steps.  Each of these has to be legitimate.  The joke here is that all the complex reasoning and symbolic manipulation fails because an essential step in the chain is missing.  No matter how good the sensors and actuators are in a robot, if the cognition subsystem between them cannot cope with long and complicated deductive chains of reasoning, the robot will not be able to perform intelligent tasks like humans can.</Paragraph></Section></Session><Session><Title>3.4 A layered model of behaviour</Title><Paragraph>Earlier we grappled with what it means either for a human or for a machine to be considered &#x2018;intelligent&#x2019;.  To be pragmatic, we can sidestep the issue and simply demand that a robot &#x2018;does the right thing&#x2019; in its environment, whether due to &#x2018;intelligence&#x2019; or not.</Paragraph><Paragraph>In the previous lesson we considered the sense&#x2013;act model and saw that simple animals and robots can &#x2018;do the right thing&#x2019; without even a cognition subsystem. They do this by relying on hardwired behaviours, created by the designer of a robot or produced as the result of evolution in the case of animals. For example, a simple vacuum cleaning robot may follow the sense&#x2013;act model; it simply moves randomly about the room, backing away when it bumps into an obstacle.</Paragraph><Paragraph>The sense&#x2013;think&#x2013;act model with its cognition subsystem has the potential for more flexible and complex behaviour in the face of an unpredictable world. But complex decision making requires both good sensory information and a &#x2018;model&#x2019; of the world, and this inevitably increases the complexity of a robot. Consider a more complex vacuum cleaning robot that covers the floor systematically; such a robot will require a model of its world. This model might include a map of the room for route planning, which implies that the robot knows both its own location and that of obstacles. The robot will also need to &#x2018;know&#x2019; that some types of object such as walls are fixed and that others such as chairs and cats may not always be found in the same place.</Paragraph><Paragraph>Even for this simple example, you can see that the complexity of the robot will increase considerably. This will be apparent both in the design and programming of the robot, and in the processing that the robot needs to carry out as it operates. The problem gets worse as we try to anticipate all the many different situations in which the robot must act intelligently.</Paragraph><Paragraph>An alternative approach is to aim for a robot that is able to learn. After all, most human behaviour is learned, and many animals searching for food learn by trial and error or by the example of others. So instead of designing and programming a robot to deal with many eventualities, perhaps it would be better to design a robot that was able to learn from its experience.</Paragraph><Paragraph>One aspect of learned behaviours in humans is that they can become <b>reactive</b>. Reactions are low level, sensor-motor actions that can be learned or improved over time. Once learned, they become automatic and no longer require conscious thought: examples would be walking, riding a bike or driving a car, which we can &#x2018;do without thinking.&#x2019; Reactive behaviours are similar to reflexes &#x2013; driven by sensory input, quick reacting and efficient &#x2013; but they are more adaptable since they can be relearned or unlearned. </Paragraph><Paragraph>A fruitful approach for robot behaviour is to combine these models in different layers. At the bottom would be reflexes; these are fast, initiated directly by sensor input, and carried out with no cognitive processing. They might be important for safety of the robot or people around the robot; collision detection and obstacle avoidance are examples.</Paragraph><Paragraph>Above this would be reactive behaviours: pre-programmed or learned packages of actions that once started require no further cognition. These respond to sensor input and require only &#x2018;local&#x2019; processing so are quick and efficient. An example might be a robot that follows a line marked on the floor where the behaviour is relatively simple: keep going if over the line, or turn towards the line if not over the line.</Paragraph><Paragraph>At the top of the hierarchy is a layer that deals with planning and strategy. This is where cognition occurs, dealing with the high-level goals and using the appropriate reactive behaviours to carry them out. A parts delivery robot in a factory might need to plan a route to various workstations along an assembly line, and then use its line-following behaviour to reach each destination.</Paragraph></Session><Session><Title>3.5 Putting it all together</Title><Paragraph>We started this lesson by introducing an abstract model of robot behaviour, the sense&#x2013;think&#x2013;act model. This model suggests that a robot would need perception, cognition and actuation subsystems and, indeed, it is possible to identify such subsystems in most robots. Real physical robots will require some other important subsystems, such as a source of power and a &#x2018;body&#x2019; to hold things together. </Paragraph><Paragraph>We can produce a high-level description of the organisation or architecture of a robot in the figure below.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f15.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f15.jpg" x_imagewidth="445" x_imageheight="288"/></Figure><Paragraph>The <b>sensory subsystem </b>provides the robot with information about the state of the world, for example through light sensors, bumper-activated touch switches, or range-finding ultrasound sensors. It is the physical counterpart to the perception subsystem of the behavioural model.</Paragraph><Paragraph>The <b>mobility/actuation subsystem</b> is what moves the robot around in the world &#x2013; wheels or legs, for example &#x2013; as well as the motors that actuate them.</Paragraph><Paragraph>The <b>control subsystem</b> is the physical counterpart of the cognition subsystem of the behavioural model. It is typically made up of one or more computer processors and will be responsible for using its own control plans along with sensory input information to decide what the robot should do. It will also need to tell the mobility / actuation subsystem to move the robot around in the world or otherwise perform useful actions. </Paragraph><Paragraph>The <b>power subsystem</b> provides power to the robot, not only to the motors or other actuators to make the robot move, but also to power the control and sensory subsystems. The power subsystem includes the power source (e.g. batteries) as well as power management and monitoring.</Paragraph><Paragraph>Finally, the <b>mechanical assembly</b> is the robot&#x2019;s body and is what holds everything together.</Paragraph><Paragraph>The details for any particular robot might be quite different &#x2013; we&#x2019;ve seen the variety of sensory subsystems and mobility subsystems possible in Lesson 2. But this model allows us to think about robot design in general terms as well as understanding the specific requirements that lead to particular design decisions.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f16.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f16.jpg" x_imagewidth="400" x_imageheight="266"/><Caption>A robot buggy created with LEGO Mindstorms</Caption></Figure><Exercise><Heading>Optional exercise</Heading><Question><Paragraph> If you have a Lego Mindstorms kit, you may wish to do this optional exercise. Can you identify which parts of the robot buggy above correspond to the:</Paragraph><BulletedList><ListItem>sensory subsystem</ListItem><ListItem>mobility system</ListItem><ListItem>control subsystem</ListItem><ListItem>power subsystem</ListItem><ListItem>mechanical assembly.</ListItem></BulletedList></Question><Answer><BulletedList><ListItem><b>sensory subsystem</b>: the sensors are a downward-facing light sensor and a simple bumper facing forward that acts on a touch sensor</ListItem><ListItem><b>mobility system:</b> two rear-mounted motors drive two wheels through a simple gear train. This makes a differential drive system. A simple skid at the front is used to balance the robot.</ListItem><ListItem><b>control subsystem:</b> the large LEGO RCX &#x2018;brick&#x2019; which forms the main body of the robot contains the control subsystem. The control subsystem is housed in the top half of the RCX unit and comprises a microprocessor, memory and interfaces that handle inputs from sensors and outputs to the motors.</ListItem><ListItem><b>power subsystem: </b>this consists of batteries housed in the lower half of the RCX unit and associated electronics to manage power, for example turning the unit off after a period to save power and warning of low power levels</ListItem><ListItem><b>mechanical assembly:</b> this consists of LEGO bricks, beams and plates. Interestingly, in this design the RCX unit (functionally the control system) also forms an important part of the robot body since other LEGO parts are mounted directly on it.</ListItem></BulletedList></Answer></Exercise><Section><Title>Robot bodies</Title><Paragraph>Robot bodies are typically made from rigid materials &#x2013; plastic in the case of small toy robots, but more usually metal. Particularly for mobile robots, there are many design challenges in creating robot bodies that are strong yet lightweight enough not to drain too much of the limited power available to the robot.</Paragraph><Paragraph>Current robot bodies are often constrained by the actuators available to the robot designer. We&#x2019;ve seen many examples of robots that are powered by electric motors and this often leads to designs with wheels or tracks. Creating walking mechanisms requires some ingenuity if they are to be driven by electric motors. Hydraulic or pneumatic mechanisms operate in ways rather more similar to animal muscles and offer an alternative way of powering walking mechanisms; you&#x2019;ve seen examples in BigDog and the Kanagawa human exoskeleton. </Paragraph><Paragraph>The rigid bodies of a typical robot also bring with them some other problems when compared to an animal or human body. Human skin is not just a simple covering for our body &#x2013; it contains a rich supply of nerves and organs that can sense touch, temperature and pain. Simple press switches can be used to provide crude &#x2018;touch&#x2019; sensors for a robot, and piezoelectric sensors (based on materials which generate a small electric charge when bent) can give a sensor that can detect degrees of pressure. This could allow, for example, a robot hand to pick up an egg without crushing it. But providing a robot with good tactile senses remains very difficult. </Paragraph><Paragraph>Robots with a combination of heavy, hard bodies and very limited sensory information raise considerable safety concerns, particularly when we begin to imagine robots working in close proximity to humans. James May in the <i>Man&#x2013;Machine</i> program dreamt of a robot that would go to the kitchen and fetch him another beer. I wonder how safe he would feel when a heavy, rigid robot with no sense of touch lumbers towards him!</Paragraph><InternalSection><Heading>The Robot Zoo</Heading><Paragraph>A rather different view of robot bodies and mechanisms is provided by <i>The Robot Zoo</i>. The Robot Zoo started life as a children&#x2019;s book, written by Prof Philip Whitfield and illustrated by John Kelly that imaginatively portrayed animals as robots, illustrating mechanics in action in cut-away drawings. This was later turned into an animatronic display for an interactive museum exhibit. Although these are not robots by most definitions, they do illustrate both the design challenges that living animals have met and solved, and some of the mechanical alternatives that robot designers could use.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f17.gif" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f17.gif" x_imagewidth="200" x_imageheight="284"/><Caption/></Figure><Paragraph>You may be fortunate enough to see the exhibition, or you can see some of the material on the web: <a href="http://www.thetech.org/exhibits/online/robotzoo/">http://www.thetech.org/exhibits/online/robotzoo/</a></Paragraph></InternalSection></Section><Section><Title>Power</Title><Paragraph>One of the main limitations of current mobile robots is the power subsystem. For larger mobile robots, such as the Big Dog quadruped or the autonomous vehicles in the DARPA Challenge, it is feasible to use internal combustion engines that have good reserves of power. But many mobile robots rely on electric batteries to provide power both for the computers used in their control subsystem and for electric motors used in their actuation or mobility subsystem. And as Ruth Aylett quips, &#x2018;if robots are going to take over the world, they&#x2019;ll have to do it pretty fast before their batteries run out&#x2019; (Aylett 2002, p. 94). </Paragraph><Paragraph>To give a robot greater autonomy, it can be designed to sense the power level remaining in its batteries and to seek out a recharging station when the power level drops. This assumes that the robot can detect a charging station and has enough power to reach it, and that introduces some complex issues in designing the robot&#x2019;s control strategies &#x2013; just when should the robot give up its current task and seek more power? </Paragraph><Paragraph>Other alternatives to conventional batteries are being explored. Solar cells can be used to recharge batteries. The amount of power that can be obtained by mounting solar cells on a mobile robot is very limited, but solar cells have been used to power robots such as the Mars exploration robots <i>Spirit</i> and <i>Opportunity</i>. These robots have been operating for five years at the time of writing (2009).</Paragraph><Paragraph>Fuel cells use a chemical reaction between hydrogen (stored on the robot as fuel) and oxygen (obtained from the air) to generate electrical power. Fuels cells potentially offer greater power for the same weight than do electrical batteries.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f18.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f18.jpg" x_imagewidth="222" x_imageheight="151"/><Caption>The UWE Slugbot</Caption><SourceReference>Courtesy of Chris Melhuish, University of the West of England (UWE)</SourceReference></Figure><Paragraph>Animals use chemical energy derived from their food and some researchers have investigated the possibility of robots that can gather and &#x2018;eat&#x2019; their own food. A microbial fuel cell developed at Kings College London uses bacteria or yeast to metabolise simple foods such as sugar and generate small quantities of electrical power. The University of Western England&#x2019;s EcoBot I &amp; II robots carried these microbial fuel cells and could be &#x2018;fed&#x2019; with either sugar or dead flies. Their SlugBot robot was designed to hunt and catch slugs; their prey could have been returned to a central fermentation system which would create methane to generate electricity to recharge the robot&#x2019;s batteries. Combining these two approaches would give an autonomous robot that could feed itself. </Paragraph><Paragraph>If you are interested, the website <a href="http://www.ias.uwe.ac.uk/robots.htm">Robots at the IAS Laboratory</a> has more information on these robots.</Paragraph></Section></Session><Session><Title>3.6 RobotLab3</Title><Paragraph>In this session you will be doing lab activities using RobotLab3.</Paragraph><Activity><Heading>RobotLab</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\robotlab_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="robotlab_42.png" x_imagewidth="42" x_imageheight="35"/></InlineFigure> Download the lab guide for <olink targetdoc="RobotLab3">RobotLab3</olink>. Work through the guide carefully and do the activities.</Paragraph></Question></Activity><Paragraph>When you have finished RobotLab3 have a go at the following SAQs.</Paragraph><SAQ><Heading>SAQ 9</Heading><Question><NumberedList class="lower-alpha"><ListItem><Paragraph>What is a branch in a computer program? Illustrate your answer by saying what the following program will do when <i>x</i> is set to 1, and when <i>x</i> is set to 2.</Paragraph><InlineFigure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f19a.gif" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f19a.gif" x_imagewidth="262" x_imageheight="145"/></InlineFigure> </ListItem><ListItem><Paragraph>What is a loop in a computer program? Illustrate your answer by writing down a program that counts aloud from 1 to 10. Hint: declare a variable <i>x </i>= 1, and in the <b>main</b> part of the program use a loop with<ComputerCode> while x &lt; 11, send x, set x = x + 1</ComputerCode> and <ComputerCode>wait 100</ComputerCode>.</Paragraph></ListItem><ListItem>Give two examples of loop commands in RobotLab.</ListItem></NumberedList></Question><Answer><NumberedList class="lower-alpha"><ListItem><Paragraph>A branch is a command that makes a decision about which command will be executed next.  This is illustrated by the following program:</Paragraph><InlineFigure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f19b.gif" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f19b.gif" x_imagewidth="279" x_imageheight="186"/></InlineFigure><Paragraph>The branch condition is<ComputerCode> if <i>x</i> = 1</ComputerCode>.  If this is true, control goes through the <ComputerCode>then</ComputerCode> to the <ComputerCode>send 1</ComputerCode> command, and the system says &#x2018;1&#x2019;.  Otherwise control goes to the comment at the end of the program.</Paragraph></ListItem><ListItem><Paragraph>A loop is a sequence of commands within which, when executed, control may pass back from the last command in the sequence to the first.  The following program illustrates the idea of loop:</Paragraph><InlineFigure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_3\t184_lesson3_f20.gif" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson3_f20.gif" x_imagewidth="326" x_imageheight="188"/> </InlineFigure> <Paragraph>When the program meets the <ComputerCode>while x &lt; 11</ComputerCode> command, it tests to see if <i>x</i> is less than 11.  If it is, the following sequence of instructions is executed, starting with the comments, followed by <ComputerCode>send x</ComputerCode> and <ComputerCode>wait 100</ComputerCode>.</Paragraph></ListItem><ListItem><Paragraph>The loop commands in RobotLab are:</Paragraph><BulletedSubsidiaryList><SubListItem>forever</SubListItem><SubListItem>repeat</SubListItem><SubListItem>while</SubListItem></BulletedSubsidiaryList></ListItem></NumberedList></Answer></SAQ><SAQ><Heading>SAQ 10</Heading><Question><Paragraph>The robots in RobotLab can speak.  Do they understand what they are saying?</Paragraph></Question><Answer><Paragraph>We make the RobotLab robots &#x2018;speak&#x2019; by playing numbered wav files.  Although we can interpret the words, all the robot knows is the number of the wav file.</Paragraph></Answer></SAQ></Session><Session><Title>3.7 Summary</Title><Paragraph>Lesson 3 has focused on how robots might &#x2018;think&#x2019;, in the context of getting information from sensors, and controlling actuators in order to interact with the environment.  To some extext we have gone beyond the sense-think-act model introduced at the beginning of the lesson, by considering how robots might engage in more abstract forms of thought.</Paragraph><Paragraph>In RobotLab3 we have considered program structures, including branches and loops.  Branches are intimately related to the &#x2018;if-then&#x2019; rules.</Paragraph><Paragraph>In Lesson 3 we have also briefly considered the neural network approach to processing information.  This is very different from the sequential programming introduced in RobotLab2, but is still artificial and a long way from biological neural systems.  Arguably, Grey Walter&#x2019;s tortoise, which we met in Lesson 2, comes closest to the biological model.</Paragraph><Paragraph>In Lesson 3 you have seen: </Paragraph><BulletedList><ListItem>cognition in the context of the sense&#x2013;think&#x2013;act model;</ListItem><ListItem>how new facts can be deduced from existing facts using reasoning;</ListItem><ListItem>what  it means for a robot to plan its actions;</ListItem><ListItem>some of the limitations on robot bodies;</ListItem><ListItem>some power technologies for robots.</ListItem></BulletedList><Paragraph>In RobotLab3 you have:</Paragraph><BulletedList><ListItem>learnt how to write simple programs to control robots;</ListItem><ListItem>seen how branches and loops are used in robot programming.</ListItem></BulletedList><Paragraph>The main conclusion to be drawn from this lesson is how different thinking machines and thinking humans are.  On these web pages and on the <i>Man-Machine</i> DVD you have seen many examples of robots and cutting-edge robotics research.  This, and the reality of programming Simon and Pedro, suggest that RoboCup&#x2019;s aspirations to have a team of humanoid soccer players capable of beating a world XI human team by 2050 is rather optimistic.  But who knows what the future holds?</Paragraph><StudyNote><Paragraph>Where next:</Paragraph><Paragraph>This is the end of Lesson 3. When you are ready go to <olink targetdoc="Lesson 4">Lesson 4</olink>.</Paragraph></StudyNote></Session></Unit><BackMatter><References><Reference>Ruth Aylett (2002) <i>Robots: Bringing Intelligent Machines to Life?</i> Quarto Publishing. </Reference><Reference>Steve Grand (2003) <i>Growing up with Lucy: how to build an android in twenty easy steps</i>, Weidenfeld &amp; Nicholson, London. ISBN: 0-297-60733-2</Reference></References></BackMatter><settings><numbering><Session autonumber="false"/><Section autonumber="false"/><SubSection autonumber="false"/><SubSubSection autonumber="false"/><Activity autonumber="false"/><Exercise autonumber="false"/><Box autonumber="false"/><CaseStudy autonumber="false"/><Quote autonumber="false"/><Extract autonumber="false"/><Dialogue autonumber="false"/><KeyPoints autonumber="false"/><Reading autonumber="false"/><StudyNote autonumber="false"/><Example autonumber="false"/><Verse autonumber="false"/><SAQ autonumber="false"/><ComputerDisplay autonumber="false"/><Summary autonumber="false"/><ProgramListing autonumber="false"/><ITQ autonumber="false"/><Tables autonumber="false"/><Figures autonumber="false"/><MediaContent autonumber="false"/><Chemistry autonumber="false"/></numbering><discussion_alias>Discussion</discussion_alias><session_prefix/></settings></Item>
