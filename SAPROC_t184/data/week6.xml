<?xml version="1.0"?>
<Item TextType="CompleteItem" SchemaVersion="1.2" id="WEB012109" ACSPageNumber="1" ACSTemplate="generic_unnumbered" ACSCourseItem="T184" LastRendering="VLE Preview" DiscussionAlias="Discussion" SessionAlias="" SecondColour="None" ThirdColour="None" FourthColour="None" Logo="colour"><CourseCode>T184</CourseCode><CourseTitle>Robotics and the meaning of life: a practical guide to things that think</CourseTitle><ItemID>T184</ItemID><ItemTitle>Lesson 6</ItemTitle><Unit><UnitID>Lesson 6 Introduction</UnitID><UnitTitle>Lesson 6 Introduction</UnitTitle><ByLine>Prepared for the module team by Jeffrey Johnson and Tony Hirst</ByLine><Session><Title>Lesson 6: Social robots</Title><Paragraph><i>Prepared for the module team by Jeffrey Johnson and Tony Hirst, updated by Jon Rosewell</i></Paragraph><Paragraph>In the previous lessons you have seen how real robots have been developed from simple mechanical automatons into mobile, intelligent systems. However, there is still an enormous gap between the robots we can build and control today, and the robots of science fiction novels and films.</Paragraph><Paragraph>In the second half of the module, you will read several short stories from the <i>I, Robot </i>collection, by Isaac Asimov. These stories raise interesting questions about possible futures, and the relationships we might come to have with advanced robots.</Paragraph><Paragraph>There is nothing unusual or necessarily sinister in the way our relationships with robots are likely to change. The invention of the washing machine, vacuum cleaner and refrigerator have completely transformed people&#x2019;s lives over the past 100 years.  Similarly, the motor car and the aeroplane have transformed human life. A hundred years ago it was not possible to have family life centred around the school run, or for the average person to take his or her holiday in the sun.</Paragraph><Paragraph>We may lament that we have lost a simpler and slower way of life, but we&#x2019;re unlikely to blame the actual machines for the impact they have had.  Our cars do not force us to drive on those occasions we could walk or get the bus.  We make these decisions, even though we know that they may have damaging effects on the environment.  When it comes to using machines of any kind, the practical, moral and ethical choices are ours.  Or are they?</Paragraph><Paragraph>In this lesson we want you to think beyond the practical constraints of today&#x2019;s technology.  Let&#x2019;s assume that anything is possible, and examine the consequences. Using this technique, you will investigate the relationships that humans may have with robots.  To what extent are we in control of their deployment and use? You will consider this in the context of the relationships humans have with other humans and with other living creatures.</Paragraph><Paragraph>Lesson 6 follows the same pattern of work you met in the first half of the module. The study sessions are listed to the left.</Paragraph><Section><Title>Learning outcomes</Title><Paragraph>When you have completed Lesson 6 you should be able to:</Paragraph><BulletedList><ListItem>analyse the Hollywood portrayal of robots;</ListItem><ListItem>explain the idea of the positronic brain;</ListItem><ListItem>explain some of the ideas behind fail-safe engineering;</ListItem><ListItem>explain how Asimov&#x2019;s three laws can be seen as guiding principles in robot design;</ListItem><ListItem>explain the significance of Asimov&#x2019;s zeroth law;</ListItem><ListItem>explain and illustrate the kinds of relationships people may have with robots;</ListItem><ListItem>discuss the ethics of the use of robots by the military and the police;</ListItem><ListItem>discuss robot rights in the context of human rights.</ListItem></BulletedList></Section></Session><Session><Title>6.1 Robots in the movies</Title><Paragraph>Many writers and film makers have explored the world of robotics. So before looking at what Asimov has to say about robots, let&#x2019;s see what other science fiction has produced in recent years.</Paragraph><Activity><Heading>Discuss</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\forum_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="forum_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure></Paragraph><Paragraph>Spend five or ten minutes jotting down the titles of any films or   television series you have seen in which robots play a major role. If none spring to mind, have a quick look at this website   <a href="http://www.robotbooks.com/movies.htm">http://www.robotbooks.com/movies.htm</a>. Scroll through the first few videos described until you get to &#x2018;The Day the Earth   Stood Still&#x2019;.</Paragraph><Paragraph>For each film, as you remember it, write down:</Paragraph><NumberedList class="lower-alpha"><ListItem>the film or series title;</ListItem><ListItem>what sort of robots appeared in the film and how they were portrayed;</ListItem><ListItem>what type of issues the film raised about our relationships with robots;</ListItem><ListItem>how feasible the robot was, from a technical point of view.</ListItem></NumberedList><Paragraph>You are urged to discuss your ideas with others on the T184 Discussion forum. </Paragraph></Question></Activity><Paragraph>I asked a friend to do this, and this is what he wrote down:</Paragraph><InternalSection><Heading>Star Wars</Heading><NumberedList class="lower-alpha"><ListItem>C3PO, a humanoid robot, and R2D2, a dustbin on wheels, were leading characters in several <i>Star Wars</i> films. C3PO could use human language, as well as being able to understand the beeps of R2D2. Both robots were very helpful.</ListItem><ListItem>I remember a scene about a robot scrapdealer. I suppose that these will be as necessary as car scrap merchants are today if robots are ever widely available.</ListItem><ListItem>The robots would not be feasible today.</ListItem></NumberedList></InternalSection><InternalSection><Heading>The Daleks (Dr Who)</Heading><NumberedList><ListItem>The Daleks are mutated biological beings inside wheeled vertical metal bin-like vehicles with domed tops. The Daleks aren&#x2019;t really robots &#x2013; rather they are protective &#x2018;bodies&#x2019; inhabited by mutants from a radioactive planet that are fixated on planetary domination! They are also very scary.</ListItem><ListItem>The Daleks are trying to assert their superiority in the universe.</ListItem><ListItem>In the original series, the Daleks&#x2019; vehicles were fairly low-tech &#x2013; they could even be foiled by a flight of stairs &#x2013; and quite feasible. But they had the ability to travel through time, and in the recent series gained the ability to fly &#x2013; neither feasible.</ListItem></NumberedList><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_6\t184_lesson6_f01.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson6_f01.jpg" x_imagewidth="340" x_imageheight="255"/><Caption>Three daleks surround the TARDIS</Caption><SourceReference><font val="Verdana"><language val="">&#xA9; BBC</language></font></SourceReference></Figure></InternalSection><InternalSection><Heading>Westworld</Heading><NumberedList class="lower-alpha"><ListItem>The film is based in a vacation theme park, where robots are humanoids playing various roles and allowing visitors to play out their fantasies. One of the robots, played by Yul Brynner, is a gunslinger that is always just slower than the thrill-seeking vacationer. Something goes wrong, and the gunslinger starts killing people. The robot wasn&#x2019;t bad to start with though. Perhaps it was infected by a computer virus.</ListItem><ListItem>The film raises the issues of people using robots as surrogates for vicarious pleasures, and the possibility of robots going wrong and running amok.</ListItem><ListItem>The humanoid robots portrayed are not feasible today.</ListItem></NumberedList></InternalSection><InternalSection><Heading>Terminator</Heading><NumberedList class="lower-alpha"><ListItem>Evil Arnie is an assassin robot from a future world where the computers and machines rule. He is sent back in time to kill the yet-to-be mother of the boy who will grow up to lead the resistance movement against the machines.</ListItem><ListItem>The Terminator is a humanoid robot with real skin. Power supply doesn&#x2019;t seem to be a problem, even at the end of the film where the robot is still capable of moving and working even though large parts of it have been torn off.</ListItem><ListItem>Issues raised include time travel, and the possibility of very powerful robots behaving in malevolent ways.</ListItem><ListItem>The humanoid Terminator robot is not feasible today.</ListItem></NumberedList><Paragraph>See the Wikipedia page <a href="http://en.wikipedia.org/wiki/Terminator_3:_Rise_of_the_Machines">Terminator 3: Rise of the machines</a>.</Paragraph></InternalSection><InternalSection><Heading>AI &#x2013; Artificial Intelligence</Heading><NumberedList class="lower-alpha"><ListItem>AI is a story in which a robot boy is adopted into a family and customised to love the mother. She, in some ways, comes to love the robot boy too. However, her natural son recovers from his coma, and there is no need for the robot any more. Rather than send it back to the factory for destruction, as the storyline deems she must, she abandons it &#x2013; fairytale style &#x2013; in the woods. The robot boy meets a gigolo robot, who befriends it, and narrowly escapes destruction in a Robot Wars style gladiatorial carnival, where old robots are horrifically destroyed in front of a human crowd.</ListItem><ListItem>The boy is a humanoid robot. Again, power supply and operating as a humanoid robot in a human world don&#x2019;t appear to present many difficulties (although the robot boy does need cleaning out by engineers when it attempts to eat some food).</ListItem><ListItem>The film raises issues about the way humans use and treat robots, and about robots&#x2019; emotional needs.</ListItem><ListItem>The humanoid robot is not feasible today.</ListItem></NumberedList><Paragraph>See the Wikipedia page <a href="http://en.wikipedia.org/wiki/A.I._Artificial_Intelligence"><font val="Arial">A.I. Artificial Intelligence</font></a><font val="Arial">.</font></Paragraph></InternalSection><Paragraph>In the main, the films offer a <a href="http://en.wikipedia.org/wiki/Dystopia">dystopian</a> view of the future and the role robots are likely to play in it. Even when the robots are not designed to be destructive, they are still likely to go wrong and become dangerous. Many of the robots portrayed are humanoid, and many of them are capable of communicating using human language. None of the robots ever seem to need their batteries recharging, and they all seem to cope well with the complexities of the physical and social world around them.</Paragraph><Paragraph>My impression of Hollywood and TV robots is that in many cases they are human replicas, either with a grudge against humanity or capable of acting as a human surrogate.</Paragraph></Session><Session><Title>6.2 I, Robot</Title><Paragraph>In Lesson 1 I introduced you to Asimov&#x2019;s three laws of robotics. You will also find them at the beginning of Asimov&#x2019;s book <i> I, Robot</i>.</Paragraph><NumberedList><ListItem>A robot may not injure a human being or, through inaction, allow a human being to come to harm.</ListItem><ListItem>A robot must obey the orders given to it by human beings except where such orders would conflict with the first law.</ListItem><ListItem>A robot must protect its own existence as long as such protection does not conflict with the first or second law.</ListItem></NumberedList><Paragraph>In my opinion the formulation of these laws in the 1940s was an intellectual and imaginative tour de force. The laws are simple but span an immense and important range of possibilities. <i>I, Robot</i> explores the consequences of the three laws and the ways they interact with each other.</Paragraph><Paragraph>The book contains a series of stories about the problems encountered by the US Robots Corporation during Dr Susan Calvin&#x2019;s 40 years as the company&#x2019;s robopsychologist. They are told as reminiscences by her to a young reporter.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_6\t184_lesson6_f02.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson6_f02.jpg" x_imagewidth="140" x_imageheight="225"/><SourceReference><font val="Verdana"><language val="">&#xA9; Harper Collins Publishers</language></font></SourceReference></Figure><Paragraph>You might find the idea of a robopsychologist strange. However, if robots are to develop as complex intelligent machines capable of learning from their own experiences, then it may be necessary to have a robopsychologist rather than a robot engineer on hand to sort out any problems.</Paragraph><Paragraph>Human cognitive psychologists attempt to explain people&#x2019;s behaviour in rational terms (for example, the reasons or ways of thinking that make a person decide to act in a particular way). An earlier school of psychologists, known as the behaviourists, tried to explain behaviour solely in terms of training: how previous experience, along with punishments and rewards, could explain current behaviour. It may be that robopsychologists will have to use similar techniques to &#x2018;debug&#x2019; faulty robots and identify the potential causes of behavioural problems!</Paragraph><Paragraph>In the UK, legislation relating to issues surrounding mental health is covered in part by the <a href="http://www.dh.gov.uk/PublicationsAndStatistics/Legislation/ActsAndBills/ActsAndBillsArticle/fs/en?CONTENT_ID=4002034&amp;chk=lmZd%2Bu">Mental Health Act 1983</a>. In particular, the Act:</Paragraph><Quote><Paragraph>... makes provision for the compulsory detention and treatment in hospital of those with mental disorder.</Paragraph></Quote><Paragraph>In common parlance, this is often referred to as &#x2018;sectioning&#x2019;. Patients with a physical injury can refuse treatment. However, sectioned patients cannot refuse treatment for their mental disorder. Should robots be offered treatment if they are behaving erratically? Should rewiring and/or reprogramming be imposed upon them? On what grounds would we want to &#x2018;section&#x2019; an artificially intelligent robot and, if necessary, forcibly treat it?</Paragraph><Paragraph>Each of the Asimov stories you will read investigates a clear theme that is relevant to this module. The first story concerns Robbie, a robot designed to act as playmate to a young girl. When you read this story, pay attention to the relationships between robots, people and animals.</Paragraph><Activity><Heading>Read</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\book_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="book_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure>Read the introduction to Asimov&#x2019;s book, then read Chapter 1: Robbie. When you are ready, answer the following SAQ.</Paragraph></Question></Activity><SAQ><Heading>SAQ 1</Heading><Question><NumberedList class="lower-alpha"><ListItem>Who are the main characters in this story? Give a brief summary of the plot.</ListItem><ListItem>What are the main points being made?</ListItem><ListItem>To what extent do you think a playmate robot such as Robbie is likely to exist in the future?</ListItem></NumberedList></Question><Answer><NumberedList class="lower-alpha"><ListItem><Paragraph>The story is about a robot, Robbie, designed to be a companion to five-year old Gloria Weston. Gloria loves the robot dearly, but she bosses him around, and she always insists on having everything her way.</Paragraph>  <Paragraph>Initially Mrs Weston is very pleased to have her workload reduced, but she begins to worry that her daughter is becoming emotionally dependent on the robot. More tellingly, there is gossip and the neighbours don&#x2019;t approve &#x2013; Robbie will have to go!</Paragraph>  <Paragraph>Mrs Weston cajoles her husband into getting rid of Robbie, and replaces the robot with a dog.  When Gloria sees the dog she is delighted and rushes to share the good news with Robbie.  She is grief-stricken when Robbie can&#x2019;t be found, and will not accept the dog as a substitute.</Paragraph>  <Paragraph>After months of grieving over the loss of Robbie, the family go on holiday to New York to cheer up Gloria.  She thinks they are going there to look for Robbie.  In New York, Mr Weston has arranged a visit to US Robotics to see the robots.  There Gloria sees Robbie, and is in imminent danger of being killed by a truck as she lunges towards him.  Robbie saves her and is grudgingly rehabilitated in Mrs Weston&#x2019;s eyes.  She is annoyed with her husband for setting up the &#x2018;chance&#x2019; encounter and the danger that resulted.  But she gives in: &#x2018;I guess he can stay with us until he rusts&#x2019;.</Paragraph></ListItem><ListItem><Paragraph>This story raises a number of issues.  The first is the nature of human&#x2013;robot interaction.  There are already robot toys and robot pets intended to entertain children. But could we have a real robot nursemaid?  Do you think robots such as ASIMO can be developed into a robot like Robbie?</Paragraph><Paragraph>It is particularly interesting to note Asimov&#x2019;s view that there will be anti-robot factions, and that they will succeed in having robots banned from Earth.  As you read through this lesson, consider the possibilities of any kind of control being put on the use of robots.</Paragraph></ListItem><ListItem>I think robot playmates will exist, but there may be dangers of parents behaving irresponsibly.  For example, even now there are occasional stories of parents leaving their children to fend for themselves while they go on holiday.  How much easier if one were able to think that a robot was looking after them.</ListItem></NumberedList></Answer></SAQ><Section><Title>The positronic brain and fail-safe engineering</Title><Paragraph>One of the remarkable ideas in <i>I, Robot </i>is the notion that robots can be manufactured with brains that have Asimov&#x2019;s laws immutably built into them &#x2013; i.e. positronic brains &#x2013; making it impossible for the laws to be disobeyed. This means that although the robots might have unpredictable behaviour, their behaviour would always be consistent with the laws. Whilst positronic brains and Asimov&#x2019;s Laws of Robotics are unlikely to ever be implemented in real robots, the very idea of the Three Laws, as well as the way in which Asimov saw them interacting with each other, can still provide much food for thought today.</Paragraph><Paragraph>It is possible to interpret Asimov&#x2019;s laws in several ways. Firstly, as the basis of a series of thought experiments, as in <i>I, Robot</i>. These allow us to explore the tension between what appears at first glance to be three eminently sensible and independent laws. The laws can be seen as embodying a moral code that Asimov&#x2019;s robot <b>must</b> follow. It is also possible to use them as a scheme for evaluating the &#x2018;responsible actions&#x2019; of a robot in a particular situation &#x2013; that is, what they <b>should</b> do.</Paragraph><Paragraph>Secondly, the laws can be used to inspire particular logical systems that can be programmed into a robot. You have already seen how robots can have &#x2018;beliefs&#x2019; about the world, which are encoded as rules. Researchers who focus on using formal logic techniques have produced many logical reasoning systems that are phrased as &#x2018;logics of belief&#x2019;. These logical systems can be very complex, and hard for anyone other than a mathematical logician to understand. However, in their own terms, they are capable of expressing statements that correspond to philosophical statements you or I might make.</Paragraph><Paragraph>A research group at the University of South Carolina has come up with a control system for &#x2018;robust&#x2019; space missions that embody particular &#x2018;philosophical&#x2019; principles within the logical system controlling the robot. For example, the control program must, among other things:</Paragraph><BulletedList><ListItem>not harm the mission;</ListItem><ListItem>not harm the mission participants;</ListItem><ListItem>not harm itself.</ListItem></BulletedList><Paragraph>It is interesting to note that these rules do not preclude a robot from participating in a mission that will, for example, harm people not regarded as members of the mission, whether deliberately, or as a side-effect. Indeed, some missions might include killing people as their primary goal.</Paragraph><Paragraph>Another view of Asimov&#x2019;s laws sees them as principles that robot designers can use to produce &#x2018;safe&#x2019; robots. For example, designers might decide to &#x2018;design out&#x2019; the possibility of the robot performing in a particular way (i.e. prevent it from being in a situation where it could break the three laws). A related view is to regulate the sale of robots to those that do not appear to contravene the three laws in their daily operation. So, while it may not be possible to build robots that implement the three laws, it would be possible to prevent their sale if they did end up breaking the laws. You might also imagine that a self-destructive robot is not likely to be a popular choice in the marketplace!</Paragraph><Paragraph>Finally, Asimov&#x2019;s laws may be thought of as embodying a control strategy. Specifically, the laws describe three rules which the robot must obey, but these rules are prioritised. So for example, although the second law states that a robot should generally follow the command of a human, a robot ordered by one human to kill another would not do so, because this would contravene the first law.</Paragraph><Paragraph>Many of Asimov&#x2019;s later stories play out situations in which applications of the laws may suggest some sort of conflict. They also consider what would happen if the priorities of particular rules were changed &#x2013; if they were made equal, for example. Although this might seem like &#x2018;just so much science fiction&#x2019;, this sort of control strategy is applied with great effect in real robot control systems.</Paragraph><Paragraph>As a system of control, the way the three laws interact can be compared to subsumption architecture (see <a href="http://people.csail.mit.edu/brooks/papers/elephants.pdf">Elephants don&#x2019;t play chess</a>). In subsumption architecture, higher level behaviours triggered by particular sensor inputs, take over from (or subsume) lower level behaviours in much the same way that the higher priority first law can &#x2018;subsume&#x2019; behaviours that may be acceptable under the second and third laws.</Paragraph><Paragraph>Thus, although they are not engineering truths, Asimov&#x2019;s laws of robotics can be a useful starting point for thinking about the social context in which a robot will be used, and the extent to which control over the robot should be delegated from a human controller to computing machinery.</Paragraph><Paragraph>Many of today&#x2019;s complex systems are controlled by computers. In the event of a failure, the computer control system may be able to shut the system down automatically. In other cases a human is kept &#x2018;in the loop&#x2019;. The question this raises, and to which Asimov&#x2019;s laws allude, is &#x2018;to what extent should we build in safeguards to protect ourselves from the control, or behavioural decisions of a machine, and the actions and consequences of those actions that are likely to result?&#x2019;</Paragraph><InternalSection><Heading>Fail-safe engineering</Heading><Paragraph>While we are unable to embed the three laws in a general purpose robot, we are still faced with the problem of how to go about building safe robots. We might ask the more general question: Can any engineered system be fail-safe? To answer this let&#x2019;s consider two major disasters: the collapse of the Tay Bridge of 1879 and the <i>Challenger</i> space mission disaster of 1986.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_6\t184_lesson6_f03.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson6_f03.jpg" x_imagewidth="396" x_imageheight="269"/><SourceReference><ItemRights><OwnerRef/><ItemRef/><ItemAcknowledgement><font val="Verdana"><language val="">From 'The new </language></font><font val="Verdana"><language val="">Tay</language></font><font val="Verdana"><language val=""> </language></font><font val="Verdana"><language val="">Bridge</language></font><font val="Verdana"><language val=""> at </language></font><font val="Verdana"><language val="">Dundee</language></font><font val="Verdana"><language val="">', Illustrated London News, August 6, 1877.</language></font></ItemAcknowledgement></ItemRights><font val="Verdana"><language val="">Illustrated London News, 1877</language></font></SourceReference></Figure><Paragraph>The Tay Bridge was a box construction with box-section legs up to 230 feet apart. It was designed to carry steam trains for a distance of almost two miles over the River Tay in Scotland. The bridge collapsed on 28 December 1879, causing a train to plunge into the waters below, killing all 75 people on board.</Paragraph><Paragraph>Professor Robert Schwartz at Mount Holyoak College writes:</Paragraph><Quote><Paragraph>... the tragedy of the first Tay Bridge and its designer, Thomas Bouch, ... was a story of continual changes and errors of design, margins cut down to the limit (for which Bouch was known), appalling negligence and sheer bad workmanship. The inquiry took the view that the defects with the bridge were so numerous that it would sooner or later have to come down.</Paragraph><Paragraph><a href="http://www.mtholyoke.edu/courses/rschwart/ind_rev/iln/viaduct.htm#tay">Click here to see whole article</a></Paragraph></Quote><Paragraph>The Tay Bridge was a disaster waiting to happen. The same might be said of the 1986 <i>Challenger</i> disaster, as documented in the Report of the Presidential Commission:</Paragraph><Quote><Paragraph>Chapter IV: The Cause of the Accident [40] ... the Commission concluded that the cause of the Challenger accident was the failure of the pressure seal in the aft field joint of the right Solid Rocket Motor. The failure was due to a faulty design unacceptably sensitive to a number of factors. These factors were the effects of temperature, physical dimensions, the character of materials, the effects of reusability, processing, and the reaction of the joint to dynamic loading.</Paragraph><Paragraph>Chapter V: The Contributing Cause of The Accident. [82] The decision to launch the Challenger was flawed. Those who made that decision were unaware of the recent history of problems concerning the O-rings and the joint and were unaware of the initial written recommendation of the contractor advising against the launch at temperatures below 53 degrees Fahrenheit [the temperature of 37&#xB0; Fahrenheit] and the continuing opposition of the engineers at Thiokol after the management reversed its position. They did not have a clear understanding of Rockwell&#x2019;s concern that it was not safe to launch because of ice on the pad. If the decision makers had known all of the facts, it is highly unlikely that they would have decided to launch 51-L [Challenger] on January 28, 1986.</Paragraph></Quote><Paragraph>Thus three kinds of human failure contributed to the disaster. The first was an engineering design error, the second was a management communication failure, and the third was the manager&#x2019;s decision to launch.</Paragraph><Paragraph>Software is another area in which engineering failure is common. The reasons for software failures include:</Paragraph><BulletedList><ListItem>poor specification &#x2013; i.e. statement of what the system should do;</ListItem><ListItem>poor implementation &#x2013; i.e. programming or fabrication;</ListItem><ListItem>poor management of the process;</ListItem><ListItem>the impossibility of testing software in all operating conditions.</ListItem></BulletedList><Paragraph>If it were possible to embed the three laws in robots, by definition they would be safe. In this case, there would probably be legislation requiring all robots to have the laws built in.</Paragraph><Paragraph>In software engineering there are two tasks: first, to get the specification right and second, to implement it. Even if you get the first one right you can&#x2019;t guarantee you&#x2019;ll manage the second.</Paragraph><Paragraph>In recent years there has been a lot of interest in so-called &#x2018;formal methods&#x2019; of software specification to address this problem. The idea is that if the specification is correct, the program will be correct. If the specification is wrong the implementation is likely to be wrong as well.</Paragraph><Paragraph>Another approach that can be taken towards ensuring that systems are implemented correctly is to implement them more than once. If the systems behave the same each time, it is assumed that they are likely to be correct implementations. If one unit fails, then the arbitrator may go with the majority decision, although there are alternative strategies. Approaches such as this are used in many space systems, such as the shuttle, as well as other safety critical situations.</Paragraph><Paragraph>There are some systems for which you cannot use formal methods &#x2013; those that can&#x2019;t be completely specified and who&#x2019;s internal behaviour is not understood, e.g. systems developed by evolution. These systems often work well but we are not quite sure how they do it.</Paragraph><Paragraph>Asimov&#x2019;s positronic brain suggests one way of trying to ensure that a robot behaves as specified. As Asimov described the positronic brain, it is logically impossible for it to do anything that is inconsistent with the three laws. Even so, a wide variety of unexpected behaviours is possible. Based on today&#x2019;s robot control systems, it is unlikely that anything like the three laws will ever satisfactorily be used in a real robot. There are simply too many problems associated with implementing rules that could cope with the levels of uncertainty likely to be present when trying to reason sensibly about potentially dangerous and quickly changing human situations.</Paragraph></InternalSection><SAQ><Heading>SAQ 2</Heading><Question><NumberedList class="lower-alpha"><ListItem>Do you think that engineered systems can be fail-safe?</ListItem><ListItem>If they existed, do you think that robots with positronic brains would be well behaved?</ListItem></NumberedList></Question><Answer><NumberedList class="lower-alpha"><ListItem>The discussion shows that engineering disasters are caused by human failings.  I think human error is part of the human condition, so the best we can do is create systems that try to identify and isolate errors, or minimise their impact when they occur.  In both the examples, there were national committees of inquiry.  Generally, such committees focus on finding out what went wrong, what caused it, and what can be done to make sure that a similar disaster never happens again.  If handled well, they can reduce the risk of failures in the future.  However, there are always risks associated with new technologies.  This may be an intrinsic price of innovation and progress.</ListItem><ListItem>At first sight Asimov&#x2019;s laws seem to guarantee that robots will be well behaved. However there are obvious dilemmas such robots would have to resolve.  For example, if two football coaches told a robot to play on their sides, which team would the robot choose without violating the second law?</ListItem></NumberedList></Answer></SAQ></Section></Session><Session><Title>6.3 Relationships with robots</Title><Paragraph>There is nothing new about humans forming strong bonds with inanimate objects. For example, there are the sports car lovers who are prepared to get wet in order to keep their cars dry, and children who treat their teddy bears like living companions.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_6\t184_lesson6_f04.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson6_f04.jpg" x_imagewidth="300" x_imageheight="186"/><Caption/></Figure><Paragraph>Would the relationships we form with robots be like the man and his sports car, or would they be different?  If so, how? Is it possible to fall in love with a robot?  In this section we will try to find the answers.</Paragraph><Section><Title>Robots in social networks</Title><Paragraph>In Asimov&#x2019;s story of Robbie there are five main characters. The diagram below is a simple example of what is called a &#x2018;social network&#x2019;, which shows the main characters in the story and the relationships between them. In fact, the network is more complicated than this &#x2013; each of the five characters has a relationship with the remaining four, making twenty relationships in total. I will summarise some of the main relationships below.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_6\t184_lesson6_f05.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson6_f05.jpg" x_imagewidth="350" x_imageheight="272"/><Caption/></Figure><Paragraph><b>Gloria &#x2013; Robbie:</b>  First there is the love that Gloria feels for Robbie, and her dependence on him.  She needs him to play with and share news such as getting a new dog.  For Gloria, Robbie is a living creature whom she bosses around a bit.  As far as Robbie is concerned, he is Gloria&#x2019;s slave.  But it&#x2019;s a subtle relationship since Robbie&#x2019;s behaviour affects Gloria&#x2019;s. This may be necessary for the robot to satisfy the girl&#x2019;s needs in a relationship.  How can she forgive him for being moody if he is not moody in the first place?  As a nursemaid, how can Robbie teach Gloria effective patterns of social behaviour if he does not try to direct her behaviour in subtle ways?</Paragraph><Paragraph><b>Gloria &#x2013; Mrs Weston:</b>   The relationship that Mrs Weston has with her daughter is a combination of maternal love and power.  Mrs Weston is a bit bossy and, like many grown-ups, has many pressing concerns and does not listen very carefully to what her daughter is saying.  But Gloria is feisty like her mother.  She knows what she wants and she&#x2019;ll argue for it.  Perhaps Gloria&#x2019;s bossiness towards Robbie imitates her mother&#x2019;s bossiness to everyone, including Mr Weston.</Paragraph><Paragraph><b>Gloria &#x2013; Mr Weston:</b>  Mr Weston appears to be a loving and caring father.  Having made a mistake in letting his wife get rid of Robbie, he devises a plan for them to meet Robbie at US Robotics, in the hope that his daughter will be happy again and that his wife won&#x2019;t be too cross with him.</Paragraph><Paragraph><b>Gloria &#x2013; the dog:</b>  Initially Gloria is disposed to have a loving relationship with the dog.  However, when she realises that the dog is supposed to be a substitute for Robbie she comes to hate it.</Paragraph><Paragraph><b>Mrs Weston &#x2013; Robbie:</b> If Gloria has a very positive relationship with Robbie, Mrs Weston has the opposite. The relationship is asymmetric.  Mrs Weston is the boss and Robbie cowers in her presence, ready to do anything she says without question. At best, Robbie is just a machine, at worst he is threatening and sinister.</Paragraph><Paragraph><b>Mrs Weston &#x2013; Mr Weston:</b> Mr Weston is an easy-going man who likes to be comfortable. After ten years of marriage he was &#x2018;so unutterably foolish&#x2019; to love his wife. Getting Robbie was his idea.  This link is not shown in the diagram but is important nonetheless.</Paragraph><Paragraph><b>Mrs Weston &#x2013; the dog:</b>  Mrs Weston bought the dog as a substitute for Robbie.  There is no emotional relationship.  The dog is a possession, and when it ceases to serve its purpose it is disposed of.  I think Mr Weston also feels like this.  This may seem rather utilitarian, but it is the attitude farmers in many parts of the world have towards their animals.   This is very much the kind of relationship people actually have with robots &#x2013; they may form attachments to them, but ultimately they see them as machines to which they have no moral commitment.</Paragraph><Paragraph><b>Mr Weston &#x2013; Robbie:</b>  Mr Weston is pro-robot, having originally had the idea of getting Robbie for Gloria.  He takes a utilitarian approach to his relationship.  Trying to resolve Gloria&#x2019;s grief, he schemes for Robbie to be brought back again.  Although he has no animosity towards Robbie, the robot is a means to an end. He has no emotional attachment to the robot either way, and for him Robbie is just another gadget, albeit one that his daughter is very attached to.  He sees nothing sinister in Robbie but eventually, under pressure from Mrs Weston, he disposes of Robbie.</Paragraph><Paragraph>As you can see, the story provides a wealth of human<b>&#x2013;</b>robot interactions. The relationships that people have with the robot are conditioned by the relationships they have with each other.</Paragraph><Paragraph>In the diagram above, the question mark beside Mrs Weston, Gloria and Robbie denotes the tension that exists between them. Gloria and her mother have a loving mother<b>&#x2013;</b>daughter relationship. Mrs Weston does not like Robbie; Gloria loves him. This tension between Gloria&#x2019;s love for Robbie, Mrs Weston&#x2019;s concerns for her daughter&#x2019;s welfare and the disapproving neighbours, and Mrs Weston&#x2019;s negative relationship with Robbie is the initial driving force of the story. Mrs Weston gets rid of Robbie because she is concerned that the love Gloria feels towards Robbie may be harming her daughter.</Paragraph><Paragraph>After this, the story is driven by Gloria&#x2019;s unhappiness and Mr Weston&#x2019;s subterfuge in causing Robbie to be &#x2018;found&#x2019; in New York.  (Mr Weston plots to get Robbie back because he feels Gloria&#x2019;s relationship with Robbie is not harmful to her, but that her unhappiness is.) These harmonious relationships between Mr Weston, Gloria and Robbie are represented by the tick in the diagram.</Paragraph><Paragraph>There are hints that Robbie feels social slights just like a human.  He cowers in the kitchen as the sharp-tongued Mrs Weston tells Gloria he should not be there. And, implicitly, he feels great fondness for his young charge.  To Mr and Mrs Weston, any feelings Robbie might have are as irrelevant as those of the dog.</Paragraph></Section><Section><Title>Robots in the family</Title><Paragraph>Although Robbie the humanoid robot is beyond current technology, there are already robots designed to live as helpful companions in the house.  For example, the Wakamaru robot designed and manufactured by Mitsubishi Heavy Industries Ltd:</Paragraph><Quote><Paragraph>[Wakamaru] has a daily rhythm of life and recognises the position in a house and the position of you in the same room. It approaches you, moves according to the time and lives with you while charging the batteries by itself.</Paragraph><Paragraph>...</Paragraph><Paragraph>[It] lives beside you from wake up time to bed time in harmony with your life. If it finds you in the same room, it comes closer to you and speaks to you. At night, it stands by at its charging station and responds if requested.</Paragraph><SourceReference/></Quote><Paragraph>Wakamaru uses the Internet to provide information and services.  It can be given commands via a mobile phone or personal computer connected to the Internet. It navigates autonomously around the house by matching ceiling images with a stored house map.  It finds people using sound and heat sensors, and an on-board camera.</Paragraph><Paragraph>Wakamaru can recognize 10,000 different words, including the name you give it. It can also identify two &#x2018;owners&#x2019; and up to eight other people by face recognition.  The robot can report if it detects a loud sound or moving object nearby.</Paragraph><Paragraph>Apart from acting as housekeeper when the owners are away, and a companion when they are at home, Wakamaru monitors its owners&#x2019; well-being by asking questions such as: &#x2018;Have you slept well?&#x2019; and &#x2018;Did you enjoy your meal?&#x2019;</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_6\t184_lesson6_f06.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson6_f06.jpg" x_imagewidth="347" x_imageheight="500"/><Caption>Wakamaru</Caption><SourceReference><font val="Verdana"><language val="">&#xA9; 2003, Mitsubishi Heavy Industries Ltd.</language></font></SourceReference></Figure><Paragraph>Robots like Wakamaru give us a glimpse into the not-so-distant future.  Do we need the services that such a robot can provide?  Someone who has an elderly relative living on their own might find the presence of such a robot reassuring.   Although there is very little crime in Japan compared to other countries, the security features built into Wakamaru could also give peace of mind to people who have to leave their houses empty during the day while at work.</Paragraph><Paragraph>These types of domestic robots can be very helpful, and no doubt they will become increasingly so. They can undertake tasks such as turning on the cooker, drawing the blinds, switching on the lights, controlling the heating, taking phone messages, setting the video recorder and even controlling less intelligent machines.</Paragraph><Paragraph>As robots become more sophisticated, there may be limitations on the things we would want them to do, or even allow them to do. Suppose you need a babysitter.  What is the youngest person you would feel comfortable leaving an eight-year old child with? How about a five-year old or a newborn baby? Would you be prepared to use a robot babysitter? If so, how old would the child have to be?</Paragraph></Section><Section><Title>Relating to others &#x2013; Is Tiggy a robot?</Title><Paragraph>How would you interact with a robot that is indistinguishable from a human?  How do you think your relationships with robot pets might differ from your relationships with real ones? You may find the following website useful: <a href="http://www.robotherapy.org/">http://www.robotherapy.org/</a></Paragraph><Paragraph>To get some insights into these questions, I conducted a thought experiment involving my pet dog Tiggy. I imagined her to be a robot.  The first thing I noted was that Tiggy is a very complex and sophisticated robot.  Although relatively easy to simulate Tiggy&#x2019;s shiny black coat, simulating her body is much more difficult.  She is infinitely more supple and flexible than the robot seal which we looked at in Lesson 1 &#x2013; although the seal does wriggle in a remarkably life-like way.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_6\t184_lesson6_f07.jpg" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson6_f07.jpg" x_imagewidth="250" x_imageheight="200"/><Caption/></Figure><Paragraph>Next I considered the intelligence of Tiggy the robot, and my relationship with her.</Paragraph><Paragraph>Every morning I take Tiggy for a walk in the woods.  She is still young and I am still trying to train her.  So every time we have to cross the road on the way to the woods, I tell her to sit. Sometimes she even does it without being told. So, Tiggy is a robot that learns.</Paragraph><Paragraph>When we get to the woods, Tiggy goes scurrying off all over the place, following what I assume are very exciting smells.  So, my robot dog has a very good sense of smell, and the necessary processing to make sense of it.</Paragraph><Paragraph>Why is a robot behaving like this?  It could be doing it to fool me that it&#x2019;s a real dog, or it could be doing it because this behaviour is built in.  I assume that a real dog behaves in this way due to instincts that lead to finding food or a mate.  Perhaps Robot Tiggy needs food and a mate?</Paragraph><Paragraph>Occasionally I have the problem that Tiggy disappears and no matter how I try to attract her, she doesn&#x2019;t come back until she&#x2019;s ready to. I&#x2019;ve tried teaching her to come back to the sound of  a whistle, and most of the time she comes racing back to get the titbit as a reward. This makes me wonder who is training who.  Does she come back when I want her to, or when she decides she wants the reward that she trained me to give her.</Paragraph><Figure><Image src="\\dog\PrintLive\Courses\t184\web\e1i1\published\images\lesson_6\t184_lesson6_f08.gif" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="t184_lesson6_f08.gif" x_imagewidth="345" x_imageheight="133"/><Caption/></Figure><Paragraph>Suppose Tiggy really were a robot. How might this change my relationship with her?</Paragraph><Paragraph>My friend Ralph told me that his wife brought home a robot dog one day.  I asked him what happened to it.  He told me that they played with it for a while, but now it is in the cupboard.  I reflected on the possibility of putting Tiggy in the cupboard.</Paragraph><Paragraph>A dog needs company and it needs to be walked frequently.  Most of the time this fits in very well with our human needs.  I love walking in the woods and Tiggy is great fun when you play with her.  However, sometimes it&#x2019;s cold and miserable, and I don&#x2019;t feel like going out at all.  But there is no choice &#x2013; I have to go. Strangely, I often enjoy these enforced walks, even though I&#x2019;d never take them without Tiggy.</Paragraph><Paragraph>If Tiggy were a robot I can imagine that I would be very selective about taking her for a walk, and very soon she would end up in the cupboard, just like the robot dog that Ralph&#x2019;s wife brought home.  So my relationship with Tiggy is conditioned by the fact that I know she is not a robot.  If she were a robot, no matter how lifelike she appeared, I would sooner or later get lazy and she would end up in the cupboard.  But I can&#x2019;t do this with Tiggy because I have a moral commitment to her &#x2013; I am compelled to look after her properly.</Paragraph><Paragraph>So for me, the relationship I have with Tiggy is different from the relationship I would have with her if she were a robot.  It comes down to my expectations.  I expect Tiggy the dog to have claims on me that Tiggy the robot could not have.</Paragraph><Paragraph>Of course, if I didn&#x2019;t know that Tiggy were a robot, I might indeed treat her like a dog. Perhaps that is what&#x2019;s really happening even now!</Paragraph></Section><Section><Title>Luddites and anti-robot sentiment</Title><Paragraph>When the reporter in Asimov&#x2019;s story asked Susan Calvin if Robbie stayed with the girl, she replied:</Paragraph><Quote><Paragraph>Susan Calvin shrugged her shoulders,</Paragraph><Paragraph>&#x2018;Of course, he didn&#x2019;t. That was 1998. By 2002, we had invented the mobile speaking robot which, of course, made all the non-speaking models out of date, and which seemed the final straw as far as the non-robot elements were concerned. Most of the world governments banned robot use on Earth for any purpose other than scientific research between 2003 and 2007.&#x2019;</Paragraph><Paragraph>&#x2018;So that Gloria had to give up Robbie eventually?&#x2019;</Paragraph><Paragraph>&#x2018;I&#x2019;m afraid so. I imagine, however, that it was easier for her at the age of fifteen than at eight. Still, it was a stupid and unnecessary attitude on the part of humanity.&#x2019;</Paragraph></Quote><Paragraph>So, Asimov predicted that there would be &#x2018;non-robot elements&#x2019; &#x2013; people against the use of robots &#x2013; who would succeed in banning the use of robots on Earth. This anti-machine sentiment has a long tradition.</Paragraph><Paragraph>In the early part of the 19th century, the industrial revolution saw the introduction of machinery into textiles manufacturing. It is said that Ned Ludd led an uprising in Nottinghamshire to destroy two large stocking frames which were producing inexpensive stockings that undercut stockings produced by skilled knitters. The voice of Ned Ludd was heard in the woollen towns of Lancashire and Yorkshire, where industrial technologies were threatening to replace skilled workers, and the short-lived Luddite movement was born.</Paragraph><Paragraph>The Luddites rose up in defence of their own livelihood, and committed several acts of machine breaking, smashing up the machines newly introduced into factories. The time was a period of significant unrest anyway. Many arbitrary acts of violence, partly arising from food shortages and the breakdown of order in a time of war (Napoleonic), came to be associated with the actions of the Luddites. In the end, the British Government responded harshly to the threat of sustained civil unrest, and in 1813 seventeen men were executed for machine breaking.</Paragraph><Paragraph>Today, the term &#x2018;Luddite&#x2019; is often used to describe someone who opposes the advance of technology. As the origins of Luddism suggest, people have always had an ambivalent relationship with new technologies, especially when their jobs or lifestyles are threatened. However, it appears to be impossible to stop the march of technological progress.</Paragraph><Paragraph>It is interesting to consider whether the widespread introduction of robots into our society will inspire passions as violent as those of the original Luddites. Industrial robots are often seen as being well suited to jobs that are too dirty, dangerous or demeaning (the three D&#x2019;s) for humans to do.</Paragraph><Paragraph>However, what irked the original Luddites was the way industrial technology looked set to replace <b>skilled</b> rather than <b>menial</b> labour.</Paragraph><Paragraph>Today, robots are not seen as a great threat to people&#x2019;s jobs, any more than any type of automation. However, this may be because robots are still very primitive and are unable to do many jobs performed by manual (and non-manual) workers. This could change if robots ever become as competent as those in Asimov&#x2019;s stories.</Paragraph><Paragraph>Do robots threaten your job or lifestyle? Before you consider your answer, you may find it interesting to look at these links on robot automation:</Paragraph><Paragraph><a href="http://www.adept.com/">http://www.adept.com/</a></Paragraph><Paragraph><a href="http://www.abb.com/robotics">http://www.abb.com/robotics</a></Paragraph><Paragraph>If RoboCup is a success, and robot football players really are able to take on the world champions in 2050, will multimillionaire football stars become a thing of the past? If so, do you think that footballers might become Luddites, campaigning against the new machines?</Paragraph></Section></Session><Session><Title>6.4 Robot rights</Title><InternalSection><Heading><b>Equal rights for robots?</b></Heading><Paragraph>In most countries there are laws to give people equal rights and opportunities. How might this work for robots? For example, should lifts be provided for wheeled robots that can&#x2019;t climb stairs? Here are some robot rights issues you might like to consider:</Paragraph><BulletedList><ListItem>the right to vote in elections;</ListItem><ListItem>the right to freedom and the abolition of robot slavery;</ListItem><ListItem>the right to an education;</ListItem><ListItem>the right to free speech;</ListItem><ListItem>the exercise of religious freedom;</ListItem><ListItem>the right to citizenship.</ListItem></BulletedList><Paragraph>In this lesson we are considering a future where anything is possible. Let us suppose robots become our equals. Would we be willing to legislate against robot discrimination? Would we decide that robots are not entitled to go on strike? Would we try to ensure equal rights for robots in the workplace?</Paragraph><Activity><Heading>Discuss</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\forum_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="forum_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure></Paragraph><Paragraph>In an industrial setting, what sorts of factors are likely to influence whether a robot or a human is used for a particular task?</Paragraph><Paragraph>Spend ten minutes or so jotting down what factors you think might influence such a decision. Post your thoughts to the T184 Discussion forum and see what some of the other students think.</Paragraph></Question></Activity><Activity><Heading>Optional reading</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\book_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="book_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure> If you have time, you may like to also read the article <a href="http://web.mit.edu/newsoffice/2000/robots-1018.html">Viewpoint: Will man-made robots rise up and demand their rights?</a> by Prof. Rodney Brooks. </Paragraph></Question></Activity></InternalSection><InternalSection><Heading>Animal rights versus robot rights</Heading><Paragraph>Although animals have rights in human societies, they certainly do not have full human rights. Animals are generally considered to be property, and for the moment, the same is true of robots. Is it possible that robots too will be given rights, but that these will be different to those enjoyed by most people in free societies?</Paragraph></InternalSection><InternalSection><Heading>Children&#x2019;s rights versus robot rights</Heading><Paragraph>Children&#x2019;s rights are protected by legislation. Could robots&#x2019; rights be protected in a similar way?</Paragraph><Paragraph>In 1989, the United Nations Children&#x2019;s Fund (UNICEF) adopted a convention on the rights of the child (<a href="http://www.unicef.org/crc/">http://www.unicef.org/crc/</a>).</Paragraph><Paragraph>The Convention reiterates some of the rights espoused under the UN&#x2019;s Universal Declaration of Human Rights, as well as introducing additional protection specifically for children relating to their physical and mental health. Parents are held to be responsible for the well-being of their children and governments are required to assist parents in this duty.</Paragraph><Paragraph>In the UK there is much legislation that relates specifically to children (for example, there is a chronology of youth related policies in the UK on Keele University&#x2019;s School of Social Relations website: <a href="http://www.keele.ac.uk/depts/so/youthchron/CivilFamilyLaw/">http://www.keele.ac.uk/depts/so/youthchron/CivilFamilyLaw/</a>).These regulations protect the rights of children by not only guaranteeing them access to certain provisions, but also preventing them from participating in certain activities.</Paragraph></InternalSection><Section><Title>Robots as slaves</Title><Paragraph>In the case of robots, it might be that companies retain rights over the ownership of some part of the robot. For Asimov, the US Robots company actually owned all the robots it produced, and users simply licensed the use of the robots. A similar situation occurs today with computer software &#x2013; you do not own your copy of Windows. Instead, the software is licensed to you by an agreement which specifies the conditions under which you are allowed to use it.</Paragraph><Paragraph>In social terms, the notion of the master&#x2013;slave relationship emphasises that the slave is the property of the master. This is not to say that the slave is necessarily not trusted by the master. In ancient Rome, slaves could often act on behalf of their master, for example by running a household. Slaves in charge of the household would often be charged with assigning duties to other slaves lower down the order.</Paragraph><Paragraph>Some slaves were also treated as part of the family, although the master&#x2013;slave relationship was still evident. Slaves could not own property &#x2013; as they were the property of their masters. The slaves&#x2019; possessions were also the property of their masters. Slaves would often be freed in old age because they could no longer perform useful service.</Paragraph><Paragraph>Individuals were often enslaved as a result of war. When Rome conquered new territories, everything in the territory, people included, became a possession of the Roman Empire. The prices of slaves depended on their status and how skilled they were. Slaves became the property of the Roman Empire, and could be sold on.</Paragraph><Paragraph>By contrast, slavery in the United States and Britain, for example, was based on racial grounds and the supposed inferiority of the slaves to their masters.</Paragraph></Section><Section><Title>Robots as victims</Title><Paragraph>If people were to &#x2018;torture&#x2019; robots or harm them for pleasure, would that make them victims of human cruelty? In the TV programme <i>Robot Wars</i> the robots are radio-controlled machines, not sentient beings. In fact, they seem little different to the cars taking part in &#x2018;banger racing&#x2019;, which speed around rough grass tracks and frequently collide and get damaged &#x2013; a case of cruelty to cars perhaps!</Paragraph><Paragraph>In the film <i>Westworld</i>, which we looked at in Session 6.1, holidaymakers went to Westworld precisely because there were humanoid robots there programmed to indulge every passion. Would such advanced robots be indifferent to abuse? Would it be cruel to treat them in this way?</Paragraph><Activity><Heading>Discuss</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\forum_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="forum_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure></Paragraph><Paragraph>In the past, cock fighting, dog fighting, bear baiting and other sports in which animals fight each other to the death were very popular. Today these activities are illegal in the UK, but do still occur. My moral commitment to my dog, Tiggy, makes the thought of her being subjected to such cruelty repulsive. But what if she really were a robot, even though she looks exactly like a dog? Would it be morally acceptable to watch a &#x2018;dog fight&#x2019; between robots? There is no legislation for this as yet, but do you think human bloodlust could be satisfied legally by using animals from the Robot Zoo? </Paragraph><Paragraph>Post your thoughts to the T184 Discussion forum.</Paragraph></Question></Activity></Section></Session><Session><Title>6.5 Robot agents</Title><Section><Title>Robots as agents of the state</Title><Paragraph>We established in Lesson 4 that it is unlikely that robots will take over the world. But perhaps they could be used less overtly to help states and the forces of government maintain control over their own civilians or prosecute wars against others.</Paragraph><Paragraph>One of the main drivers of robotic technology is the desire by governments to reduce the number of casualties on their side. The increasing automation of weapons of war, as well as the increasing use of unmanned surveillance vehicles, not only ground based but also aerial and underwater, may point to a future of robot warriors.</Paragraph><Paragraph>You have already seen in Lesson 1 that there are examples of robots such as cruise missiles that don&#x2019;t obey Asimov&#x2019;s first law. This creates a considerable moral dilemma: How can we justify robots that are designed to kill people?</Paragraph><Paragraph>Humankind has shown violence to members of its own species since the earliest days, and conflict is a recurrent theme in human history. In human societies the taking of life is occasionally thought of as acceptable &#x2013; repelling invading armies or using capital punishment, for example. Asimov came across this &#x2018;problem&#x2019; in some of his stories. He formulated a zeroth law that takes precedence over the other three laws:</Paragraph><Quote><Paragraph>A robot may not injure humanity or, through inaction, allow humanity to come to harm.</Paragraph></Quote><Paragraph>Note the use of the word &#x2018;humanity&#x2019; in the zeroth law. In fact, herein lies the problem with the zeroth law: What constitutes &#x2018;humanity&#x2019;? And who judges what is harmful to it? For example, cars sometimes cause fatal accidents &#x2013; this is true anywhere there are cars. Is this not harmful to humanity? We continue to drive because we judge driving to give humanity (or our bit of it) great benefits, which outweigh the cost in human life. So, what is beneficial to humanity is a matter of judgement, involving a trade-off of good against bad. These judgements depend on individual value systems, but these of course vary widely.</Paragraph><Paragraph>Human beings are unable to agree a universal set of values. Despite organisations such as the United Nations, there are many military conflicts in the world. In the context of a human disagreement, could a robot decide what would &#x2018;allow humanity to come to harm&#x2019;?</Paragraph></Section><Section><Title>Robots as the agents of war and peace</Title><Paragraph>We saw earlier how the RoboCup challenge is helping to drive robotics research forward. In a similar way the US military is also using challenges in its drive to develop robots to support military action.</Paragraph><Paragraph>In January 2003, the US Department of Defense Agency (DARPA) announced a million dollar prize for the winner of a competition for autonomous robots to race from Los Angeles to Las Vegas within ten hours:</Paragraph><Quote><Paragraph>Now the US Department of Defense is stepping up efforts to develop future autonomous robotic ground vehicles that would operate in concert with manned systems to form an integrated fighting force. The goal is not simply to replace people with machines, but to team people with robots to create a more capable, agile, and cost-effective force that lowers the risk of US casualties.</Paragraph><Paragraph>As part of the effort [we] are conducting the DARPA Grand Challenge for autonomous robotic ground vehicles. Scheduled for March 2004, it will cover a route of approximately 300 miles between Los Angeles and Las Vegas. The robotic vehicle that most quickly completes the route in less than [10 hours] will earn its team a cash prize of $1 million. The challenge is intended to spur the accelerated development of autonomous robotic ground vehicle technology for military applications, and is the first in a series of Grand Challenges planned by DARPA.</Paragraph><SourceReference>DARPA,  2003 </SourceReference></Quote><Paragraph> </Paragraph><Paragraph>Although no robot car has yet driven from Los Angeles and Las Vegas, as you saw in Lesson 1 the progress in the DARPA challenge has been considerable. But where will it all end? Will the spin-offs from RoboCup be not only robots that can play football better than humans, but also robots that fight and defeat human soldiers? According to the DARPA announcement, the idea is currently that humans will work together with autonomous robots. Presumably this collaboration will minimise the dangers to humans. Can we &#x2018;look forward&#x2019; to nations having proxy wars through their robot armies? Should robot soldiers fight our wars and police our streets?</Paragraph><Paragraph>Professor Noel Sharkey has written about the ethical concerns that arise with the increasing use of battlefield robots. By December 2008, over 5000 mobile robots had been deployed in Iraq and Afghanistan. These are mostly used for surveillance and bomb disposal but some are now armed. Unmanned aerial vehicles are now used in combat. All currently have a human &#x2018;in the loop&#x2019; to decide when to use lethal force, but there are pressures to give increasing autonomy to such robots. And in the words of Prof Sharkey, an expert in AI, &#x2018;We are going to give decisions on human fatality to machines that are not bright enough to be called stupid.&#x2019; </Paragraph><Activity><Heading>Optional reading</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\book_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="book_42.png" x_imagewidth="42" x_imageheight="42"/></InlineFigure> If you would like to read some of Professor Sharkey&#x2019;s work, here are links to two articles:</Paragraph><UnNumberedList><ListItem><a href="http://www.guardian.co.uk/commentisfree/2007/aug/18/comment.military">Robot wars are a reality</a></ListItem><ListItem><a href="http://www.sciencemag.org/content/322/5909/1800.full">The Ethical Frontiers of Robotics Science</a></ListItem><ListItem/></UnNumberedList></Question></Activity><Paragraph>On the humanitarian side there is also considerable interest in the use of battlefield medical robots. The US Military, for example, announced a major program of research in this area: <a href="http://newswww.bbc.net.uk/1/hi/world/americas/4391161.stm">http://newswww.bbc.net.uk/1/hi/world/americas/4391161.stm</a></Paragraph><Paragraph>Another humanitarian role for robots is the part they are increasingly likely to play in search and rescue missions. For example, the <a href="http://www.crasar.org/">CRASAR Group</a> is developing new generations of rescue robots. RoboCup includes a competition based on urban search and rescue missions. </Paragraph></Section></Session><Session><Title>6.6 RobotLab6</Title><Paragraph>In this session you will be doing lab activities using RobotLab6.</Paragraph><Activity><Heading>RobotLab</Heading><Question><Paragraph><InlineFigure><Image src="\\dog\PrintLive\nonCourse\design_templates\ICONS\Web\png's\42_pixels\robotlab_42.png" height="" width="100%" alt="" id="" webthumbnail="" x_imagesrc="robotlab_42.png" x_imagewidth="42" x_imageheight="35"/></InlineFigure>Download the lab guide for <olink targetdoc="RobotLab6">RobotLab6</olink>. Work through the guide carefully and do the activities.</Paragraph></Question></Activity></Session><Session><Title>6.7 Summary</Title><Paragraph>In Lesson 6 we have begun to analyse in some depth the relationships that people have with robots.  In this lesson you have looked at:</Paragraph><BulletedList><ListItem>the Hollywood portrayal of robots;</ListItem><ListItem>how Asimov&#x2019;s laws can be seen as guiding principles in robot design;</ListItem><ListItem>the idea of the positronic brain and fail-safe engineering;</ListItem><ListItem>the type of relationships people may have with robots;</ListItem><ListItem>robot rights in the context of human rights;</ListItem><ListItem>the significance of Asimov&#x2019;s zeroth law;</ListItem><ListItem>the use of robots by the military and the police.</ListItem></BulletedList><Paragraph>It is clear that there are many moral and ethical issues concerning human&#x2013;robot relationships.  Currently these do not seem very pressing, since today&#x2019;s robots are quite basic and far removed from any artificial forms of life.  As technology improves, and humanoid robots become more and more lifelike, it will become increasingly important to reflect on these issues.</Paragraph><StudyNote><Heading>Where next:</Heading><Paragraph>This is the end of Lesson 6. When you are ready go on to <olink targetdoc="Lesson 7">Lesson 7</olink></Paragraph></StudyNote></Session></Unit><BackMatter><References><Reference>Darpa (2003), Press Release &#x2018;Robotics Technology Increasingly Important to Department of Defense&#x2019; April 24 2003 [Online at <a href="http://www.darpa.mil/grandchallenge04/media/fut_military_rel.pdf">http://www.darpa.mil/grandchallenge04/media/fut_military_rel.pdf</a>]</Reference><Reference>Noel Sharkey (2007) Robot wars are a reality. <i>The Guardian</i>, Saturday 18 August 2007. [Online at <a href="http://www.guardian.co.uk/commentisfree/2007/aug/18/comment.military">http://www.guardian.co.uk/commentisfree/2007/aug/18/comment.military</a>]</Reference><Reference>Noel Sharkey (2008) The Ethical Frontiers of Robotics Science 19 December 2008: Vol. 322. no. 5909, pp. 1800 &#x2013; 1801 <a href="http://www.sciencemag.org/content/322/5909/1800.full">http://www.sciencemag.org/content/322/5909/1800.full</a></Reference></References></BackMatter><settings><numbering><Session autonumber="false"/><Section autonumber="false"/><SubSection autonumber="false"/><SubSubSection autonumber="false"/><Activity autonumber="false"/><Exercise autonumber="false"/><Box autonumber="false"/><CaseStudy autonumber="false"/><Quote autonumber="false"/><Extract autonumber="false"/><Dialogue autonumber="false"/><KeyPoints autonumber="false"/><Reading autonumber="false"/><StudyNote autonumber="false"/><Example autonumber="false"/><Verse autonumber="false"/><SAQ autonumber="false"/><ComputerDisplay autonumber="false"/><Summary autonumber="false"/><ProgramListing autonumber="false"/><ITQ autonumber="false"/><Tables autonumber="false"/><Figures autonumber="false"/><MediaContent autonumber="false"/><Chemistry autonumber="false"/></numbering><discussion_alias>Discussion</discussion_alias><session_prefix/></settings></Item>
